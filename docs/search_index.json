[
["index.html", "Introduction to Data Science Preface", " Introduction to Data Science Rafael A. Irizarry 2018-06-04 Preface These are the class notes used in the HarvardX Data Science Series. The code to generate the notes is available on GitHub. We will be adding chapters during the next few months. For updates follow @rafalab This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. "],
["introduction.html", "Chapter 1 Introduction 1.1 Case Studies 1.2 Who Will Find This Book Useful? 1.3 What Does This Book Cover? 1.4 What is Not Covered by This Book?", " Chapter 1 Introduction The demand for skilled data science practitioners in industry, academia, and government is rapidly growing. This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown. 1.1 Case Studies Throughout the series, we use motivating case studies. For each of the concepts covered, we start by asking a specific question and answer these through data analysis. We learn the concepts as a means to answer the questions. Examples of the case studies included in the book are: Case Study Concept US Murder Rates by State R basics Trends in World Health and Economics Data Visualization The Financial Crisis of 2007-2008 Probability Election Forecasting Statistical Inference Money Ball: Building a Baseball Team Linear Regression Movie Recommendation Systems Machine Learning Throughout the book, we will be using the R software environment for all our analysis. You will learn R, statistical concepts, and data analysis techniques simultaneously. By working through the examples we gain experience in R and packages such as dplyr, ggplot2 and other tidyverse tools. In each case study we try to realistically mimic a data scientist’s experience. In a chapter on Data Wrangling we will demonstrate how we prepared the data for these analysis from its raw form. 1.2 Who Will Find This Book Useful? This book is meant to be a textbook for a first course in Data Science. No previous knowledge of R or statistics is necessary, although we do assume a high level of quantitative ability and some experience with programming. If you possess these skills, read all the chapters and complete all the exercises, you will be well positioned to perform basic data analysis tasks and to take more advanced courses and become an expert. 1.3 What Does This Book Cover? We start by going over the basics of R. You learn R throughout the book, but in the first chapter we go over the building blocks needed to keep learning during the rest of the book. Data visualization is the topic of the following chapter. The growing availability of informative datasets and software tools has led to increased reliance on data visualizations in many fields. We demonstrate how to use ggplot2 to generate graphs and describe important data visualization principles. In the third chapter we introduce probability which is useful in many contexts, in particular those that depend on data affected by chance in some way. As a result, knowledge of probability is indispensable for data science. Probability is also necessary to understand the topic of the next chapter, statistical inference. Here we describe the statistical theory used by election forecasting. In the fifth chapter we provide a brief introduction to the tools we use on a day-to-day basis in data science projects. These are the UNIX/Linux shell, git and GitHub and RStudio. Chapter six goes over several examples to familiarize the reader with data wrangling. Among the specific skills we learn are data import, web scrapping, using regular expressions, and reshaping data. We do this using tidyverse tools. After describing the skills and concepts we are ready to embark on more complex data analysis tasks. In chapter seven we delineate challenges that lead us to describe linear regression and, in chapter 8, challenges that lead us to introduce machine learning. 1.4 What is Not Covered by This Book? This book focuses on the data analysis aspects of data science. We therefore do not cover aspects related to data management or engineering. Although R programming is an essential part of the book, we do not teach more advanced topics such as data structures, optimization, and algorithm theory. Similarly, we do not cover topics such as web services, interactive graphics, parallel computing, and data streaming processing. "],
["introduction-1.html", "Chapter 2 Introduction", " Chapter 2 Introduction In this book we will be using the R software environment for all our analysis. You will learn R and data analysis techniques simultaneously. However, we need to introduce basic R syntax to get you started. In this chapter, rather than cover every R skill you need, we introduce just enough so that you can understand subsequent chapters where we provide more in depth coverage, building upon what you learned previously. We find that we better retain R knowledge when we apply it to solving specific problems. Therefore, in this chapter, as well as throughout the book, we will use a motivating case study. We ask a specific question related to crime in the United States and provide a relevant dataset. Some basic R skills will permit us to answer the motivating question. "],
["motivating-case-study.html", "Chapter 3 Motivating case study", " Chapter 3 Motivating case study Imagine you live in Europe and are offered a job in a US company with many locations across all states. It is a great job but news with headlines such as US Gun Homicide Rate Higher Than Other Developed Countries have you worried. Charts like this may concern you even more: Or even worse, this version from everytown.org: But then you remember that the US is a large and diverse country with 50 very different states as well as the District of Columbia (DC). California, for example, has a larger population than Canada and 20 US states have populations larger than that of Norway. In some respects the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial. You have options of where to live and want to determine the safety of each particular state. We will gain some insights by examining data related to gun homicides in the US using R. Before we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Be aware that the usefulness of some of these building blocks may not be immediately obvious, but later in the book you will appreciate having mastered these skills. "],
["getting-started.html", "Chapter 4 Getting Started 4.1 Why R? 4.2 Installing R 4.3 The R console 4.4 Scripts 4.5 Installing RStudio 4.6 The R ecosystem", " Chapter 4 Getting Started 4.1 Why R? R is not a programming language like C or Java. It was not created by software engineers for software development. Instead, it was developed by statisticians as an interactive environment for data analysis. You can read the full history here. The interactivity is an indispensable feature in data science because, as you will soon learn, the ability to quickly explore data is a necessity for success in this field. However, like in other programming languages, you can save your work as scripts that can be easily executed at any moment. These scripts serve as a record of the analysis you performed, a key feature that facilitates reproducible work. If you are an expert programmer, you should not expect R to follow the conventions you are used to since you will be disappointed. If you are patient, you will come to appreciate the unequal power of R when it comes to data analysis and data visualization specifically. Other attractive features of R are the following: R is free and open source. It runs across all major platforms: Windows, Mac Os, UNIX/Linux. Scripts and data objects can be shared seamlessly across platforms. There is a large, growing, and active community of R users and, as a result, there are numerous resources for learning and asking questions. It easy for others to contribute add-ons which enables developers to share software implementations of new data science methodologies. This gives R users early access to the latest methods and to tools which are developed for a wide variety of disciplines, including ecology, molecular biology, social sciences, and geography, just to name a few examples. 4.2 Installing R You can download R freely from the Comprehensive R Archive Network (CRAN). It is relatively straightforward, but if you need further help you can try the following resources: Installing R on Windows Installing R on Mac Installing R on Ubuntu If you want to try out R without installing it, you can access a web based console such as R fiddle. 4.3 The R console Interactive data analysis usually occurs on the R console that executes commands as you type them. There are several ways to gain access to an R console. One way is to simply start R on your computer. The console looks something like this: As a quick example, try using the console to calculate a 15% tip on a meal that cost $19.71: 0.15 * 19.71 #&gt; [1] 2.96 Note that in this book, grey boxes are used to show R code typed into the R console. The symbol #&gt; is used to denote what the R console outputs. 4.4 Scripts One of the great advantages of R over point-and-click analysis software is that you can save your work as scripts. You can edit and save these scripts using a text editor. We highly recommend working on an interactive integrated development environment (IDE) such as RStudio. RStudio includes an editor with many R specific features, a console to execute your code, and other useful panes, including one to show figures. Most web-based R consoles also provide a pane to edit scripts, but not all permit you to save the scripts for later use. 4.5 Installing RStudio Instructions on how to install RStudio are here and, for Windows, we have special instructions here. Once you install RStudio you can simply start RStudio rather than R since that program automatically starts R. But don’t be confused. R is a different piece of software and you can’t run RStudio without first installing R. If you are going to follow along with RStudio as you read this book, you might consider reading the chapter five sections specifically dedicated to RStudio. RStudio includes many useful features other than providing a script editor and you want to familiarize yourself with these. And remember the R scripts used to generate this book can be found on GitHub. 4.6 The R ecosystem The functionality provided by a fresh install of R is only a small fraction of what is possible. In fact, we refer to what you get after your first install as base R. The extra functionality comes from add-ons available from developers. There are currently hundreds of these available from CRAN and many others shared via other repositories such as GitHub. However, because not everybody needs all available functionality, we instead make different components available via packages. R makes it very easy to install packages from within R. For example, to install the dslabs package which we use to share dataset and code related to this book, you would type: install.packages(&quot;dslabs&quot;) In RStudio you can navigate to the Tools tab and select install packages. We can then load the package into our R sessions using the library function: library(dslabs) As you go through this book, you will see that we load packages without installing them. This is because once you install a package, it remains installed and only needs to be loaded with library. The package remains loaded until we quit the R session. If you try to load a package and get an error, it probably means you need to install it first. "],
["the-very-basics.html", "Chapter 5 The very basics 5.1 Objects 5.2 The workspace 5.3 Functions 5.4 Other prebuilt objects 5.5 Variable names 5.6 Saving your workspace 5.7 Scripts 5.8 Comments Exercises", " Chapter 5 The very basics Before we get started with the motivating dataset, we need to cover the very basics of R. 5.1 Objects Suppose a high school student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). The quadratic formula gives us the solutions: \\[ \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\] which of course change depending on the values of \\(a\\), \\(b\\), and \\(c\\). One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define: a &lt;- 1 b &lt;- 1 c &lt;- -1 which stores the values to use later. We use &lt;- to assign values to the variables. We can also assign values using = instead of &lt;-, but we recommend against using = to avoid confusion. Copy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. To see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value: a #&gt; [1] 1 A more explicit way to ask R to show us the value stored in a is using print like this: print(a) #&gt; [1] 1 We use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later. 5.2 The workspace As we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing: ls() #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;fifty_states&quot; #&gt; [5] &quot;murders&quot; In RStudio the Environment tab shows the values We should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found. Now since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula: (-b + sqrt(b^2 - 4*a*c) ) / ( 2*a ) #&gt; [1] 0.618 (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a ) #&gt; [1] -1.62 5.3 Functions Once you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several predefined functions and most of the analysis pipelines we construct make extensive use of these. We already used the install.packages, library, and ls functions. And we used the function sqrt to solve the quadratic equation above. There are many more prebuild functions and even more can be added through packages. These functions do not appear in the workspace because you did not define them, but they are available for immediate use. In general, to evaluate a function we need to use parentheses. If you type ls the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace. Unlike ls, most functions require one or more arguments. Here is an example of how we assign an object to the argument of the function log. Remember that we defined a to be 1 above: log(8) #&gt; [1] 2.08 log(a) #&gt; [1] 0 You can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this: help(&quot;log&quot;) and for most functions we can use the shorthand: ?log The help file will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting, in the help document, that a default value is assigned with =. Defining these is optional. For example, the base of the function log defaults to base = exp(1) making log the natural log by default. If you want a quick look at the arguments without opening the help system you can type: args(log) #&gt; function (x, base = exp(1)) #&gt; NULL You can change the default values by simply assigning another object: log(8, base = 2) #&gt; [1] 3 Note that we have not been specifying the argument x as such: log(x = 8, base = 2) #&gt; [1] 3 The above code works, but we can save ourselves some typing because, if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base: log(8,2) #&gt; [1] 3 If using the arguments’ names, then we can include them in whatever order we want: log(base = 2, x = 8) #&gt; [1] 3 To specify arguments we must use =, and cannot use &lt;-. There are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example: 2 ^ 3 #&gt; [1] 8 You can see the arithmetic operators by typing: help(&quot;+&quot;) or ?&quot;+&quot; and the relational operators typing help(&quot;&gt;&quot;) or ?&quot;&gt;&quot; 5.4 Other prebuilt objects There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing: data() This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name, for example typing: co2 will show you Mauna Loa atmospheric CO2 concentration data. Other prebuilt objects are mathematical quantities such as the constant \\(\\pi\\) and \\(\\infty\\) pi #&gt; [1] 3.14 Inf+1 #&gt; [1] Inf 5.5 Variable names Here we used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R is that they have to start with a letter, can’t contain spaces and should variables that are predefined in R. For example, don’t use install.packages as a variable name and don’t type install.packages &lt;- 2! A nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations we could use something like this: solution_1 &lt;- (-b + sqrt(b^2 - 4*a*c) ) / ( 2*a ) solution_2 &lt;- (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a ) 5.6 Saving your workspace Values remain in the workspace until you end your session or erase them with the function rm. But workspaces can be saved for later use. In fact, when you quit R, the programs asks you if you want to do this. If you do, the next time you start R, the program will restore the workspace. We actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image and load to learn more. 5.7 Scripts To solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above but this time redefine the variables and recompute the solution: a &lt;- 3 b &lt;- 2 c &lt;- -1 (-b + sqrt(b^2 - 4*a*c) ) / ( 2*a ) (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a ) By creating and saving a script with the code above, we would not need to retype everything each time and instead simply change the variable names. Try writing the script above into an editor and notice how much easier it is to change the variables and receive an answer. 5.8 Comments If a line of R code starts with the symbol # it is not evaluated. We can use this to write reminders of why we wrote a particular code. For example, in the script above we could add: ## Code to compute solution to quadratic equation of the form ax^2 + bx + c ## define the variables a &lt;- 3 b &lt;- 2 c &lt;- -1 ## now compute the solution (-b + sqrt(b^2 - 4*a*c) ) / ( 2*a ) (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a ) Exercises What is the sum of the first 100 positive integers? There is a formula that tells us the sum of integers \\(1\\) through \\(n\\). It is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum? Now use the same formula to compute the sum of the integers from 1 through 1,000. Look at the result of typing the following code into R: n &lt;- 1000 x &lt;- seq(1, n) sum(x) Based on the result, what do you think the functions seq and sum do? You can use the help system: sum creates a list of numbers and seq adds them up. seq creates a list of numbers and sum adds them up. seq computes the difference between two arguments and sum computes the sum of 1 through 1000. sum always returns the same number. In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100. Which of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want. log(10^x) log10(x^10) log(exp(x)) exp(log(x, base = 2)) "],
["data-types.html", "Chapter 6 Data types 6.1 Data frames 6.2 Examining an object 6.3 The accessor 6.4 Vectors: numerics, characters, and logical 6.5 Factors 6.6 Lists Exercises", " Chapter 6 Data types Variables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have: a &lt;- 2 class(a) #&gt; [1] &quot;numeric&quot; To work efficiently in R, it is important to learn the different types of variables and what we can do with these. 6.1 Data frames Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object. We stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function: library(dslabs) data(murders) To see that this is in fact a data frame we type: class(murders) #&gt; [1] &quot;data.frame&quot; 6.2 Examining an object The function str is useful for finding out more about the structure of an object: str(murders) #&gt; &#39;data.frame&#39;: 51 obs. of 5 variables: #&gt; $ state : chr &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... #&gt; $ abb : chr &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; ... #&gt; $ region : Factor w/ 4 levels &quot;Northeast&quot;,&quot;South&quot;,..: 2 4 4 2 4 4 1 2 2 2 ... #&gt; $ population: num 4779736 710231 6392017 2915918 37253956 ... #&gt; $ total : num 135 19 232 93 1257 ... This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head: head(murders) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 In this dataset each state is considered an observation and five variables are reported for each state. Now, before we go any further in answering our original question about different states, let’s learn more about the components of this object. 6.3 The accessor For our analysis we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way: murders$population #&gt; [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 #&gt; [8] 897934 601723 19687653 9920000 1360301 1567582 12830632 #&gt; [15] 6483802 3046355 2853118 4339367 4533372 1328361 5773552 #&gt; [22] 6547629 9883640 5303925 2967297 5988927 989415 1826341 #&gt; [29] 2700551 1316470 8791894 2059179 19378102 9535483 672591 #&gt; [36] 11536504 3751351 3831074 12702379 1052567 4625364 814180 #&gt; [43] 6346105 25145561 2763885 625741 8001024 6724540 1852994 #&gt; [50] 5686986 563626 But how did we know to use population? Above, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using: names(murders) #&gt; [1] &quot;state&quot; &quot;abb&quot; &quot;region&quot; &quot;population&quot; &quot;total&quot; It is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders. Tip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio. 6.4 Vectors: numerics, characters, and logical The object murders$population is not one number, but several. We call these types of objects vectors. A single number is technically a vector, but in general we use vectors to refer to objects with several entries. The function length tells you how many entries are in the vector: pop &lt;- murders$population length(pop) #&gt; [1] 51 This particular vector is numeric since population sizes are numbers: class(pop) #&gt; [1] &quot;numeric&quot; In a numeric vector, every entry must be a number. To store character strings, vectors can also be of class character. For example, the state names are characters: class(murders$state) #&gt; [1] &quot;character&quot; As with numeric vectors, all entries in a character vector need to be a character. Another important type of vectors are logical vectors. These must be either TRUE or FALSE. z &lt;- 3 == 2 z #&gt; [1] FALSE class(z) #&gt; [1] &quot;logical&quot; Here the == is a relational operator asking if 3 is equal to 2. Remember that in R, if you just use one = you actually assign. You can see the other relational operators by typing: ?Comparison In future sections, you will see how useful relational operators can be. Advanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with as.integer(1) or add by adding an L like this: 1L. Note the class by typing: class(1L) 6.5 Factors In the murders dataset, we might expect the region to also be a character vector. However, it is not: class(murders$region) #&gt; [1] &quot;factor&quot; It is a factor. Factors are useful for storing categorical data. We can notice that there are only 4 regions by using the levels function: levels(murders$region) #&gt; [1] &quot;Northeast&quot; &quot;South&quot; &quot;North Central&quot; &quot;West&quot; In the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. Yet factors are also a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs. In general, we recommend avoiding factors as much as possible, although they are sometimes necessary to fit models containing categorical data. 6.6 Lists Data frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you: record #&gt; $name #&gt; [1] &quot;John Doe&quot; #&gt; #&gt; $student_id #&gt; [1] 1234 #&gt; #&gt; $grades #&gt; [1] 95 82 91 97 93 #&gt; #&gt; $final_grade #&gt; [1] &quot;A&quot; class(record) #&gt; [1] &quot;list&quot; We won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason we show you some basics here. As with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list. record$student_id #&gt; [1] 1234 We can also use double brackets like this: record[[&quot;student_id&quot;]] #&gt; [1] 1234 You should get used to the fact that, in R, there are several ways to do the same thing, in particular accessing entries. Exercises Load the US murders dataset. library(dslabs) data(&quot;murders&quot;) Use the function str to examine the structure of the murders object. We can see that this object is a data frame with 51 rows and five columns. Which of the following best describes the variables represented in this data frame: The 51 states. The murder rates for all 50 states and DC. The state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010. str shows no relevant information. What are the column names used by the data frame for these five variables? Use the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object? Now use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same. We saw that the regions column stores a factor. You can corroborate this by typing: class(murders$region) With one line of code, use the function levels and length to determine the number of regions defined by this dataset. The function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region. "],
["vectors.html", "Chapter 7 Vectors 7.1 Creating vectors 7.2 Names 7.3 Sequences 7.4 Subsetting 7.5 Coercion 7.6 Not Availables (NA) Exercises", " Chapter 7 Vectors In R, the most basic unit available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame each column is a vector. Here we learn more about this important class. 7.1 Creating vectors We can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way: codes &lt;- c(380, 124, 818) codes #&gt; [1] 380 124 818 We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names. country &lt;- c(&quot;italy&quot;, &quot;canada&quot;, &quot;egypt&quot;) In R you can also use single quotes: country &lt;- c(&#39;italy&#39;, &#39;canada&#39;, &#39;egypt&#39;) But be careful not to confuse the single quote ’ with the back quote `. By now you should know that if you type: country &lt;- c(italy, canada, egypt) you receive an error because the variables italy, canada and egypt are not defined: R looks for variables with those names and returns an error. 7.2 Names Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two: codes &lt;- c(italy = 380, canada = 124, egypt = 818) codes #&gt; italy canada egypt #&gt; 380 124 818 The object codes continues to be a numeric vector: class(codes) #&gt; [1] &quot;numeric&quot; but with names: names(codes) #&gt; [1] &quot;italy&quot; &quot;canada&quot; &quot;egypt&quot; If the use of strings without quotes looks confusing, know that you can use the quotes as well: codes &lt;- c(&quot;italy&quot; = 380, &quot;canada&quot; = 124, &quot;egypt&quot; = 818) codes #&gt; italy canada egypt #&gt; 380 124 818 There is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages. We can also assign names using the names functions: codes &lt;- c(380, 124, 818) country &lt;- c(&quot;italy&quot;,&quot;canada&quot;,&quot;egypt&quot;) names(codes) &lt;- country codes #&gt; italy canada egypt #&gt; 380 124 818 7.3 Sequences Another useful function for creating vectors generates sequences: seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The first argument defines the start, and the second defines the end. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by: seq(1, 10, 2) #&gt; [1] 1 3 5 7 9 If we want consecutive integers, we can use the following shorthand: 1:10 #&gt; [1] 1 2 3 4 5 6 7 8 9 10 When we use these functions, R produces integers, not numerics, because they are typically used to index something: class(1:10) #&gt; [1] &quot;integer&quot; However, as soon as we create sequence including non-integer, the class changes: class(seq(1, 10)) #&gt; [1] &quot;integer&quot; class(seq(1, 10, 0.5)) #&gt; [1] &quot;numeric&quot; 7.4 Subsetting We use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using: codes[2] #&gt; canada #&gt; 124 You can get more than one entry by using a multi-entry vector as an index: codes[c(1,3)] #&gt; italy egypt #&gt; 380 818 The sequences defined above are particularly useful if we want to access, say, the first two elements: codes[1:2] #&gt; italy canada #&gt; 380 124 If the elements have names, we can also access the entries using these names. Below are two examples. codes[&quot;canada&quot;] #&gt; canada #&gt; 124 codes[c(&quot;egypt&quot;,&quot;italy&quot;)] #&gt; egypt italy #&gt; 818 380 7.5 Coercion In general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples. We said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error: x &lt;- c(1, &quot;canada&quot;, 3) But we don’t get one, not even a warning! What happened? Look at x and its class: x #&gt; [1] &quot;1&quot; &quot;canada&quot; &quot;3&quot; class(x) #&gt; [1] &quot;character&quot; R coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings &quot;1&quot; and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R. R also offers functions to change from one type to another. For example, you can turn numbers into characters with: x &lt;- 1:5 y &lt;- as.character(x) y #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; And you can turn it back with as.numeric: as.numeric(y) #&gt; [1] 1 2 3 4 5 This function is actually quite useful since datasets that include numbers as character strings are common. 7.6 Not Availables (NA) When a function tries to coerce one type to another and encounters an impossible case, it usually gives us a warning and turns the entry into a special value called an NA for “not available”. For example: x &lt;- c(&quot;1&quot;, &quot;b&quot;, &quot;3&quot;) as.numeric(x) #&gt; Warning: NAs introduced by coercion #&gt; [1] 1 NA 3 R does not have any guesses for what number you want when you type b so it does not try. As a data scientist, you will encounter the NA often as they are generally used for missing data, a common problem in real life datasets. Exercises Use the function c to create a vector with the average high temperatures in January for Beijing, Lagos, Paris, Rio de Janeiro, San Juan, and Toronto, which are 35, 88, 42, 84, 81, and 30 degrees Fahrenheit. Call the object temp. Now create a vector with the city names and call the object city. Use the names function and the objects defined in the previous exercises to associate the temperature data with its corresponding city. Use the [ and : operators to access the temperature of the first three cities on the list. Use the [ operator to access the temperature of Paris and San Juan. Use the : operator to create a sequence of numbers \\(12,13,14,\\dots,73\\). Create a vector containing all the positive odd numbers smaller than 100. Create a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6+4/7, 6+8/7, etc.. How many numbers does the list have? Hint: use seq and length. What is the class of the following object a &lt;- seq(1, 10, 0.5)? What is the class of the following object a &lt;- seq(1, 10)? The class of class(a&lt;-1) is numeric not integer. R defaults to numeric and to force an integer, you need to add the letter L. Confirm that the class of 1L is integer. Define the following vector: x &lt;- c(&quot;1&quot;, &quot;3&quot;, &quot;5&quot;) and coerce it to get integers. "],
["sorting.html", "Chapter 8 Sorting 8.1 sort 8.2 order 8.3 max and which.max 8.4 rank 8.5 Beware of recycling Exercise", " Chapter 8 Sorting Now that we have mastered some basic R knowledge, let’s try to gain some insights into the safety of different states in the context of gun murders. 8.1 sort Say we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing: library(dslabs) data(murders) sort(murders$total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 #&gt; [15] 32 36 38 53 63 65 67 84 93 93 97 97 99 111 #&gt; [29] 116 118 120 135 142 207 219 232 246 250 286 293 310 321 #&gt; [43] 351 364 376 413 457 517 669 805 1257 However, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257 murders in 2010. 8.2 order The function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it: x &lt;- c(31, 4, 15, 92, 65) sort(x) #&gt; [1] 4 15 31 65 92 Rather than sort the input vector, the function order returns the index that sorts input vector: index &lt;- order(x) x[index] #&gt; [1] 4 15 31 65 92 This is the same output as that returned by sort(x). If we look at this index, we see why it works: x #&gt; [1] 31 4 15 92 65 order(x) #&gt; [1] 2 3 1 5 4 The second and fourth entry of x are the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on. How does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. So, for example, these two vectors, containing the state names and abbreviations respectively, are matched by their order: murders$state[1:10] #&gt; [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; #&gt; [4] &quot;Arkansas&quot; &quot;California&quot; &quot;Colorado&quot; #&gt; [7] &quot;Connecticut&quot; &quot;Delaware&quot; &quot;District of Columbia&quot; #&gt; [10] &quot;Florida&quot; murders$abb[1:10] #&gt; [1] &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; &quot;CA&quot; &quot;CO&quot; &quot;CT&quot; &quot;DE&quot; &quot;DC&quot; &quot;FL&quot; This means we can now order the state names by their total murders by, first, obtaining the index that orders the vectors according to murder totals and then indexing the state names or abbreviation vector: ind &lt;- order(murders$total) murders$abb[ind] #&gt; [1] &quot;VT&quot; &quot;ND&quot; &quot;NH&quot; &quot;WY&quot; &quot;HI&quot; &quot;SD&quot; &quot;ME&quot; &quot;ID&quot; &quot;MT&quot; &quot;RI&quot; &quot;AK&quot; &quot;IA&quot; &quot;UT&quot; &quot;WV&quot; #&gt; [15] &quot;NE&quot; &quot;OR&quot; &quot;DE&quot; &quot;MN&quot; &quot;KS&quot; &quot;CO&quot; &quot;NM&quot; &quot;NV&quot; &quot;AR&quot; &quot;WA&quot; &quot;CT&quot; &quot;WI&quot; &quot;DC&quot; &quot;OK&quot; #&gt; [29] &quot;KY&quot; &quot;MA&quot; &quot;MS&quot; &quot;AL&quot; &quot;IN&quot; &quot;SC&quot; &quot;TN&quot; &quot;AZ&quot; &quot;NJ&quot; &quot;VA&quot; &quot;NC&quot; &quot;MD&quot; &quot;OH&quot; &quot;MO&quot; #&gt; [43] &quot;LA&quot; &quot;IL&quot; &quot;GA&quot; &quot;MI&quot; &quot;PA&quot; &quot;NY&quot; &quot;FL&quot; &quot;TX&quot; &quot;CA&quot; According to the above, California had the most murders. 8.3 max and which.max If we are only interested in the entry with the largest value, we can use max for the value: max(murders$total) #&gt; [1] 1257 and which.max for the index of the largest value: i_max &lt;- which.max(murders$total) murders$state[i_max] #&gt; [1] &quot;California&quot; For the minimum, we can use min and which.min in the same way. So is California the most dangerous state? In an upcoming section we argue that we should be considering rates not totals. Before doing that, we introduce one last order related function: rank 8.4 rank Although not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example: x &lt;- c(31, 4, 15, 92, 65) rank(x) #&gt; [1] 3 1 2 5 4 To summarize, let’s look at the results of the three functions we have introduced: original sort order rank 31 4 2 3 4 15 3 1 15 31 1 2 92 65 5 5 65 92 4 4 8.5 Beware of recycling Another common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens: x &lt;- c(1,2,3) y &lt;- c(10, 20, 30, 40, 50, 60, 70) x+y #&gt; Warning in x + y: longer object length is not a multiple of shorter object #&gt; length #&gt; [1] 11 22 33 41 52 63 71 We do get a warning but no error. For the output, R has recycled the numbers in x: notice the last digit of numbers in the output. Exercise For these exercises we will use the US murders dataset. Make sure you load it. library(dslabs) data(murders) Use the $ operator to access the population size data and store it the object pop. Then use the sort function to redefine pop so that it is sorted. Finally use the [ operator to report the smallest population size. Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort. We can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this. Now we know how small the smallest state is and we know which row represents it. However, which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population. You can create a data frame using the data.frame function. Here is a quick example: temp &lt;- c(35, 88, 42, 84, 81, 30) city &lt;- c(&quot;Beijing&quot;, &quot;Lagos&quot;, &quot;Paris&quot;, &quot;Rio de Janeiro&quot;, &quot;San Juan&quot;, &quot;Toronto&quot;) city_temps &lt;- data.frame(name = city, temperature = temp) Use the rank function to determine the population size rank (from smallest to biggest) of each state. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df. Repeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame. The na_example represents a series of counts. You can quickly examine the object using: data(na_example) str(na_example) #&gt; int [1:1000] 2 1 3 2 1 3 1 4 3 2 ... However, when we compute the average we obtain an NA: mean(na_example) #&gt; [1] NA The is.na returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have. Now compute the average again, but only for the entries that are not NA. Hint: remember the ! operator. "],
["vector-arithmetics.html", "Chapter 9 Vector arithmetics 9.1 Rescaling 9.2 Two vectors Exercises", " Chapter 9 Vector arithmetics California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can very quickly confirm that, indeed, California has the largest population: murders$state[which.max(murders$population)] #&gt; [1] &quot;California&quot; with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy. 9.1 Rescaling In R, arithmetic operations on vectors occur element wise. For a quick example, suppose we have height in inches: heights &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70) and want to covert to centimeters. Notice what happens when we multiply heights by 2.54: heights * 2.54 #&gt; [1] 175 157 168 178 178 185 170 185 170 178 It multiplied each element by 2.54. Similarly, if we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this: heights - 69 #&gt; [1] 0 -7 -3 1 1 4 -2 4 -2 1 9.2 Two vectors If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows: \\[ \\begin{pmatrix} a\\\\ b\\\\ c\\\\ d \\end{pmatrix} + \\begin{pmatrix} e\\\\ f\\\\ g\\\\ h \\end{pmatrix} = \\begin{pmatrix} a +e\\\\ b + f\\\\ c + g\\\\ d + h \\end{pmatrix} \\] The same holds for other mathematical operations, such as -, * and /. This implies that to compute the murder rates we can simply type: murder_rate &lt;- murders$total / murders$population * 100000 Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate: murders$state[order(murder_rate)] #&gt; [1] &quot;Vermont&quot; &quot;New Hampshire&quot; &quot;Hawaii&quot; #&gt; [4] &quot;North Dakota&quot; &quot;Iowa&quot; &quot;Idaho&quot; #&gt; [7] &quot;Utah&quot; &quot;Maine&quot; &quot;Wyoming&quot; #&gt; [10] &quot;Oregon&quot; &quot;South Dakota&quot; &quot;Minnesota&quot; #&gt; [13] &quot;Montana&quot; &quot;Colorado&quot; &quot;Washington&quot; #&gt; [16] &quot;West Virginia&quot; &quot;Rhode Island&quot; &quot;Wisconsin&quot; #&gt; [19] &quot;Nebraska&quot; &quot;Massachusetts&quot; &quot;Indiana&quot; #&gt; [22] &quot;Kansas&quot; &quot;New York&quot; &quot;Kentucky&quot; #&gt; [25] &quot;Alaska&quot; &quot;Ohio&quot; &quot;Connecticut&quot; #&gt; [28] &quot;New Jersey&quot; &quot;Alabama&quot; &quot;Illinois&quot; #&gt; [31] &quot;Oklahoma&quot; &quot;North Carolina&quot; &quot;Nevada&quot; #&gt; [34] &quot;Virginia&quot; &quot;Arkansas&quot; &quot;Texas&quot; #&gt; [37] &quot;New Mexico&quot; &quot;California&quot; &quot;Florida&quot; #&gt; [40] &quot;Tennessee&quot; &quot;Pennsylvania&quot; &quot;Arizona&quot; #&gt; [43] &quot;Georgia&quot; &quot;Mississippi&quot; &quot;Michigan&quot; #&gt; [46] &quot;Delaware&quot; &quot;South Carolina&quot; &quot;Maryland&quot; #&gt; [49] &quot;Missouri&quot; &quot;Louisiana&quot; &quot;District of Columbia&quot; Exercises Previously we created this data frame: temp &lt;- c(35, 88, 42, 84, 81, 30) city &lt;- c(&quot;Beijing&quot;, &quot;Lagos&quot;, &quot;Paris&quot;, &quot;Rio de Janeiro&quot;, &quot;San Juan&quot;, &quot;Toronto&quot;) city_temps &lt;- data.frame(name = city, temperature = temp) Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. What is the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler we know it should be close to \\(\\pi^2/6\\). Compute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average? "],
["indexing.html", "Chapter 10 Indexing 10.1 Subsetting with logicals 10.2 Logical operators 10.3 which 10.4 match 10.5 %in% Exercises", " Chapter 10 Indexing R provides a powerful and convenient way of indexing vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example. 10.1 Subsetting with logicals We have now calculated the murder rate using: murder_rate &lt;- murders$total / murders$population * 100000 Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above: ind &lt;- murder_rate &lt; 0.71 Or if we want to know if its less or equal we can use: ind &lt;- murder_rate &lt;= 0.71 ind #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [12] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; [34] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [45] FALSE TRUE FALSE FALSE FALSE FALSE FALSE Note that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals. murders$state[ind] #&gt; [1] &quot;Hawaii&quot; &quot;Iowa&quot; &quot;New Hampshire&quot; &quot;North Dakota&quot; #&gt; [5] &quot;Vermont&quot; In order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using: sum(ind) #&gt; [1] 5 10.2 Logical operators Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with &amp;. This operation results in TRUE, only when both logicals are TRUE. To see this consider this example: TRUE &amp; TRUE #&gt; [1] TRUE TRUE &amp; FALSE #&gt; [1] FALSE FALSE &amp; FALSE #&gt; [1] FALSE For our example, we can form two logicals: west &lt;- murders$region == &quot;West&quot; safe &lt;- murder_rate &lt;= 1 and we can use the &amp; to get a vector of logicals that tells us which states satisfy both conditions: ind &lt;- safe &amp; west murders$state[ind] #&gt; [1] &quot;Hawaii&quot; &quot;Idaho&quot; &quot;Oregon&quot; &quot;Utah&quot; &quot;Wyoming&quot; 10.3 which Suppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type: ind &lt;- which(murders$state == &quot;California&quot;) ind #this is the index that matches the California entry #&gt; [1] 5 murder_rate[ind] #&gt; [1] 3.37 10.4 match If instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector: ind &lt;- match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) ind #&gt; [1] 33 10 44 Now we can look at the murder rates: murder_rate[ind] #&gt; [1] 2.67 3.40 3.20 10.5 %in% If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota and Washington are states, you can find out like this: c(&quot;Boston&quot;, &quot;Dakota&quot;, &quot;Washington&quot;) %in% murders$state #&gt; [1] FALSE FALSE TRUE We will be using %in% often throughout the book. Advanced: There is a connection between match and %in% through which. To see this, notice that the following two lines of code are equivalent: match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) #&gt; [1] 33 10 44 which(murders$state%in%c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;)) #&gt; [1] 10 33 44 Exercises Start by loading the library and data. library(dslabs) data(murders) Compute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use the logical operators to create a logical vector, name it low; that tells us which entries of murder_rate are lower than 1. Now use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1. Use the results from the previous exercise to report the names of the states with murder rates lower than 1. Now extend the code from exercise 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator &amp;. In a previous exercise we computed the murder rate for each rate and the average of these numbers. How many states are below the average? Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states. Use the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU ? Extend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index. "],
["basic-data-wrangling.html", "Chapter 11 Basic data wrangling 11.1 Adding a column with mutate 11.2 Subsetting with filter 11.3 Selecting columns with select 11.4 The pipe: %&gt;% 11.5 Creating a data frame Exercises", " Chapter 11 Basic data wrangling Up to now we have been changing vectors by reordering them and subsetting them through indexing. However, once we start more advanced analyses, we will want to prepare data tables for data analysis. We refer to this task as data wrangling. For this purpose, we will introduce the dplyr package which provides intuitive functionality for working with tables. In Chapter 6, we will cover the dplyr package in much more depth. Once you install dplyr you can load it using: library(dplyr) This package introduces functions that perform the most common operations in data wrangling and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select. We can also perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %&gt;%. Some details are included below. 11.1 Adding a column with mutate We want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our data frame. The function mutate takes the data frame as a first argument and the name and values of the variable in the second using the convention name = values. So to add murder rates we use: murders &lt;- mutate(murders, rate = total / population * 100000) Notice that here we used total and population inside the function, which are objects that are not defined in our workspace. So why do we not get an error? This is one of the main features of dplyr. Function in this package, such mutate, know to look for variables in the data frame provided in the first argument. So in the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable. We can see that the new column is added: head(murders) #&gt; state abb region population total rate #&gt; 1 Alabama AL South 4779736 135 2.82 #&gt; 2 Alaska AK West 710231 19 2.68 #&gt; 3 Arizona AZ West 6392017 232 3.63 #&gt; 4 Arkansas AR South 2915918 93 3.19 #&gt; 5 California CA West 37253956 1257 3.37 #&gt; 6 Colorado CO West 5029196 65 1.29 Although we have over-written the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will over-write our mutated version. 11.2 Subsetting with filter Now suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this, we use the filter function which takes the data table as an argument and then the conditional statement as the next. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace. filter(murders, rate &lt;= 0.71) #&gt; state abb region population total rate #&gt; 1 Hawaii HI West 1360301 7 0.515 #&gt; 2 Iowa IA North Central 3046355 21 0.689 #&gt; 3 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Vermont VT Northeast 625741 2 0.320 11.3 Selecting columns with select Although our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object: new_table &lt;- select(murders, state, region, rate) filter(new_table, rate &lt;= 0.71) #&gt; state region rate #&gt; 1 Hawaii West 0.515 #&gt; 2 Iowa North Central 0.689 #&gt; 3 New Hampshire Northeast 0.380 #&gt; 4 North Dakota North Central 0.595 #&gt; 5 Vermont Northeast 0.320 In the call to select, the first argument, murders, is an object but state, region, and rate are variable names. 11.4 The pipe: %&gt;% We wrote the code above because we wanted to show the three variables for states that have murder rates below 0.71. To do this we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do: \\[ \\mbox{original data } \\rightarrow \\mbox{ select } \\rightarrow \\mbox{ filter } \\] For such an operation, we can use the pipe %&gt;%. The code looks like this: murders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71) #&gt; state region rate #&gt; 1 Hawaii West 0.515 #&gt; 2 Iowa North Central 0.689 #&gt; 3 New Hampshire Northeast 0.380 #&gt; 4 North Dakota North Central 0.595 #&gt; 5 Vermont Northeast 0.320 This line of code is equivalent to the two lines of code above. What is going on here? In general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example: 16 %&gt;% sqrt() #&gt; [1] 4 We can continue to pipe values along: 16 %&gt;% sqrt() %&gt;% log2() #&gt; [1] 2 The above statement is equivalent to log2(sqrt(16)). Remember that the pipe sends values to the first argument so we can define arguments as if the first argument is already defined: 16 %&gt;% sqrt() %&gt;% log(base = 2) #&gt; [1] 2 Therefore when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote: murders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71) murders is the first argument of the select function and the new data frame, formerly new_table, is the first argument of the filter function. 11.5 Creating a data frame It is sometimes useful for us to create our own data frames. You can do this by using the data.frame function: grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90)) grades #&gt; names exam_1 exam_2 #&gt; 1 John 95 90 #&gt; 2 Juan 80 85 #&gt; 3 Jean 90 85 #&gt; 4 Yao 85 90 Warning: By default the function data.frame coerces characters into factors: class(grades$names) #&gt; [1] &quot;factor&quot; To avoid this we use the rather cumbersome argument stringsAsFactors: grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90), stringsAsFactors = FALSE) class(grades$names) #&gt; [1] &quot;character&quot; Exercises Load the dplyr package and the murders dataset. library(dplyr) library(dslabs) data(murders) You can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted. Like this: murders &lt;- mutate(murders, population_in_millions = population / 10^6) We can write population rather than murders$population. The function mutate knows we are grabing columns from murders. Use the function mutate to add a murders column named rate with the per 100,000 murder rate. Make sure you redefine murders as done in the example code above and remember the murder rate is defined by the total divided by the population size times 100,000. If rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders. With dplyr we can use select to show only certain columns. For example, with this code we would only show the states and population sizes: select(murders, state, population) %&gt;% head() #&gt; state population #&gt; 1 Alabama 4779736 #&gt; 2 Alaska 710231 #&gt; 3 Arizona 6392017 #&gt; 4 Arkansas 2915918 #&gt; 5 California 37253956 #&gt; 6 Colorado 5029196 Use select to show the state names and abbreviations in murders. Just show it, do not define a new object. The dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this: filter(murders, state == &quot;New York&quot;) #&gt; state abb region population total rate #&gt; 1 New York NY Northeast 19378102 517 2.67 You can use other logical vector to filter rows. Use filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column. We can remove rows using the != operator. For example, to remove Florida we would do this: no_florida &lt;- filter(murders, state != &quot;Florida&quot;) Create a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this. We can also use the %in% to filter with dplyr. You can thus see the data from New York and Texas like this: filter(murders, state %in% c(&quot;New York&quot;, &quot;Texas&quot;)) #&gt; state abb region population total rate #&gt; 1 New York NY Northeast 19378102 517 2.67 #&gt; 2 Texas TX South 25145561 805 3.20 Create a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category? Suppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter: filter(murders, population &lt; 5000000 &amp; region == &quot;Northeast&quot;) #&gt; state abb region population total rate #&gt; 1 Connecticut CT Northeast 3574097 97 2.714 #&gt; 2 Maine ME Northeast 1328361 11 0.828 #&gt; 3 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 4 Rhode Island RI Northeast 1052567 16 1.520 #&gt; 5 Vermont VT Northeast 625741 2 0.320 Add a murder rate column and a rank column as done before. Create a table, call it my_states, that satisfies both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate and the rank. library(dplyr) library(dslabs) data(murders) The pipe %&gt;% can be used to perform operations sequentially without having to define intermediate objects. After redefining murder to include rate and rank. library(dplyr) murders &lt;- mutate(murders, rate = total / population * 100000, rank = rank(-rate)) in the solution to the previous exercise we did the following: # Created a table my_states &lt;- filter(murders, region %in% c(&quot;Northeast&quot;, &quot;West&quot;) &amp; rate &lt; 1) # Used select to show only the state name, the murder rate and the rank select(my_states, state, rate, rank) #&gt; state rate rank #&gt; 1 Hawaii 0.515 49 #&gt; 2 Idaho 0.766 46 #&gt; 3 Maine 0.828 44 #&gt; 4 New Hampshire 0.380 50 #&gt; 5 Oregon 0.940 42 #&gt; 6 Utah 0.796 45 #&gt; 7 Vermont 0.320 51 #&gt; 8 Wyoming 0.887 43 The pipe %&gt;% permits us to perform both operation sequentially and without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this: mutate(murders, rate = total / population * 100000, rank = rank(-rate)) %&gt;% select(state, rate, rank) #&gt; state rate rank #&gt; 1 Alabama 2.824 23 #&gt; 2 Alaska 2.675 27 #&gt; 3 Arizona 3.630 10 #&gt; 4 Arkansas 3.189 17 #&gt; 5 California 3.374 14 #&gt; 6 Colorado 1.292 38 #&gt; 7 Connecticut 2.714 25 #&gt; 8 Delaware 4.232 6 #&gt; 9 District of Columbia 16.453 1 #&gt; 10 Florida 3.398 13 #&gt; 11 Georgia 3.790 9 #&gt; 12 Hawaii 0.515 49 #&gt; 13 Idaho 0.766 46 #&gt; 14 Illinois 2.837 22 #&gt; 15 Indiana 2.190 31 #&gt; 16 Iowa 0.689 47 #&gt; 17 Kansas 2.208 30 #&gt; 18 Kentucky 2.673 28 #&gt; 19 Louisiana 7.743 2 #&gt; 20 Maine 0.828 44 #&gt; 21 Maryland 5.075 4 #&gt; 22 Massachusetts 1.802 32 #&gt; 23 Michigan 4.179 7 #&gt; 24 Minnesota 0.999 40 #&gt; 25 Mississippi 4.044 8 #&gt; 26 Missouri 5.360 3 #&gt; 27 Montana 1.213 39 #&gt; 28 Nebraska 1.752 33 #&gt; 29 Nevada 3.110 19 #&gt; 30 New Hampshire 0.380 50 #&gt; 31 New Jersey 2.798 24 #&gt; 32 New Mexico 3.254 15 #&gt; 33 New York 2.668 29 #&gt; 34 North Carolina 2.999 20 #&gt; 35 North Dakota 0.595 48 #&gt; 36 Ohio 2.687 26 #&gt; 37 Oklahoma 2.959 21 #&gt; 38 Oregon 0.940 42 #&gt; 39 Pennsylvania 3.598 11 #&gt; 40 Rhode Island 1.520 35 #&gt; 41 South Carolina 4.475 5 #&gt; 42 South Dakota 0.983 41 #&gt; 43 Tennessee 3.451 12 #&gt; 44 Texas 3.201 16 #&gt; 45 Utah 0.796 45 #&gt; 46 Vermont 0.320 51 #&gt; 47 Virginia 3.125 18 #&gt; 48 Washington 1.383 37 #&gt; 49 West Virginia 1.457 36 #&gt; 50 Wisconsin 1.706 34 #&gt; 51 Wyoming 0.887 43 Notice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %&gt;% Repeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %&gt;% to do this in just one line. Now we will make murders the original table one gets when loading using data(murders). Use just one line to create a new data frame, called my_states, that has a murder rate and a rank column, considers only states in the Northeast or West, which have a murder rate lower than 1, and contains only the state, rate, and rank columns. The line should have four components separated by three %&gt;%. The original dataset murders. A call to mutate to add the murder rate and the rank. A call to filter to keep only the states from the Northeast or West and that have a murder rate below 1. A call to select that keeps only the columns with the state name, the murder rate and the rank. The line should look something like this my_states &lt;- murders %&gt;% mutate something %&gt;% filter something %&gt;% select something. "],
["basic-plots.html", "Chapter 12 Basic plots 12.1 Scatterplots 12.2 Histograms 12.3 Boxplot Exercises", " Chapter 12 Basic plots Exploratory data visualization is perhaps the strength of R. One can quickly go from idea to data to plot with a unique balance of flexibility and ease. For example, Excel may be easier than R, but it is nowhere near as flexible. D3.js may be more flexible and powerful than R, but it takes much longer to generate a plot. Before we focus on data visualization in the next chapter, we first we want to introduce some very basic plotting functions. 12.1 Scatterplots Earlier we inferred that states with larger populations are likely to have more murders. This can be confirmed with an exploratory visualization that plots these two quantities against each other: population_in_millions &lt;- murders$population/10^6 total_gun_murders &lt;- murders$total plot(population_in_millions, total_gun_murders) We can clearly see a relationship. Advanced: For a quick plot that avoids accessing variables twice, we can use the with function: with(murders, plot(population, total)) 12.2 Histograms We will describe histograms as they relate to distribution in the next chapter. Here we will simply note that histograms are a powerful graphical summary of a list of numbers that gives you a general overview of the types of values you have. We can make a histogram of our murder rates by simply typing: hist(murders$rate) We can see that there is a wide range of values with most of them between 2 and 3 and one very extreme case with a murder rate of more than 15: murders$state[which.max(murders$rate)] #&gt; [1] &quot;District of Columbia&quot; 12.3 Boxplot Boxplots will be described in more detail in the next chapter as well. Boxplots provide a more terse summary than histograms, but they are easier to stack with other boxplots. For example, here we can use them to compare the different regions: boxplot(rate~region, data = murders) We can see that the South has higher murder rates than the other three regions. Exercises We made a plot of total murders versus population and noted a strong relationship: not surprisingly, states with larger populations had more murders. library(dslabs) data(murders) population_in_millions &lt;- murders$population/10^6 total_gun_murders &lt;- murders$total plot(population_in_millions, total_gun_murders) Keep in mind that many states have populations below 5 million and are bunched up. We may gain further insights from making this plot in the log scale. Transform the variables using the log10 transformation and then plot them. Create a histogram of the state populations. Generate boxplots of the state populations by region. "],
["importing-data.html", "Chapter 13 Importing data 13.1 read.csv", " Chapter 13 Importing data In this chapter, we used a data set already stored in an R object. A data scientist will rarely have such luck and will have to import data into R from either a file, a database, or other source. We cover this in more detail in Chapter 7. But because it is so common to read data from a file, we will briefly describe the key approach and function, in case you want to use your new knowledge on one of your own datasets. Small datasets such as the one used in this chapter are commonly stored as Excel files. Although there are R packages designed to read Excel (xls) format, you generally want to avoid this format and save files as comma delimited (Comma-Separated Value/CSV) or tab delimited (Tab-Separated Value/TSV/TXT) files. These plain-text formats make it easier to share data since commercial software is not required for working with the data. Paths and the working directory The first step is to find the file containing your data and know its path. When you are working in R, it is useful to know your working directory. This is the folder in which R will save or look for files by default. You can see your working directory by typing: getwd() You can also change your working directory using the function setwd or you can change it through RStudio by clicking on “Session”. The functions that read and write files (there are several in R) assume you mean to look for files or write files in the working directory. Our recommended approach for beginners will have you reading and writing to the working directory. However, you can also type the full path, which will work independently of the working directory. We have included the US murders data in CSV file as part of the dslabs package. We recommend placing your data in your working directory. Because knowing where packages store files is rather advanced, we provide the following code that finds the directory and copies the file: dir &lt;- system.file(package=&quot;dslabs&quot;) #extracts the location of package filename &lt;- file.path(dir,&quot;extdata/murders.csv&quot;) file.copy(filename, &quot;murders.csv&quot;) #&gt; [1] TRUE You should be able to see the file in your working directory and can check using: list.files() #&gt; [1] &quot;_bookdown_files&quot; &quot;_bookdown.yml&quot; &quot;_common.R&quot; #&gt; [4] &quot;_output.yml&quot; &quot;book_cache&quot; &quot;book_files&quot; #&gt; [7] &quot;book.bib&quot; &quot;book.Rmd&quot; &quot;dataviz&quot; #&gt; [10] &quot;docs&quot; &quot;dsbook.Rproj&quot; &quot;index.Rmd&quot; #&gt; [13] &quot;inference&quot; &quot;intro.Rmd&quot; &quot;LICENSE&quot; #&gt; [16] &quot;murders.csv&quot; &quot;packages.bib&quot; &quot;preamble.tex&quot; #&gt; [19] &quot;prob&quot; &quot;R&quot; &quot;README.md&quot; #&gt; [22] &quot;style.css&quot; 13.1 read.csv We are ready to read in the file. There are several functions for reading in tables. Here we introduce one included in base R: dat &lt;- read.csv(&quot;murders.csv&quot;) head(dat) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 We can see that we have read in the file. Warning: read.csv automatically converts characters to factors. Note for example that: class(dat$state) #&gt; [1] &quot;factor&quot; You can avoid this using: dat &lt;- read.csv(&quot;murders.csv&quot;, stringsAsFactors = FALSE) class(dat$state) #&gt; [1] &quot;character&quot; With this call the region variable is no longer a factor, but we can easily change this with: dat &lt;- mutate(dat, region = as.factor(region)) Now that we are done with this example, we remove the murders.csv file from our working directory file.remove(&quot;murders.csv&quot;) #&gt; [1] TRUE "],
["programming-basics.html", "Chapter 14 Programming basics 14.1 Conditionals expressions 14.2 Defining functions 14.3 For-loops 14.4 Vectorization and functionals 14.5 map 14.6 Other functions Exercises", " Chapter 14 Programming basics We teach R because it greatly facilitates data analysis, the main topic of this book. By coding in R, we can efficiently perform exploratory data analysis, build data analysis pipelines and prepare data visualization to communicate results. However, R is not just a data analysis environment but a programming language. Advanced R programmers can develop complex packages and even improve R itself, but we do not cover advanced programming in this book. Nonetheless, in this section we introduce three key programming concepts: conditional expressions, for-loops and functions. These are not just key building blocks for advanced programming, but are sometimes useful during data analysis. We also provide a list of powerful functions that we do not cover in the book, but are worth exploring since they are powerful tools commonly used by expert data analysts. 14.1 Conditionals expressions Conditional expressions are one of the basic features of programming. They are used for what is called flow control. The most common conditional expression is the if-else statement. In R, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally and once you start writing your own functions and packages, you will need them. Here is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of a unless a is 0: a &lt;- 0 if(a!=0){ print(1/a) } else{ print(&quot;No reciprocal for 0.&quot;) } #&gt; [1] &quot;No reciprocal for 0.&quot; Let’s look at one more example using the US murders data frame: library(dslabs) data(murders) murder_rate &lt;- murders$total/murders$population*100000 Here is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The if statement protects us from the case in which no state satisfies the condition. ind &lt;- which.min(murder_rate) if(murder_rate[ind] &lt; 0.5){ print(murders$state[ind]) } else{ print(&quot;No state has murder rate that low&quot;) } #&gt; [1] &quot;Vermont&quot; If we try it again with a rate of 0.25, we get a different answer: if(murder_rate[ind] &lt; 0.25){ print(murders$state[ind]) } else{ print(&quot;No state has a murder rate that low.&quot;) } #&gt; [1] &quot;No state has a murder rate that low.&quot; A related function that is very useful is ifelse. This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example: a &lt;- 0 ifelse(a &gt; 0, 1/a, NA) #&gt; [1] NA The function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE. a &lt;- c(0,1,2,-4,5) result &lt;- ifelse(a &gt; 0, 1/a, NA) This table helps us see what happened: a is_a_positive answer1 answer2 result 0 FALSE Inf NA NA 1 TRUE 1.00 NA 1.0 2 TRUE 0.50 NA 0.5 -4 FALSE -0.25 NA NA 5 TRUE 0.20 NA 0.2 Here is an example of how this function can be readily used to replace all the missing values in a vector with zeros: data(na_example) no_nas &lt;- ifelse(is.na(na_example), 0, na_example) sum(is.na(no_nas)) #&gt; [1] 0 Two other useful functions are any and all. The any function takes a vector of logicals and returns TRUE, if any of the entries is TRUE. The all function takes a vector of logicals and returns TRUE, if all of the entries are TRUE. Here is an example: z &lt;- c(TRUE, TRUE, FALSE) any(z) #&gt; [1] TRUE all(z) #&gt; [1] FALSE 14.2 Defining functions As you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector x using the sum and length functions: sum(x)/length(x). But because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the mean function and it is included in base R. However, you will encounter situations in which the function does not already exist, so R permits you to write your own. A simple version of a function that computes the average can be defined like this: avg &lt;- function(x){ s &lt;- sum(x) n &lt;- length(x) s/n } Now avg is a function that computes the mean: x &lt;- 1:100 identical(mean(x), avg(x)) #&gt; [1] TRUE Notice that variables defined inside a function are not saved in the workspace. So while we use s and n when we call avg, the values are created and changed only during the call. Here is an illustrative example: s &lt;- 3 avg(1:10) #&gt; [1] 5.5 s #&gt; [1] 3 Note how s is still 3 after we call avg. In general, functions are objects, so we assign them to variable names with &lt;-. The function function tells R you are about to define a function. The general form of a function definition looks like this: my_function &lt;- function(VARIABLE_NAME){ perform operations on VARIABLE_NAME and calculate VALUE VALUE } The functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this: avg &lt;- function(x, arithmetic = TRUE){ n &lt;- length(x) ifelse(arithmetic, sum(x)/n, prod(x)^(1/n)) } We will learn more about how to create functions through experience as we face more complex tasks. 14.3 For-loops The formula for the sum \\(1+2+\\dots+n\\) is \\(n(n+1)/2\\). What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the \\(S_n\\): compute_s_n &lt;- function(n){ x &lt;- 1:n sum(x) } Now if we can compute \\(S_n\\) for various values of \\(n\\), say \\(n=1,\\dots,25\\), how do we do it? Do we write 25 lines of code calling compute_s_n? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over and the only thing that is changing is the value of \\(n\\). for-loops let us define the range that our variable takes (in our example \\(n=1,\\dots,10\\)), then change the value as you loop and evaluate expression as you loop. Perhaps the simplest example of a for-loop is this useless piece of code: for(i in 1:5){ print(i) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 And here is the for-loop we would write for our \\(S_n\\) example: m &lt;- 25 s_n &lt;- vector(length = m) # create an empty vector for(n in 1:m){ s_n[n] &lt;- compute_s_n(n) } In each iteration \\(n=1\\), \\(n=2\\), etc…, we compute \\(S_n\\) and store it in the \\(n\\)th entry of s_n. Now we can create a plot to search for a pattern: n &lt;- 1:m plot(n, s_n) If you noticed that it appears to be a quadratic, you are on the right track because the formula is \\(n(n+1)/2\\), which we can confirm with a table: #&gt; s_n formula #&gt; 1 1 1 #&gt; 2 3 3 #&gt; 3 6 6 #&gt; 4 10 10 #&gt; 5 15 15 #&gt; 6 21 21 We can also overlay the two results by using the function lines to draw a line over the previously plotted points: plot(n, s_n) lines(n, n*(n+1)/2) 14.4 Vectorization and functionals Although for-loops are an important concept to understand, in R we rarely use them. As you learn more R, you will realize that vectorization is the preferred over for-loops since it results in shorter and clearer code. We already saw examples in the Vector Arithmetic Section. A vectorized function is a function that will apply the same operation on each of the vectors. x &lt;- 1:10 sqrt(x) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 y &lt;- 1:10 x*y #&gt; [1] 1 4 9 16 25 36 49 64 81 100 To make this calculation, there is no need for for-loops. But not all functions work this way. For instance, the function we just wrote, compute_s_n, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of n: n &lt;- 1:25 compute_s_n(n) Functionals are functions that help us apply the same function to each entry in a vector, matrix, data frame or list. Here we cover the functional that operates on numeric, logical and character vectors: sapply. The function sapply permits us to perform element-wise operations on any function. Here is how it works: x &lt;- 1:10 sapply(x, sqrt) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 Here each element of x is passed on to the function sqrt and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original x. This implies that the for-loop above can be written as follows: n &lt;- 1:25 s_n &lt;- sapply(n, compute_s_n) plot(n, s_n) Other functionals are apply, lapply, tapply, mapply,vapply, and replicate. We only use sapply and replicate in this book, but we recommend familiarizing yourselves with the others as they can be very useful. 14.5 map Another series of functionals are provided by the purrr package. These are similar to sapply, but, in some instances, they are somewhat easier to use for beginners. The general idea is the same library(purrr) n &lt;- 1:25 s_n &lt;- map(n, compute_s_n) One advantage that becomes clear latter is that with the purrr functionals you know what type of object you will get back. This is different with sapply that can return several different types. The map function always returns a list. class(s_n) #&gt; [1] &quot;list&quot; If we want a numeric vector we can instead use: s_n &lt;- map_dbl(n, compute_s_n) plot(n, s_n) We will learn other advantages of map in later chapters. 14.6 Other functions Other functions that are widely used, but that we will not cover in this book are split, cut, do.call and Reduce. Exercises What will this conditional expression return? x &lt;- c(1,2,-3,4) if(all(x&gt;0)){ print(&quot;A ll Postives&quot;) } else{ print(&quot;Not all positives&quot;) } #&gt; [1] &quot;Not all positives&quot; Which of the following expressions is always FALSE when at least one entry of a logical vector x is TRUE? A. all(x) B. any(x) C. any(!x) D. all(!x) The function nchar tells you how many characters long a character vector is. Write a line of code that assigns to the object new_names the state abbreviation when the state name is longer than 8 characters. Create a function sum_n that for any given value, say \\(n\\), computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000. Create a function altman_plot that takes two arguments, x and y, and plots the difference against the sum. After running the code below, what is the value of x? x &lt;- 3 my_func &lt;- function(y){ x &lt;- 5 y+5 } Write a function compute_s_n that for any given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Report the value of the sum when \\(n=10\\). Define an empty numerical vector s_n of size 25 using s_n &lt;- vector(&quot;numeric&quot;, 25) and store in the results of \\(S_1, S_2, \\dots S_{25}\\) using a for-loop. Repeat exercise 8, but this time use sapply. Repeat exercise 8, but this time use map_dbl. Plot \\(S_n\\) versus \\(n\\). Use points defined by \\(n=1,\\dots,25\\). Confirm that the formula for this sum is \\(S_n= n(n+1)(2n+1)/6\\). "],
["matrices.html", "Chapter 15 Matrices 15.1 Accessing matrix values", " Chapter 15 Matrices Matrices are another type of object that are common in R. Matrices are similar to data frames in that they are two dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors and numbers in them. Yet matrices have a major advantage over data frames and it’s that we can perform a very powerful type of mathematics operations with them called matrix algebra. We do not describe these operations in this book, but much of what happens in the background when you perform a data analysis involves matrices. For this reason, and others, some of the functions we will learn return matrices. We will describe them briefly here. One way matrices show up is if a function called with sapply returns two or more values. For example, suppose we write a function that for any number x returns x, x^2 and x^3: cubic &lt;- function(x) { c(x, x^2, x^3) } cubic(2) #&gt; [1] 2 4 8 If we use sapply to compute these three numbers for each of the elements in a vector, R automatically binds the results as columns in a matrix: n &lt;- seq(1,5) mat &lt;- sapply(n, cubic) mat #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 2 3 4 5 #&gt; [2,] 1 4 9 16 25 #&gt; [3,] 1 8 27 64 125 15.1 Accessing matrix values You can access specific entries in a matrix using [. If you want the second row, third column you use: mat[2,3] #&gt; [1] 9 If you want the entire second row, you leave the column spot empty: mat[2, ] #&gt; [1] 1 4 9 16 25 Notice that this returns a vector, not a matrix. Similarly, if you want the entire third column, you leave the row spot entry: mat[ ,3] #&gt; [1] 3 9 27 This is also a vector, not a matrix. You can access more than one column or more than one row if you like. This will give you a new matrix. mat[ ,2:4] #&gt; [,1] [,2] [,3] #&gt; [1,] 2 3 4 #&gt; [2,] 4 9 16 #&gt; [3,] 8 27 64 You can subset both rows and columns: mat[1:2,2:4] #&gt; [,1] [,2] [,3] #&gt; [1,] 2 3 4 #&gt; [2,] 4 9 16 In some of our illustrations, we will convert matrices into data frames so that we can, for example, use them with dplyr. For this we can use the function as.data.frame: as.data.frame(mat) #&gt; V1 V2 V3 V4 V5 #&gt; 1 1 2 3 4 5 #&gt; 2 1 4 9 16 25 #&gt; 3 1 8 27 64 125 You can also use the [ to access rows and columns of a data frame: data(&quot;murders&quot;) murders[25, 1] #&gt; [1] &quot;Mississippi&quot; murders[2:3, ] #&gt; state abb region population total #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 "],
["introduction-2.html", "Chapter 16 Introduction", " Chapter 16 Introduction Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table: #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 What do you learn from staring at this table? How quickly can you determine which states have the largest populations? Which states have the smallest? How large is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most human brains, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to all the questions above are readily available from examining this plot: Figure 16.1: Murder totals versus population size by state. We are reminded of the saying “a picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. The growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting. A particularly effective example is a Wall Street Journal article showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced. Another striking example comes from the New York Times and shows data on scores from the NYC Regents Exams. These scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic: The most common test score is the minimum passing grade, with a few scores just below. This unexpected result is consistent with students close to passing having their scores bumped up. This is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey, considered the father of EDA, once said, “The greatest value of a picture is when it forces us to notice what we never expected to see.” Figure 16.2: John W. Tukey (source: Wikipedia). Many widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked. Data visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty and The Best Stats You’ve Ever Seen, Hans Rosling forced us to to notice the unexpected with a series of plots related to world health and economics. In his videos, he used animated graphs to show us how the world was changing and how old narratives were no longer true. Figure 16.3: Life expectancy versus fertility for two years: 1962 and 2013. It is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important. Data visualization is a powerful approach to detecting these problems. We refer to this particular task as exploratory data analysis (EDA). In this chapter, we will learn the basics of data visualization and exploratory data analysis by using three motivating examples. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above 1) world health and economics and 2) infectious disease trends in the United States. Of course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more: ER Tufte (1983) The visual display of quantitative information. Graphics Press. ER Tufte (1990) Envisioning information. Graphics Press. ER Tufte (1997) Visual explanations. Graphics Press. A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130. NB Robbins (2004) Creating more effective graphs. Wiley. We also do not cover interactive graphics, a topic that is too advanced for this book. Some useful resources for those interested in learning more can be found below: https://shiny.rstudio.com/ https://d3js.org/ "],
["variable-types.html", "Chapter 17 Variable types Exercise", " Chapter 17 Variable types We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous. When each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered, for example spiciness (mild, medium, hot), even if they are not numbers per se. In statistics textbooks, they sometimes refer to these as ordinal data. Example of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches respectively. Counts, such as population sizes, are discrete because they have to be round numbers. Keep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data. Exercise Define variables containing the heights of males and females like this: library(dslabs) data(heights) male &lt;- heights$height[heights$sex==&quot;Male&quot;] female &lt;- heights$height[heights$sex==&quot;Female&quot;] How many measurements do we have for each? Suppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, …, 90th percentiles for each sex. Then create a data frame with these two as columns. Study the following boxplots showing population sizes by country: Which continent has the country with the biggest population size ? What continent has the largest median population size? What is median population size for Africa to the nearest million? What proportion of countries in Europe have populations below 14 million? A. 0.99 B. 0.75 C. 0.50 D. 0.25 If we use a log transformation, which continent shown above has the largest interquartile range ? "],
["distributions.html", "Chapter 18 Distributions 18.1 Case study: student heights 18.2 Distribution function 18.3 Cumulative distribution functions 18.4 Histograms 18.5 Smoothed density Exercises 18.6 The normal distribution 18.7 Percentiles 18.8 Case study coninued: summarizing male heights with two numbers Exercises 18.9 Boxplots 18.10 Case study continued: female student heights", " Chapter 18 Distributions You may have noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. So, for example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list? Our first data visualization building block is learning to summarize lists of factors or numeric vectors. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as distribution, there are several data visualization techniques to effectively relay this information. 18.1 Case study: student heights Here we introduce a new motivating problem. It is an artificial one, but it will help us illustrate the concepts needed to understand distributions. Pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions. We collect the data and save it in a data frame: library(dslabs) data(heights) head(heights) #&gt; sex height #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 One way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights. 18.2 Distribution function It turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough. The most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for most of you. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is: #&gt; #&gt; Female Male #&gt; 0.227 0.773 This two category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with the US state regions: Figure 18.1: Distribution of regions acorss the 50 states and D.C. This particular plot is simply showing us four numbers: one for each category. We usually use barplots to display a few numbers. Although this particular plot, a graphical representation of a frequency table, does not provide much more insight than a table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. Once the data is numerical, the task of displaying distributions is more challenging. 18.3 Cumulative distribution functions Numerical data, that are not categorical, also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters respectively. Statistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\). This function is called the cumulative distribution function (CDF). In statistics, the following notation is used: \\[ F(a) = \\mbox{Pr}(x \\leq a) \\] Here is a plot of \\(F\\) for the height data: Figure 18.2: Empirical cummulative distribution function for male height. Similar to what the frequency table does for categorical data, the CDF defines the distribution for numerical data. From the plot we can see that 34% of the values are below 65, since \\(F(66)=\\) 0.164, or that 90% of the values are below 72, since \\(F(72)=\\) 0.841, etc.. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousands word”, in this case, a picture is as informative as 812 numbers. A final note: because CDFs can be defined mathematically, as opposed to using data as we do here, the word empirical is added to distinguish and we use the term empirical CDF (ECDF) instead. 18.4 Histograms Although the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret. The simplest way to make a histograms is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\([49.5, 50.5], [51.5,52.5],(53.5,54.5],...,(82.5,83.5]\\) Figure 18.3: Histogrom of male heights. As you can see in the figure above, a histogram is similar to a barplot, but it differs in that the x-axis is numerical, not categorical. If we send this plot to ET, he will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, ET could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts. So what information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers. 18.5 Smoothed density Smooth density plots are aesthetically more appealing than histograms. Here is what a smooth density plot looks like for our heights data: Figure 18.4: Smooth density of male heights. In this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density. To understand the smooth densities, we have to understand estimates, a topic we don’t cover until a later chapter. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool. The main new concept you must understand is that we assume that our list of observed values comes from a much larger list of unobserved values. In the case of heights, you can imagine that our list of 1050 students comes from a hypothetical list containing all the heights of all the students in all the world measured very precisely. Let’s say there are 1,000,000 of these. This list of values, like any list of values, has a distribution and this is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it. However, we make an assumption that helps us perhaps approximate it. Because we have 1,000,000 values, measured very precisely, we can make a histogram with very, very small bins. The assumption is that if we do this, consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps. Below we have a hypothetical histogram with bins of size 1: The smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5 and 0.1: The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts Now, back to reality. We don’t have millions of measurements. Instead, we have 812 and we can’t make a histogram with very small bins. We therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars: Remember that smooth is a relative term. We can actually control the smoothness of the curve that defines the smooth density through an option in the function that computes the smooth density. Here are two examples using different degrees of smoothness on the same histogram: p1 &lt;- heights %&gt;% filter(sex==&quot;Male&quot;)%&gt;% ggplot(aes(height)) + geom_histogram(aes(y=..density..), binwidth = 1) + geom_density(col=&quot;#00BFC4&quot;, adjust = 0.5) p2 &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(height)) + geom_histogram(aes(y=..density..), binwidth = 1) + geom_density(col=&quot;#00BFC4&quot;, adjust = 2) grid.arrange(p1,p2, ncol=2) We need to make this choice with care as the resulting visualizations can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71, than to the proportion that is 78 or 65. This implies that the curve should be pretty smooth; that is, more like the example on the right than on the left. While the histogram is an assumption free summary, the smoothed density is based on some assumptions. Interpreting the y-axis Finally, we point out that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. But this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68: The proportion of this area is about 0.32, meaning that about that proportion is between 65 and 68 inches. By understanding this we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption and, therefore, with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data: Densities permit stratification As a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities makes it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights: heights %&gt;% ggplot(aes(height, fill=sex)) + geom_density(alpha = 0.2) With the right argument, ggplot automatically shades the intersecting region with a different color. Exercises In the murders dataset, the region is a categorical variable and the following is its distribution: To the closet 5%, what proportion of the states are in the North Central region? Which of the following is true: A. The graph above is a histogram. B. The graph above shows only four numbers with a bar plot. C. Categories are not numbers so it does not make sense to graph the distribution. D. The colors, not the height of the bars, describe the distribution. The plot below shows the eCDF for male heights: Based on the plot, what percentage of males are shorter than 75 inches? A. 100% B. 95% C. 80% D. 72 inches To the closest inch, what height m has the property that 1/2 of the male students are taller than m and 1/2 are shorter? A. 61 inches B. 64 inches C. 69 inches D. 74 inches Here is an eCDF of the murder rates across states: Knowing that there are 51 states (counting DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people? A. 1 B. 5 C. 10 D. 50 Based on the eCDF above, which of the following statements are true: A. About half the states have murder rates above 7 per 100,000 and the other half below. B. Most states have murder rates below 2 per 100,000. C. All the states have murder rates above 2 per 100,000. D. With the exception of 4 states, the murder rates are below 5 per 100,000. Below is a histogram of male heights in our heights dataset: Based on this plot, how many males are between 63.5 and 65.5? A. 10 B. 24 C. 34 D. 100 About what percentage are shorter than 60 inches? A. 1% B. 10% C. 25% D. 50% Based on the density plot below, about what proportion of US states have populations larger than 10 million? A. 0.02 B. 0.15 C. 0.50 D. 0.55 Below are three density plots. Is it possible that they are from the same dataset? Which of the following statements is true: A. It is impossible that they are from the same dataset. B. They are from the same dataset, but different due to code errors. C. They are the same dataset, but the first and second undersmooth and the third oversmooths. D. They are the same dataset, but the first is not in the log scale, the second undersmooths and the third oversmooths. 18.6 The normal distribution Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution. The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations. Examples include gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these in a later chapter. Here we focus on how the normal distribution helps us summarize data. Rather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula: \\[\\mbox{Pr}(a &lt; x &lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left\\{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2\\right\\} \\, dx\\] You don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: \\(m\\) and \\(s\\). The rest of the symbols in the formula represent the interval ends that we determine, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(\\mathrm{e}\\). These two parameters, \\(m\\) and \\(s\\), are referred to as the average, also called the mean, and the standard deviation (SD) of the distribution respectively. The distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what it looks like when the average is 0 and the SD is 1: The fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation, which we now define for an arbitrary list of numbers. For a list of numbers contained in a vector x, the average is defined as: average &lt;- sum(x) / length(x) and the SD is defined as: SD &lt;- sqrt( sum( (x-mu)^2) / length(x)) which can be interpreted as the average distance between values and their average. Let’s compute the values for the height for males which we will store in the object \\(x\\): index &lt;- heights$sex==&quot;Male&quot; x &lt;- heights$height[index] The pre-built functions mean and sd [Footnote: SD divides by length(x)-1] can be used here: average &lt;- mean(x) SD &lt;- sd(x) c(average=average,SD=SD) #&gt; average SD #&gt; 69.31 3.61 Here is a plot of the smooth density and the normal distribution with mean average = 69.315 and SD = 3.611: It does appear to be quite a good approximation. We now will see how well this approximation works at predicting proportion of values within intervals. 18.6.1 Standardized units For data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value \\(x\\), we define it as \\(z = (x-\\mbox{average})/\\mbox{SD}\\). If you look back at the formula for the normal distribution, you see that what is being exponentiated is \\(- z^/2\\). The maximum of \\(\\exp{-z^2/2}\\) is when \\(z=0\\), which explains why the maximum of the distribution is at the mean. It also explains the symmetry since \\(- z^/2\\) is symmetric around 0. If we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (\\(z=0\\)), one of the largest (\\(z=2\\)), one of the smallest (\\(z=-2\\)) or an extremely rare occurrence (\\(z&gt;3\\) or \\(z &lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to data that is approximately normal. In R, we can obtain standard units using the function scale: z &lt;- scale(x) Now to see how many men are within 2 SDs from the average we simply type: mean(abs(z) &lt; 2) #&gt; [1] 0.95 The proportion is about 95%, which is what the normal distribution predicts! To further confirm that in fact the approximation is a good one, we can use quantile-quantile plots. 18.6.2 Quantile-quantile QQ plots A systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, the approach of the QQ-plot is as follows: Define a series of proportions \\(p=0.05,\\dots .95\\). For each \\(p\\), determine the value \\(q\\) so that the proportion of values in the data below \\(q\\) is \\(p\\). The \\(q\\)s are referred to as the quantiles. To give a quick example, for the male heights data we have that: mean(x &lt;= 69.5) #&gt; [1] 0.515 50% are shorter or equal to 69 inches. This implies that if \\(p=0.50\\) then \\(q=69.5\\). Now we define a series of \\(p\\): p &lt;- seq(0.05, 0.95, 0.05) If the quantiles for the data match the quantiles for the normal, then it must be because the data follows a normal distribution. To obtain the quantiles from the data, we can use the quantile function like this: observed_quantiles &lt;- quantile(x, p) To obtain the theoretical normal distribution quantiles, with the corresponding average and SD, we use the qnorm function: theoretical_quantiles &lt;- qnorm( p, mean = mean(x), sd = sd(x)) To see if they match or not, we plot them against each other and draw the identity line: plot(theoretical_quantiles, observed_quantiles) abline(0,1) Notice that this code becomes much cleaner if we use standard units: observed_quantiles &lt;- quantile(z, p) theoretical_quantiles &lt;- qnorm(p) plot(theoretical_quantiles, observed_quantiles) abline(0,1) 18.7 Percentiles Before we move on, let’s define some terms that are commonly used in exploratory data analysis. Percentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). We call, for example, the case of \\(p=0.25\\) the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median. For the normal distribution the median and average are the same, but this is generally not the case. Another special case that receives a name are the quartiles , which are obtained when setting \\(p=0.25,0.50\\), and \\(0.75\\). 18.8 Case study coninued: summarizing male heights with two numbers Using the histogram, density plots and qq-plots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.315 inches and a SD of 3.611 inches. With this information ET will have a good idea of what to expect when he meets our male students. Exercises Load the height data set and create a vector x with just the male heights: library(dslabs) data(heights) x &lt;- heights$height[heights$sex==&quot;Male&quot;] What proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: a logical operator and mean. Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the pnorm function to predict the proportions. Notice that the approximation calculated in question two is very close to the exact calculation in the first question. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81]. How many times bigger is the actual proportion than the approximation? Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as seven footers. Hint: use the pnorm function. There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many these men (18-40 year olds) are seven feet tall or taller in the world? There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world’s 18 to 40 year old seven footers are in the NBA? Repeat the calculations performed in the previous question for Lebron James’ height: 6 feet 8 inches. There are about 150 players that are that tall. In answering the previous questions, we found that it is not at all rare for a seven footer to become an NBA player. What would be a fair critique of our calculations: A. Practice and talent are what make a great basketball player, not height. B. The normal approximation is not appropriate for heights. C. As seen in question 3, the normal approximation tends to underestimate the extreme values. It’s possible that there are more seven footers than we predicted. D. As seen in question 3, the normal approximation tends to overestimate the extreme values. It’s possible that there are less seven footers than we predicted. 18.9 Boxplots To introduce boxplots we will go back to the US murder data. Suppose we want to summarize the murder rate distribution. Using the data visualization technique we have learned, we can quickly see that the normal approximation does not apply here: Figure 18.5: Histogram and qqplot of US states murder rates. In this case, the histogram, or a smooth density plot, would serve as a relatively succinct summary. Now suppose those used to receiving just two numbers as summaries ask us for a more compact summary. Here Tukey offered some advice. Provide a five number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). Tukey further suggested that we ignore outliers when computing the range and instead plot these as independent points. We provide a detailed explanation of outliers later in the chapter. Finally, he suggested we plot these numbers as a “box” with “whiskers”&quot; like this: with the box defined by the 25% and 75% percentile and the whiskers showing the range. The distance between these two is called the interquartile range. The two points are outliers according to Tukey’s definition. The median is shown with a horizontal line. Today, we call these boxplots. From just this simple plot, we know that the median is about 2.5, that the distribution is not symmetric, and that the range is 0 to 5 for the great majority of states with two exceptions. Boxplots are even more useful when we want to quickly compare two or more distributions. For example, here are the heights for men and women: heights %&gt;% ggplot(aes(x=sex, y=height, fill=sex)) + geom_boxplot() The plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. 18.10 Case study continued: female student heights We have to give ET a full summary of our heights, but we have not yet summarized female heights. We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful: We see something we did not see for the males: the density plot has a second “bump”. Also, the qqplot shows that the highest points tend to be taller than expected by the normal distribution. Finally, we also see five points in the qqplot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights. However, go back and read Tukey’s quote. We have noticed what we didn’t expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, FEMALE was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data. Regarding the five smallest values, note that these values are: heights %&gt;% filter(sex==&quot;Female&quot;) %&gt;% top_n(5, desc(height)) %&gt;% .$height #&gt; [1] 51 53 55 52 52 Because these are reported heights, a possibility is that the student meant to enter 5’1“, 5’2”, 5’3&quot; or 5’5“. "],
["robust-summaries.html", "Chapter 19 Robust summaries 19.1 Outliers 19.2 Median 19.3 The inter quartile range (IQR) 19.4 Tukey’s definition of an outlier 19.5 Median absolute deviation Exercises 19.6 Case study: self-reported heights", " Chapter 19 Robust summaries 19.1 Outliers We previously described how boxplots show outliers, but we did not provide a precise definition. Here we discuss outliers, approaches that can help detect them, and summaries that take into account their presence. Outliers are very common in data science. Data recording can be complex and it is common to observe data points generated in error. For example, an old monitoring device may read out nonsensical measurements before completely failing. Human error is also a source of outliers, in particular when data entry is done manually. An individual, for instance, may mistakenly enter their height in centimeters instead of inches. Now, how do we distinguish an outlier from measurements that were too big or too small simply due to expected variability? This is not always an easy question to answer, but we try to provide some guidance. Let’s begin with a simple case. Suppose a colleague is charged with collecting demography data for a group of males. The data is stored in the object: #&gt; num [1:500] 5.59 5.8 5.54 6.15 5.83 5.54 5.87 5.93 5.89 5.67 ... Our colleague uses the fact that heights are usually well approximated by a normal distribution and summarizes the data with average and standard deviation: mean(outlier_example) #&gt; [1] 6.1 sd(outlier_example) #&gt; [1] 7.8 and writes a report on the interesting fact that this group of males is much taller than usual. The average height is over six feet tall! Using your data science skills, however, you notice something else that is unexpected: the standard deviation is over 7 inches. Adding and subtracting two standard deviations, you note that 95% of this population will have heights between -9.489, 21.697 inches, which does not make sense. A quick plot reveals the problem: hist(outlier_example) There appears to be at least one value that is nonsensical, since we know that a height of 180 feet is impossible. The boxplot detects this point as an outlier: boxplot(outlier_example) 19.2 Median When we have an outlier like this, the average can become very large. Mathematically, we can make the average as large as we want by simply changing one number: with 500 data points, we can increase the average by any amount \\(\\Delta\\) by adding \\(\\Delta \\times\\) 500 to a single number. The median, defined as the value for which half the values are smaller and the other half are bigger, is robust to such outliers. No matter how large we make the largest point, the median remains the same. With this data the median is: median(outlier_example) #&gt; [1] 5.74 which is about 5 feet and 9 inches. The median is what boxplots display as a horizontal line. 19.3 The inter quartile range (IQR) The box in boxplots are defined by the first and third quartile. These are meant to provide an idea of the variability in the data: 50% of the data is within this range. The difference between the 3rd and 1st quartile (or 75th and 25th percentiles) is referred to as the inter quartile range (IQR). As is the case with the median, this quantity will be robust to outliers as large values do not affect it. We can do some math to see that for normal data the IQR / 1.349 approximates the standard deviation of the data had an outlier not been present. We can see that this works well in our example since we get a standard deviation estimate of: IQR(outlier_example) / 1.349 #&gt; [1] 0.245 which is about 3 inches. 19.4 Tukey’s definition of an outlier In R, points falling outside the whiskers of the boxplot are referred to as outliers. This definition of outlier was introduced by Tukey. The top whisker ends at the 75th percentile plus 1.5 \\(\\times\\) IQR. Similarly the bottom whisker ends at the 25th percentile minus 1.5\\(\\times\\) IQR. If we define the first and third quartiles as \\(Q_1\\) and \\(Q_3\\) respectively, then an outlier is anything outside the range: \\[[Q_1 - 1.5 \\times (Q_3 - Q1), Q_3 + 1.5 \\times (Q_3 - Q1)]\\]. When the data is normally distributed. the standard units of these values are: q3 &lt;- qnorm(0.75) q1 &lt;- qnorm(0.25) iqr &lt;- q3 - q1 r &lt;- c(q1 - 1.5*iqr, q3 + 1.5*iqr) r #&gt; [1] -2.7 2.7 Using the pnorm function, we see that 99.3% of the data falls in this interval. Keep in mind that this is not such an extreme event: if we have 1000 data points that are randomly distributed, we expect to see about 7 outside of this range. But these would not be outliers since we expect to see them under the typical variation. If we want an outlier to be rarer, we can increase the 1.5 to a larger number. Tukey also used 3 and called these far out outliers. With a normal distribution, 100% of the data falls in this interval. This translates into about 1 in a million chance of being outside the range. In the geom_boxplot function, this can be controlled by the outlier.size argument, which defaults to 1.5. The 180 feet measurement is well beyond the range of the height data: max_height &lt;- quantile(outlier_example, 0.75) + 3*IQR(outlier_example) If we take this value out, we can see that the data is in fact normally distributed as expected: x &lt;- outlier_example[outlier_example &lt; max_height] qqnorm(x) qqline(x) 19.5 Median absolute deviation Another way to estimate the standard deviation in the presence of outliers is to use the median absolute deviation (MAD). To compute the MAD, we first compute the median, and then for each value we compute the distance between that value and the median. The MAD is defined as the median of these distances. For technical reasons not discussed here, this quantity needs to be multiplied by 1.4826 to assure it approximates the actual standard deviation. The mad function already incorporates this correction. For the height data, we get a MAD of: mad(outlier_example) #&gt; [1] 0.237 which is about 3 inches. Exercises Load the height data set and create a vector x with just the male heights used in Galton’s data on the heights of parents and their children from his historic research on heredity. ##You can install data like this: install.packages(&quot;HistData&quot;) library(HistData) data(Galton) x &lt;- Galton$child Compute the average and median of these data. Compute the median and median absolute deviation of these data. Now suppose Galton made a mistake when entering the first value, forgetting to use the decimal point. You can imitate this error by typing: x_with_error &lt;- x x_with_error[1] &lt;- x_with_error[1]*10 How many inches does the average grow after this mistake? How many inches does the SD grow after this mistake? How many inches does the median grow after this mistake? How many inches does the MAD grow after this mistake? How could you use exploratory data analysis to detect that an error was made? A. Since it is only one value out of many, we will not be able to detect this. B. We would see an obvious shift in the distribution. C. A boxplot, histogram, or qq-plot would reveal a clear outlier. D. A scatter plot would show high levels of measurement error. How much can the average accidentally grow with mistakes like this? Write a function called error_avg that takes a value k and returns the average of the vector x after the first entry changed to k. Show the results for k=10000 and k=-10000. 19.6 Case study: self-reported heights data(reported_heights) str(reported_heights) #&gt; &#39;data.frame&#39;: 1095 obs. of 3 variables: #&gt; $ time_stamp: chr &quot;2014-09-02 13:40:36&quot; &quot;2014-09-02 13:46:59&quot; &quot;2014-09-02 13:59:20&quot; &quot;2014-09-02 14:51:53&quot; ... #&gt; $ sex : chr &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... #&gt; $ height : chr &quot;75&quot; &quot;70&quot; &quot;68&quot; &quot;74&quot; ... Height is a character vector so we create a new column with the numeric version: reported_heights &lt;- reported_heights %&gt;% mutate(original_heights = height, height = as.numeric(height)) We get a warning about NAs. This is because some of the self reported heights were not numbers. We can see why we get these: reported_heights %&gt;% filter(is.na(height)) %&gt;% head() #&gt; time_stamp sex height original_heights #&gt; 1 2014-09-02 15:16:28 Male NA 5&#39; 4&quot; #&gt; 2 2014-09-02 15:16:37 Female NA 165cm #&gt; 3 2014-09-02 15:16:52 Male NA 5&#39;7 #&gt; 4 2014-09-02 15:16:56 Male NA &gt;9000 #&gt; 5 2014-09-02 15:16:56 Male NA 5&#39;7&quot; #&gt; 6 2014-09-02 15:17:09 Female NA 5&#39;3&quot; Some students self reported their heights using feet and inches rather than just inches. Others used centimeters and others were just trolling. For now we will remove these entries: reported_heights &lt;- filter(reported_heights, !is.na(height)) If we compute the average and standard deviation, we notice that we obtain strange results. The average and standard deviation are different from the median and MAD: reported_heights %&gt;% group_by(sex) %&gt;% summarize(average = mean(height), sd = sd(height), median = median(height), MAD = mad(height)) #&gt; # A tibble: 2 x 5 #&gt; sex average sd median MAD #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 63.4 27.9 64.2 4.05 #&gt; 2 Male 103. 530. 70 4.45 This suggests that we have outliers, which is confirmed by simply creating a boxplot: reported_heights %&gt;% ggplot(aes(sex, height)) + geom_boxplot() We can see some rather extreme values. To see what these values are, we can quickly look at the largest values using the arrange function: reported_heights %&gt;% arrange(desc(height)) %&gt;% top_n(10, height) #&gt; time_stamp sex height original_heights #&gt; 1 2014-09-03 23:55:37 Male 11111 11111 #&gt; 2 2016-04-10 22:45:49 Male 10000 10000 #&gt; 3 2015-08-10 03:10:01 Male 684 684 #&gt; 4 2015-02-27 18:05:06 Male 612 612 #&gt; 5 2014-09-02 15:16:41 Male 511 511 #&gt; 6 2014-09-07 20:53:43 Male 300 300 #&gt; 7 2014-11-28 12:18:40 Male 214 214 #&gt; 8 2017-04-03 16:16:57 Male 210 210 #&gt; 9 2015-11-24 10:39:45 Male 192 192 #&gt; 10 2014-12-26 10:00:12 Male 190 190 #&gt; 11 2016-11-06 10:21:02 Female 190 190 The first seven entries look like strange errors. However, the next few look like they were entered as centimeters instead of inches. Since 184 cm is equivalent to six feet tall, we suspect that 184 was actually meant to be 72 inches. We can review all the nonsensical answers by looking at the data considered to be far out by Tukey: max_height &lt;- quantile(reported_heights$height, .75) + 3*IQR(reported_heights$height) min_height &lt;- quantile(reported_heights$height, .25) - 3*IQR(reported_heights$height) c(min_height, max_height ) #&gt; 25% 75% #&gt; 44 93 outliers &lt;- reported_heights %&gt;% filter(!between(height, min_height, max_height)) %&gt;% arrange(height) outliers$height #&gt; [1] 0.00 0.70 1.00 1.00 1.00 1.60 1.70 #&gt; [8] 2.00 5.00 5.00 5.00 5.00 5.10 5.10 #&gt; [15] 5.10 5.10 5.11 5.11 5.11 5.11 5.20 #&gt; [22] 5.20 5.20 5.25 5.25 5.30 5.40 5.40 #&gt; [29] 5.50 5.50 5.50 5.50 5.50 5.50 5.50 #&gt; [36] 5.50 5.50 5.50 5.50 5.51 5.57 5.60 #&gt; [43] 5.60 5.60 5.60 5.60 5.60 5.60 5.69 #&gt; [50] 5.70 5.70 5.70 5.70 5.70 5.70 5.70 #&gt; [57] 5.75 5.80 5.80 5.80 5.80 5.80 5.90 #&gt; [64] 5.90 5.90 5.90 6.00 6.00 6.00 6.00 #&gt; [71] 6.00 6.00 6.00 6.00 6.00 6.00 6.00 #&gt; [78] 6.00 6.00 6.00 6.00 6.00 6.00 6.00 #&gt; [85] 6.00 6.10 6.10 6.20 6.20 6.30 6.50 #&gt; [92] 6.50 6.50 6.70 7.00 12.00 19.00 22.00 #&gt; [99] 23.00 25.00 34.00 34.00 100.00 103.20 111.00 #&gt; [106] 120.00 120.00 150.00 150.00 152.00 157.00 157.00 #&gt; [113] 158.00 158.00 158.00 159.00 160.00 160.00 161.00 #&gt; [120] 162.00 162.00 162.00 163.00 164.00 164.00 165.00 #&gt; [127] 165.00 165.00 167.00 167.00 167.00 168.00 168.00 #&gt; [134] 168.00 168.00 169.00 169.00 169.00 169.00 170.00 #&gt; [141] 170.00 170.00 170.00 170.00 170.00 170.00 170.00 #&gt; [148] 170.00 172.00 172.00 172.00 172.00 172.00 172.00 #&gt; [155] 172.00 173.00 173.00 173.00 174.00 174.00 174.00 #&gt; [162] 174.00 175.00 175.00 175.00 175.00 175.00 176.00 #&gt; [169] 176.00 177.00 178.00 178.00 178.00 178.00 178.00 #&gt; [176] 179.00 180.00 180.00 180.00 180.00 180.00 180.00 #&gt; [183] 180.00 180.00 180.00 182.00 183.00 183.00 183.00 #&gt; [190] 184.00 184.00 184.00 185.00 185.00 186.00 190.00 #&gt; [197] 190.00 192.00 210.00 214.00 300.00 511.00 612.00 #&gt; [204] 684.00 10000.00 11111.00 Examining these heights we see two common mistakes: entries in centimeters, which turn out to be too large, and entries of the form x.y with x and y representing feet and inches respectively. Some of the even smaller values, such as 1.6, could be entries in meters. In the data wrangling chapter, we will learn techniques for correcting these values and converting them into inches. Here we were able to detect this problem using careful data exploration to uncover issues with the data: the first step in the great majority of data science projects. "],
["summarizing-data-with-dplyr.html", "Chapter 20 Summarizing data with dplyr 20.1 Summarize 20.2 The dot operator 20.3 Group then summarize 20.4 Sorting data frames Exercises", " Chapter 20 Summarizing data with dplyr An important part of exploratory data analysis is summarizing data. We learned about the average and standard deviation as a two summary statistic that provides all the necessary information to summarize data that is normally distributed. We also learned that better summaries can be achieved by splitting data into groups before using the normal approximation. For example, in our heights dataset, we described the height of men and women separately. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using what we call the dot placeholder. Finally, we also learn to use arrange, which helps us examine data after sorting. 20.1 Summarize The summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights: library(dslabs) data(heights) that computes the average and standard deviation for males: s &lt;- heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% summarize(average = mean(height), standard_deviation = sd(height)) s #&gt; average standard_deviation #&gt; 1 69.3 3.61 This takes our original data table as input, filters it to keep only males and then produces a new, summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same. Because the resulting table, stored in s, is a data frame, we can access the components with the accessor $, which in this case will be a numeric: s$average #&gt; [1] 69.3 s$standard_deviation #&gt; [1] 3.61 As with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), it is accessing the column with the name, and then computing the average of the respective numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, min and max like this: heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% summarize(median = median(height), minimum = min(height), maximum = max(height)) #&gt; median minimum maximum #&gt; 1 69 50 82.7 We can obtain these three values with just one line using the quantiles function; e.g. quantile(x, c(0,0.5,1)) returns the min, median, and max of the vector x. However, if we attempt to use a function that returns two or more values: heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% summarize(range = quantile(height, c(0, 0.5, 1))) we will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In a later chapter, we will learn how to deal with functions that return more than one value. For another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column: data(murders) murders &lt;- murders %&gt;% mutate(murder_rate = total/population*100000) Remember that the US murder is not the average of the state murder rates: summarize(murders, mean(murder_rate)) #&gt; mean(murder_rate) #&gt; 1 2.78 This is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total US murders divided by the total US population. So the correct computation is: us_murder_rate &lt;- murders %&gt;% summarize(rate = sum(total) / sum(population) * 100000) us_murder_rate #&gt; rate #&gt; 1 3.03 This computation counts larger states proportionally to their size which results in a larger value. 20.2 The dot operator The us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame: class(us_murder_rate) #&gt; [1] &quot;data.frame&quot; since, as most dplyr functions, summarize always returns a data frame. This might be problematic if we want to use the result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data piped via %&gt;%: when a data object is piped it can be accessed using the dot .. To understand what we mean take a look at this line of code: us_murder_rate %&gt;% .$rate #&gt; [1] 3.03 This returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate. To understand this line, you just need to think of . as a placeholder for the data that is being passed through the pipe. Because this data object is a data frame, we can access its columns with the $. To get a number from the original data table with one line of code we can type: us_murder_rate &lt;- murders %&gt;% summarize(rate = sum(total) / sum(population) * 100000) %&gt;% .$rate us_murder_rate #&gt; [1] 3.03 which is now a numeric: class(us_murder_rate) #&gt; [1] &quot;numeric&quot; We will see other instances in which using the . is useful. For now, we will only use it to produce numeric vectors from pipelines constructed with dplyr. 20.3 Group then summarize A common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this. If we type this: heights %&gt;% group_by(sex) #&gt; # A tibble: 1,050 x 2 #&gt; # Groups: sex [2] #&gt; sex height #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; 6 Female 65 #&gt; # ... with 1,044 more rows the result does not look very different from heights, except we see this Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens: heights %&gt;% group_by(sex) %&gt;% summarize(average = mean(height), standard_deviation = sd(height)) #&gt; # A tibble: 2 x 3 #&gt; sex average standard_deviation #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 64.9 3.76 #&gt; 2 Male 69.3 3.61 The summarize function applies the summarization to each group separately. For another example, let’s compute the median murder rate in the four regions of the country: murders %&gt;% group_by(region) %&gt;% summarize(median_rate = median(murder_rate)) #&gt; # A tibble: 4 x 2 #&gt; region median_rate #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Northeast 1.80 #&gt; 2 South 3.40 #&gt; 3 North Central 1.97 #&gt; 4 West 1.29 20.4 Sorting data frames When examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size when we type: murders %&gt;% arrange(population) %&gt;% head() #&gt; state abb region population total murder_rate #&gt; 1 Wyoming WY West 563626 5 0.887 #&gt; 2 District of Columbia DC South 601723 99 16.453 #&gt; 3 Vermont VT Northeast 625741 2 0.320 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Alaska AK West 710231 19 2.675 #&gt; 6 South Dakota SD North Central 814180 8 0.983 We get to decide which column to sort by. To see the states by population, from smallest to largest, we arrange by murder_rate instead: murders %&gt;% arrange(murder_rate) %&gt;% head() #&gt; state abb region population total murder_rate #&gt; 1 Vermont VT Northeast 625741 2 0.320 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 3 Hawaii HI West 1360301 7 0.515 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Iowa IA North Central 3046355 21 0.689 #&gt; 6 Idaho ID West 1567582 12 0.766 Note that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order we can type: murders %&gt;% arrange(desc(murder_rate)) %&gt;% head() #&gt; state abb region population total murder_rate #&gt; 1 District of Columbia DC South 601723 99 16.45 #&gt; 2 Louisiana LA South 4533372 351 7.74 #&gt; 3 Missouri MO North Central 5988927 321 5.36 #&gt; 4 Maryland MD South 5773552 293 5.07 #&gt; 5 South Carolina SC South 4625364 207 4.48 #&gt; 6 Delaware DE South 897934 38 4.23 Nested sorting If we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region then, within region, we order by murder rate: murders %&gt;% arrange(region, murder_rate) %&gt;% head() #&gt; state abb region population total murder_rate #&gt; 1 Vermont VT Northeast 625741 2 0.320 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 3 Maine ME Northeast 1328361 11 0.828 #&gt; 4 Rhode Island RI Northeast 1052567 16 1.520 #&gt; 5 Massachusetts MA Northeast 6547629 118 1.802 #&gt; 6 New York NY Northeast 19378102 517 2.668 The top \\(n\\) In the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. Here are the first 10 rows: murders %&gt;% top_n(10, murder_rate) #&gt; state abb region population total murder_rate #&gt; 1 Arizona AZ West 6392017 232 3.63 #&gt; 2 Delaware DE South 897934 38 4.23 #&gt; 3 District of Columbia DC South 601723 99 16.45 #&gt; 4 Georgia GA South 9920000 376 3.79 #&gt; 5 Louisiana LA South 4533372 351 7.74 #&gt; 6 Maryland MD South 5773552 293 5.07 #&gt; 7 Michigan MI North Central 9883640 413 4.18 #&gt; 8 Mississippi MS South 2967297 120 4.04 #&gt; 9 Missouri MO North Central 5988927 321 5.36 #&gt; 10 South Carolina SC South 4625364 207 4.48 top_n picks the highest n based on the column given as a second argument. However, the rows are not sorted. If the second argument is left blank, then it returns the first n columns. This means that to see the 10 states with the highest murder rates we can type: murders %&gt;% arrange(desc(murder_rate)) %&gt;% top_n(10) #&gt; Selecting by murder_rate #&gt; state abb region population total murder_rate #&gt; 1 District of Columbia DC South 601723 99 16.45 #&gt; 2 Louisiana LA South 4533372 351 7.74 #&gt; 3 Missouri MO North Central 5988927 321 5.36 #&gt; 4 Maryland MD South 5773552 293 5.07 #&gt; 5 South Carolina SC South 4625364 207 4.48 #&gt; 6 Delaware DE South 897934 38 4.23 #&gt; 7 Michigan MI North Central 9883640 413 4.18 #&gt; 8 Mississippi MS South 2967297 120 4.04 #&gt; 9 Georgia GA South 9920000 376 3.79 #&gt; 10 Arizona AZ West 6392017 232 3.63 Exercises For these exercises we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999 about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package which can install using: install.packages(&quot;NHANES&quot;) Once you install it you can load the data this way: library(NHANES) data(NHANES) The NHANES data has many missing values. Remember that the main summarization function in R will return NA if any of the entries of the input vector is an NA. Here is an example: library(dslabs) data(na_example) mean(na_example) #&gt; [1] NA sd(na_example) #&gt; [1] NA To ignore the NAs we can use the na.rm argument: mean(na_example, na.rm=TRUE) #&gt; [1] 2.3 sd(na_example, na.rm=TRUE) #&gt; [1] 1.22 Let’s now explore the NHANES data. We will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-29 year old females. Note that the category is coded with 20-29, with a space in front! The AgeDecade is a categorical variable with these ages. What is the average and standard deviation of systolic blood pressure, as saved in the BPSysAve variable? Save it to a variable called ref. Hint: Use filter and summarize and use the na.rm=TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter. Using only one line of code, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then the dot. Now report the min and max values for the same group. Compute the average and standard deviation for females, but for each age group separately. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age, filter by Gender and then use group_by. Now do the same for males. We can actually combine both these summaries into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender). For males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure. "],
["ggplot2.html", "Chapter 21 ggplot2 21.1 The cheat sheet 21.2 The components of a graph 21.3 ggplot objects: a blank slate 21.4 Geometries 21.5 Aesthetic mappings 21.6 Layers 21.7 Tinkering with arguments 21.8 Global versus local aesthetic mappings 21.9 Scales 21.10 Labels and titles 21.11 Categories as colors 21.12 Annotation and shapes 21.13 Adjustments 21.14 Add-on packages 21.15 Putting it all together 21.16 Other geometries 21.17 Grids of plots 21.18 Quick plots with qplot", " Chapter 21 ggplot2 We have now described several data visualization techniques and are ready to learn how to create them in R. Throughout the book, we will be using the ggplot2 package. We can load it, along with dplyr, as part of the tidyverse: Many other approaches are available for creating plots in R. In fact, the plotting capabilities that come with a basic installation of R are already quite powerful. We have seen examples of these already with the functions plot, hist and boxplot. There are also other packages for creating graphics such as grid and lattice. We chose to use ggplot2 in this book because it breaks plots into components in a way that permits beginners to create relatively complex and aesthetically pleasing plots using syntax that is intuitive and comparatively easy to remember. One reason ggplot2 is generally more intuitive for beginners is that it uses a grammar of graphics, the gg in ggplot2. This is analogous to the way learning grammar can help a beginner construct hundreds of different sentences by learning just a handful of verbs, nouns and adjectives without having to memorize each specific sentence. Similarly, by learning a handful of ggplot2 building blocks and its grammar, you will be able to create hundreds of different plots. Another reason ggplot2 makes it easier for beginners is that its default behavior is carefully chosen to satisfy the great majority of cases and is visually pleasing. As a result, it is possible to create informative and elegant graphs with relatively simple and readable code. One limitation is that ggplot2 is designed to work exclusively with data tables in which rows are observations and columns are variables. However, a substantial percentage of datasets that beginners work with are, or can be converted into, this format. An advantage of this approach is that, assuming that our data follows this format, it simplifies the code and learning the grammar. 21.1 The cheat sheet To use ggplot2 you will have to learn several functions and arguments. These are hard to memorize so we highly recommend you have the a ggplot2 sheet cheat handy. You can get a copy here: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf or simply perform an internet search for “ggplot2 cheat sheet”. 21.2 The components of a graph We construct a graph that summarizes the US murders dataset: Figure 21.1: Muder totals versus population size for US states. We can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color which depicts how most southern states have murder rates above the average. This data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part. The first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main three components to note are: Data: The US murders data table is being summarized. We refer to this as the data component. Geometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histograms, smooth densities, qqplots, and boxplots. Aesthetic mapping: The x-axis values are used to display population size, the y-axis values are used to display the total number of murders, text is used to identify the states, and colors are used to denote the four different regions. These are the aesthetic mappings component. How we define the mapping depends on what geometry we are using. We also note that: The range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales. We refer to this as the scale component. There are labels, a title, a legend, and we use the style of The Economist magazine. We will now construct the plot piece by piece. We start by loading the dataset: library(dslabs) data(murders) 21.3 ggplot objects: a blank slate The first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object: ggplot(data = murders) We can also pipe the data. So this line of code is equivalent to the one above: murders %&gt;% ggplot() It renders a plot, in this case a blank slate, since no geometry has been defined. The only style choice we see is a grey background. What has happened above is that the object was created and because it was not assigned, it was automatically evaluated. But we can define an object, for example, like this: p &lt;- ggplot(data = murders) class(p) #&gt; [1] &quot;gg&quot; &quot;ggplot&quot; To render the plot associated with this object, we simply print the object p. The following two lines of code produce the same plot we see above: print(p) p 21.4 Geometries In ggplot we create graphs by adding layers. Layers can define geometries, compute summary statistics, define what scales to use, or even change styles. To add layers, we use the the symbol +. In general a line of code will look like this: DATA %&gt;% ggplot() + LAYER 1 + LAYER 2 + … + LAYER N Usually, the first added layer defines the geometry. We want to make a scatterplot. So what geometry do we use? Taking a quick look at the cheat sheet, we see that the function used to create plots with this geometry is geom_point. Geometry function names follow this pattern: geom and the name of the geometry connected by an underscore. For geom_point to know what to do, we need to provide data and a mapping. We have already connected the object p with the murders data table and, if we add as a layer geom_point, we will default to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file geom_point help file: Aesthetics geom_point understands the following aesthetics (required aesthetics are in bold): x y alpha colour and, as expected, we see that at least two arguments are required x and y. 21.5 Aesthetic mappings aes will be one of the functions you will most use. This function connects data with what we see on the graph. We refer to this connect as the aesthetic mappings. The outcome of this function is often used as the argument of a geometry function. This example produces a scatterplot of total murders versus population in millions: murders %&gt;% ggplot() + geom_point(aes(x = population/10^6, y = total)) We can drop the x = and y = if we wanted to since these are the first and second expected arguments, as seen in the help page. We can also add a layer to the p object that has defined above as p &lt;- ggplot(data = murders): p + geom_point(aes(population/10^6, total)) The scale and labels are defined by default when adding this layer. We also use the variable names from the object component: population and total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error. 21.6 Layers A second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot, without and with a rectangle behind the text respectively. Because each state (each point) has a label, we need an aesthetic mapping to make the connection. By reading the help file, we learn that we supply the mapping between point and label through the label argument of aes. So the code looks like this: p + geom_point(aes(population/10^6, total)) + geom_text(aes(population/10^6, total, label = abb)) We have successfully added a second layer to the plot. As an example of the unique behavior of aes mentioned above, note that this call: p_test &lt;- p + geom_text(aes(population/10^6, total, label = abb)) is fine, whereas this call: p_test &lt;- p + geom_text(aes(population/10^6, total), label = abb) will give you an error since abb is not found once it is outside of the aes function. The layer geom_text does not know where to find abb since it is not a global variable. 21.7 Tinkering with arguments Each geometry function has many arguments other than aes and data. They tend to be specific to the function. For example, in the plot we wish to make, the points are larger than the default ones. In the help file we see that size is an aesthetic and we can change it like this: p + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb)) size is not a mapping, it affects all the points so we do not need to include it inside aes. Now that the points are larger, it is hard to see the labels. If we read the help file for geom_text, we see the nudge_x argument, which moves the text slightly to the right: p + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb), nudge_x = 1) This is preferred as it makes it easier to read the text. 21.8 Global versus local aesthetic mappings In the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings: args(ggplot) #&gt; function (data = NULL, mapping = aes(), ..., environment = parent.frame()) #&gt; NULL If we define a mapping in ggplot, then all the geometries that are added as layers will default to this mapping. We redefine p: p &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) and then we can simply use code as follows: p + geom_point(size = 3) + geom_text(nudge_x = 1.5) We keep the size and nudge_x argument in geom_point and geom_text respectively because we want to only increase the size of points and only nudge the labels. Also note that the geom_point function does not need a label argument and therefore ignores it. If necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example: p + geom_point(size = 3) + geom_text(aes(x = 10, y = 800, label = &quot;Hello there!&quot;)) Clearly, the second call to geom_text does not use the population and total. 21.9 Scales First, our desired scales are in log-scale. This is not the default so this change needs to be added through a scales layer. A quick look at the cheat sheet reveals the scale_x_continuous lets us control the behavior of scales. We use them like this: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) Because we are in the log-scale now, the nudge must be made smaller. This particular transformation is so common that ggplot2 provides specialized functions: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() 21.10 Labels and titles Similarly, the cheat sheet quickly reveals that to change labels and add a title, we use the following functions: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) We are almost there! All we have left to do is add color, a legend and optional changes to the style. 21.11 Categories as colors We can change the color of the points using the col argument in the geom_point function. To facilitate exposition, we will redefine p to be everything except the points layer: p &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) and then test out what happens by adding different calls to geom_point. We can make all the points blue by adding the color argument: p + geom_point(size = 3, color =&quot;blue&quot;) This, of course, is not what we want. We want to assign color depending on the geographical region. A nice default behavior of ggplot2 is that if we assign a categorical variable to color, it automatically assigns a different color to each category. It also adds a legend! To map each point to a color, we need to use aes since this is a mapping. We use the following code: p + geom_point(aes(col=region), size = 3) The x and y mappings are inherited from those already defined in p. So we do not redefine them. We also move aes to the first argument since that is where the mappings are expected in this call. Here we see yet another useful default behavior: ggplot2 has automatically added a legend that maps color to region. 21.12 Annotation and shapes We often want to add shapes or annotation to figures that are not derived directly from the aesthetic mapping. Examples in include labels, boxes, shaded areas and lines. Here we want to add a line that represents the average murder rate for the entire country. Once we determine the per million rate to be \\(r\\), this line is defined by the formula: \\(y = r x\\), with \\(y\\) and \\(x\\) our axes: total murders and population in millions respectively. In the log-scale this line turns into: \\(\\log(y) = \\log(r) + \\log(x)\\). So in our plot it’s a line with slope 1 and intercept \\(\\log(r)\\). To compute this value we use what we our dplyr skills: r &lt;- murders %&gt;% summarize(rate = sum(total) / sum(population) * 10^6) %&gt;% .$rate To add a line we use the geom_abline function. ggplot2 uses ab in the name to remind us we are supplying the intercept (a) and slope (b). The default line has slope 1 and intercept 0 so we only have to define the intercept: p + geom_point(aes(col=region), size = 3) + geom_abline(intercept = log10(r)) Here geom_abline does not use any information from the data object. We can change the line type and color of the lines using arguments. Also, we draw it first so it doesn’t go over our points. p &lt;- p + geom_abline(intercept = log10(r), lty = 2, color = &quot;darkgrey&quot;) + geom_point(aes(col=region), size = 3) Note that we redefined p. 21.13 Adjustments The default plots created by ggplot2 are already very useful. However, we frequently need to make minor tweaks to the default behavior. Although it is not always obvious how to make these even with the cheat sheet, ggplot2 is very flexible. For example, we can make changes to the legend via the scale_color_discrete function. In our plot the word region is capitalized and we can change it like this: p &lt;- p + scale_color_discrete(name = &quot;Region&quot;) 21.14 Add-on packages The power of ggplot2 is augmented further due to the availability of add-on packages. The remaining changes needed to put the finishing touches on our plot require the ggthemes and ggrepel packages. The style of a ggplot2 graph can be changed using the theme functions. Several themes are included as part of the ggplot2 package. In fact, for most of the plots in this book, we use a function in the dslabs package that automatically sets a default theme: ds_theme_set() Many other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this: library(ggthemes) p + theme_economist() You can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead. The final difference has to do with the position of the labels. In our plot, some of the labels fall on top of each other. The add-on package ggrepel includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change geom_text with geom_text_repell. 21.15 Putting it all together Now that we are done testing, we can write one piece of code that produces our desired plot from scratch. library(ggthemes) library(ggrepel) ### First define the slope of the line r &lt;- murders %&gt;% summarize(rate = sum(total) / sum(population) * 10^6) %&gt;% .$rate ## Now make the plot murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_abline(intercept = log10(r), lty = 2, color = &quot;darkgrey&quot;) + geom_point(aes(col=region), size = 3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) + scale_color_discrete(name = &quot;Region&quot;) + theme_economist() 21.16 Other geometries Now let’s try to make the summary plots we have described in this chapter. Histogram Let’s start with the histogram. First, we need to use dplyr to filter the data: heights %&gt;% filter(sex==&quot;Male&quot;) Once we have a dataset, the next step is deciding what geometry we need. If you guessed geom_histogram, you guessed correctly. Looking at the help file for this function we learn that the only required argument is x, the variable for which we will construct a histogram. The code looks like this: p &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(x = height)) p + geom_histogram() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. As before, we can drop the x =. This call gives us a message: stat_bin() using bins = 30. Pick better value with binwidth. We previously used a bin size of 1 inch, so the code looks like this: p + geom_histogram(binwidth = 1) Finally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title: Density To create a smooth density, we need a different geometry: we used geom_density instead. p + geom_density() To fill in with color, we can use the fill argument. p + geom_density(fill=&quot;blue&quot;) QQ-plots For qq-plots we use the geom_qq geometry. From the help file, we learn that we need to specify the sample (we will learn about samples in a later chapter). p &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = height)) p + geom_qq() By default the sample variable is compared to a normal distribution with average 0 and standard deviation 1. To change this, again from the help file, we use the dparams arguments. params &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% summarize(mean = mean(height), sd = sd(height)) p + geom_qq(dparams = params) Adding an identity line is as simple as assigning another layer. For straight lines, we use the geom_abline function. To help you remember the name of this function, remember that the ab in front of line serves to remind us that we need to supply an intercept (a) and slope (b) to draw the line \\(y=a+bx\\). The default is the identity a=0 and b=1 p + geom_qq(dparams = params) + geom_abline() Another option here is to scale the data first and the make a qqplot against the standard normal: heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = scale(height))) + geom_qq() + geom_abline() 21.17 Grids of plots There are often reasons to graph plots next to each other. The gridExtra package permits us to do that: p &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(x = height)) p1 &lt;- p + geom_histogram(binwidth = 1, fill = &quot;blue&quot;, col=&quot;black&quot;) p2 &lt;- p + geom_histogram(binwidth = 2, fill = &quot;blue&quot;, col=&quot;black&quot;) p3 &lt;- p + geom_histogram(binwidth = 3, fill = &quot;blue&quot;, col=&quot;black&quot;) To print them all side-by-side, we can use the function grid.arrange in the gridExtra package: library(gridExtra) grid.arrange(p1,p2,p3, ncol = 3) 21.18 Quick plots with qplot library(tidyverse) library(dslabs) data(murders) We have learned the powerful approach to generating visualization with ggplot. However, there are instance in which all we want is to make quick plot of, for example, a histogram of the values in a vector, a scatter plots of the values in two vectors, or a boxplot using a categorical and numeric vectors. We demonstrated how to generate these plots with hist, plot, and boxplot. However, if we want to keep consistent with the ggplot style we can use the function qplot. So if we have values in a vector, say x &lt;- murders$population and we want to make a histogram with ggplot, we would have to type something like data.frame(x = x) %&gt;% ggplot(aes(x)) + geom_histogram(bins=15) Using R-base it is much quicker: hist(x) However, using qplot we can generate a plot using the ggplot style just as quickly: qplot(x, bins=15) Looking at the help file for the qplot function we see several ways in which we can improve the look of the plot: qplot(x, bins=15, color = I(&quot;black&quot;), xlab = &quot;Population&quot;) The reason we use I(&quot;black&quot;) is because we want qplot to treat &quot;black&quot; as a character rather than convert it to a factor, which is the default behavior within aes, which is internally called here. In general, the function I is used in R to say “keep it as it is!”. One convenient feature of qplot is that it guesses what plot we want. For example, if we call it with two variables we get a scatterplot y &lt;- murders$total qplot(x, y) and if the first argument is a factor we get a points plot like this: f &lt;-murders$region qplot(f, y) We can also explicitly ask for a geometry using the geom argument: qplot(f, y, geom = &quot;boxplot&quot;) We can also explicitly tell qplot what dataset to use: qplot(population, total, data = murders) "],
["case-study-trends-in-world-health-and-economics.html", "Chapter 22 Case study: Trends in world health and economics 22.1 Example 1: Life expectancy and fertility rates 22.2 Faceting 22.3 Fixed scales for better comparisons 22.4 Time series plots 22.5 Labels for legends 22.6 Example 2: Income distribution 22.7 Transformations 22.8 Stratify and boxplot 22.9 Comparing distributions 22.10 Ecological fallacy", " Chapter 22 Case study: Trends in world health and economics In this section we will demonstrate how relatively simple ggplot2 code can create insightful and aesthetically pleasing plots that help us better understand trends in world health and economics. We later augment the code somewhat to perfect the plots and describe some general data visualization principles. 22.1 Example 1: Life expectancy and fertility rates Hans Rosling was the co-founder of the Gapminder Foundation, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. The organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies and other unfortunate events. As stated in the Gapminder Foundation’s website: Journalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs. them!” , “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!” Hans Rosling conveyed actual data-based trends in a dramatic way of his own, using effective data visualization. This section is based on two talks that exemplify this approach to education: New Insights on Poverty and The Best Stats You’ve Ever Seen. Specifically, in this section, we use data to attempt to answer the following two questions: Is it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia and Latin America? Has income inequality across countries worsened during the last 40 years? To answer these questions, we will be using the gapminder dataset provided in dslabs. This dataset was created using a number of spreadsheets available from the Gapminder Foundation. You can access the table like this: library(dslabs) data(gapminder) head(gapminder) #&gt; country year infant_mortality life_expectancy fertility #&gt; 1 Albania 1960 115.4 62.9 6.19 #&gt; 2 Algeria 1960 148.2 47.5 7.65 #&gt; 3 Angola 1960 208.0 36.0 7.32 #&gt; 4 Antigua and Barbuda 1960 NA 63.0 4.43 #&gt; 5 Argentina 1960 59.9 65.4 3.11 #&gt; 6 Armenia 1960 NA 66.9 4.55 #&gt; population gdp continent region #&gt; 1 1636054 NA Europe Southern Europe #&gt; 2 11124892 1.38e+10 Africa Northern Africa #&gt; 3 5270844 NA Africa Middle Africa #&gt; 4 54681 NA Americas Caribbean #&gt; 5 20619075 1.08e+11 Americas South America #&gt; 6 1867396 NA Asia Western Asia Hans Rosling’s quiz As done in the New Insights on Poverty video, we start by testing our knowledge regarding differences in child mortality across different countries. For each of the six pairs of countries below, which country do you think had the highest child mortality rates in 2015? Which pairs do you think are most similar? Sri Lanka or Turkey Poland or South Korea Malaysia or Russia Pakistan or Vietnam Thailand or South Africa When answering these questions without data, the non-European countries are typically picked as having higher child mortality rates: Sri Lanka over Turkey, South Korea over Poland, and Malaysia over Russia. It is also common to assume that countries considered to be part of the developing world: Pakistan, Vietnam, Thailand and South Africa, have similarly high mortality rates. To answer these questions with data, we can use dplyr. For example, for the first comparison we see that: library(tidyverse) gapminder %&gt;% filter(year == 2015 &amp; country %in% c(&quot;Sri Lanka&quot;,&quot;Turkey&quot;)) %&gt;% select(country, infant_mortality) #&gt; country infant_mortality #&gt; 1 Sri Lanka 8.4 #&gt; 2 Turkey 11.6 Turkey has the higher rate. We can use this code on all comparisons and find the following: country infant_mortality country1 infant_mortality1 Sri Lanka 8.4 Turkey 11.6 Poland 4.5 South Korea 2.9 Malaysia 6.0 Russia 8.2 Pakistan 65.8 Vietnam 17.3 Thailand 10.5 South Africa 33.6 We see that the European countries on this list have higher child mortality rates: Poland has a higher rate than South Korea, and Russia has a higher rate than Malaysia. We also see that Pakistan has a much higher rate than Vietnam, and South Africa has a much higher rate than Thailand. It turns out that most people do worse than if they were guessing, which implies that more than ignorant, we are misinformed. A scatterplot The reason for this stems from the preconceived notion that the world is divided into two groups: the western world (Western Europe and North America), characterized by long life spans and small families, versus the developing world (Africa, Asia, and Latin America) characterized by short life spans and large families. But, does the data support this dichotomous view? The necessary data to answer this question is also available in our gapminder table. Using our newly learned data visualization skills we will be able to tackle this challenge. In order to analyze this world view, our first plot is a scatterplot of life expectancy versus fertility rates (average number of children per woman). We start by looking at data from about 50 years ago, when perhaps this view was first cemented in our minds. filter(gapminder, year==1962) %&gt;% ggplot( aes(fertility, life_expectancy)) + geom_point() Most points fall into two distinct categories: Life expectancy around 70 years and 3 or less children per family. Life expectancy lower then 65 years and more than 5 children per family. To confirm that indeed these countries are from the regions we expect, we can use color to represent continent. filter(gapminder, year==1962) %&gt;% ggplot( aes(fertility, life_expectancy, color = continent)) + geom_point() So in 1962, “the west versus developing world” view was grounded in some reality. But is this still the case 50 years later? 22.2 Faceting We could easily plot the 2012 data in the same way we did for 1962. But to compare, side by side plots are preferable. In ggplot2, we can achieve this by faceting variables: we stratify the data by some variable and make the same plot for each strata. To achieve faceting, we add a layer with the function facet_grid, which automatically separates the plots. This function lets you facet by up to two variables using columns to represent one variable and rows to represent the other. The function expects the row and column variables to be separated by a ~. Here is an example of a scatterplot with facet_grid added as the last layer: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid(continent~year) We see a plot for each continent/year pair. However, this is just an example and more than what we want, which is simply to compare 1962 and 2012. In this case, there is just one variable and we use . to let facet know that we are not using one of the variables: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid( . ~ year) This plot clearly shows that the majority of countries have moved from the developing world cluster to the western world one. In 2012, the western versus developing world view no longer makes sense. This is particularly clear when comparing Europe to Asia, the latter which includes several countries that have made great improvements. facet_wrap To explore how this transformation happened through the years, we can make the plot for several years. For example, we can add 1970, 1980, 1990, and 2000. If we do this, we will not want all the plots on the same row, the default behavior of facet_grid, since they will become too thin to show the data. Instead we will want to use multiple rows and columns. The function facet_wrap permits us to do this by automatically wrapping the series of plots so that each display has viewable dimensions: years &lt;- c(1962, 1980, 1990, 2000, 2012) continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder %&gt;% filter(year %in% years &amp; continent %in% continents) %&gt;% ggplot( aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_wrap(~year) This plot clearly shows how most Asian countries have improved at a much faster rate than European ones. 22.3 Fixed scales for better comparisons The default choice of the range of the axes is an important one. When not using facet, this range is determined by the data shown in the plot. When using facet, this range is determined by the data shown in all plots and therefore kept fixed across plots. This makes comparisons across plots much easier. For example, in the above plot, we can see that life expectancy has increased and the fertility has decreased across most countries. We see this because the cloud of points moves. This is not the case if we adjust the scales: In the plot above, we have to pay special attention to the range to notice that the plot on the right has a larger life expectancy. 22.4 Time series plots The visualizations above effectively illustrates that data no longer supports the western versus developing world view. Once we see these plots, new questions emerge. For example, which countries are improving more and which ones less? Was the improvement constant during the last 50 years or was it more accelerated during certain periods? For a closer look that may help answer these questions, we introduce time series plots. Time series plots have time in the x-axis and an outcome or measurement of interest on the y-axis. For example, here is a trend plot of United States fertility rates: gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(year,fertility)) + geom_point() We see that the trend is not linear at all. Instead there is sharp drop during the 60s and 70s to below 2. Then the trend comes back to 2 and stabilizes during the 90s. When the points are regularly and densely spaced, as they are here, we create curves by joining the points with lines, to convey that these data are from a single country. To do this we use the geom_line function instead of geom_point. gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(year,fertility)) + geom_line() This is particularly helpful when we look at two countries. If we subset the data to include two countries, one from Europe and one from Asia, then adapt the code above: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year,fertility)) + geom_line() Unfortunately, this is not the plot that we want. Rather than a line for each country, the points for both countries are joined. This is actually expected since we have not told ggplot anything about wanting two separate lines. To let ggplot know that there are two curves that need to be made separately, we assign each point to a group, one for each country: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year,fertility, group = country)) + geom_line() #&gt; Warning: Removed 2 rows containing missing values (geom_path). But which line goes with which country? We can assign colors to make this distinction. A useful side-effect of using the color argument to assign different colors to the different countries is that the data is automatically grouped: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year,fertility, col = country)) + geom_line() #&gt; Warning: Removed 2 rows containing missing values (geom_path). The plot clearly shows how South Korea’s fertility rate dropped drastically during the 60s and 70s, and by 1990 had a similar rate to that of Germany. 22.5 Labels for legends For trend plots we recommend labeling the lines rather than using legends since the viewer can quickly see which line is which country. This suggestion actually applies to most plots: labeling is usually preferred over legends. We demonstrate how we can do this using the life expectancy data. We define a data table with the label locations and then use a second mapping just for these labels: labels &lt;- data.frame(country = countries, x = c(1975,1965), y = c(60,72)) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year, life_expectancy, col = country)) + geom_line() + geom_text(data = labels, aes(x, y, label = country), size = 5) + theme(legend.position = &quot;none&quot;) The plot clearly shows how an improvement in life expectancy followed the drops in fertility rates. In 1960, Germans lived 15 years longer than South Koreans, although by 2010 the gap is completely closed. It exemplifies the improvement that many non-western countries have achieved in the last 40 years. 22.6 Example 2: Income distribution Another commonly held notion is that wealth distribution across the world has become worse during the last decades. When general audiences are asked if poor countries have become poorer and rich countries become richer, the majority answers yes. By using stratification, histograms, smooth densities, and boxplots, we will be able to understand if this is in fact the case. We will also learn how transformations can sometimes help provide more informative summaries and plots. 22.7 Transformations The gapminder data table includes a column with the countries gross domestic product (GDP). GDP measures the market value of goods and services produced by a country in a year. The GDP per person is often used as a rough summary of a country’s wealth. Here we divide this quantity by 365 to obtain the more interpretable measure dollars per day. Using current US dollars as a unit, a person surviving on an income of less than $2 a day is defined to be living in absolute poverty. We add this variable to the data table: gapminder &lt;- gapminder %&gt;% mutate(dollars_per_day = gdp/population/365) The GDP values are adjusted for inflation and represent current US dollars, so these values are meant to be comparable across the years. Of course, these are country averages and within each country there is much variability. All the graphs and insights described below relate to country averages and not to individuals. Country income distribution Here is a histogram of per day incomes from 1970: past_year &lt;- 1970 gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) We use the color = &quot;black&quot; argument to draw a boundary and clearly distinguish the bins. In this plot we see that for the majority of countries, averages are below $10 a day. However, the majority of the x-axis is dedicated to the 35 countries with averages above $10. So the plot is not very informative about countries with values below $10 a day. It might be more informative to quickly be able to see how many countries have average daily incomes of about $1 (extremely poor), $2 (very poor), $4 (poor), $8 (middle), $16 (well off), $32 (rich), $64 (very rich) per day. These changes are multiplicative and log transformations convert multiplicative changes into additive ones: when using base 2, a doubling of a value turns into an increase by 1. Here is the distribution if we apply a log base 2 transform: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(log2(dollars_per_day))) + geom_histogram(binwidth = 1, color = &quot;black&quot;) In a way this provides a close up of the mid to lower income countries. Which base? In the case above, we used base 2 in the log transformations. Other common choices are base \\(\\mathrm{e}\\) (the natural log) and base 10. In general, we do not recommend using the natural log for data exploration and visualization. This is because while \\(2^2, 2^3, 2^4, \\dots\\) or \\(10^1, 10^2, \\dots\\) are easy to compute in our heads, the same is not true for \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\). In the dollars per day example, we used base 2 instead of base 10 because the resulting range is easier to interpret. The range of the values being plotted is 0.327, 48.885. In base 10, this turns into a range that includes very few integers: just 0 and 1. With base two, our range includes -2, -1, 0, 1, 2, 3, 4 and 5. It is easier to compute \\(2^x\\) and \\(10^x\\) when \\(x\\) is an integer and between -10 and 10, so we prefer to have smaller integers in the scale. Another consequence of a limited range is that choosing the binwidth is more challenging. With log base 2, we know that a binwidth of 1 will translate to a bin with range \\(x\\) to \\(2x\\). For an example in which base 10 makes more sense, consider population sizes. A log base 10 is preferable since the range for these is: filter(gapminder, year == past_year) %&gt;% summarize(min = min(population), max = max(population)) #&gt; min max #&gt; 1 46075 8.09e+08 Here is the histogram of the transformed values: gapminder %&gt;% filter(year == past_year) %&gt;% ggplot(aes(log10(population))) + geom_histogram(binwidth = 0.5, color = &quot;black&quot;) In the above, we quickly see that country populations range between ten thousand and ten billion. Transform the values or the scale? There are two ways we can use log transformations in plots. We can log the values before plotting them or use log scales in the axes. Both approaches are useful and have different strengths. If we log the data, we can more easily interpret intermediate values in the scale. For example, if we see: —-1—-x—-2——–3—- for log transformed data, we know that the value of \\(x\\) is about 1.5. If the scales are logged: —-1—-x—-10——100— then, to determine x, we need to compute \\(10^{1.5}\\), which is not easy to do in our heads. The advantage of using logged scales is that we see the original values on the axes. However, the advantage of showing logged scales is that the original values are displayed in the plot, which are easier to interpret. For example, we would see “32 dollars a day” instead of “5 log base 2 dollar a day”. As we learned earlier, if we want to scale the axis with logs, we can use the scale_x_ccontinuous function. So instead of logging the values first, we apply this layer: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) Note that the log base 10 transformation has its own function: scale_x_log10(), but currently base 2 does not, although we could easily define our own. There are other transformations available through the trans argument. As we learn later on, the square root (sqrt) transformation, for example, is useful when considering counts. The logistic transformation (logit) is useful when plotting proportions between 0 and 1. The reverse transformation is useful when we want smaller values to be on the right or on top. Modes In statistics these bumps are sometimes referred to as modes. The mode of a distribution is the value with the highest frequency. The mode of the normal distribution is the average. When a distribution, like the one above, doesn’t monotonically decrease from the mode, we call the locations where it goes up and down again local modes and say that the distribution has multiple modes. The histogram above suggests that the 1970 country income distribution has two modes: one at about 2 dollars per day (1 in the log 2 scale) and another at about 32 dollars per day (5 in the log 2 scale). This bimodality is consistent with a dichotomous world made up of countries with average incomes less than $8 (3 in the log 2 scale) a day and countries above that. 22.8 Stratify and boxplot The histogram showed us that the income distribution values show a dichotomy. However, the histogram does not show us if the two groups of countries are west versus the developing world. To see distributions by geographical region, we first stratify the data into regions and then examine the distribution for each. Because of the number of regions: n_distinct(gapminder$region) #&gt; [1] 22 looking at histograms or smooth densities for each will not be useful. Instead, we can stack boxplots next to each other: p &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(region, dollars_per_day)) p + geom_boxplot() Now we can’t read the region names because the default gpplot2 behavior is to write the labels horizontally and, here, we run out of room. We can easily fix this by rotating the labels. Consulting the cheat sheet we find we can rotate the names by changing the theme through element_text. The hjust=1 justifies the text so that it is next to the axis. p + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) We can already see that there is indeed a “west versus the rest”&quot; dichotomy. Do not order alphabetically There are a few more adjustments we can make to this plot that help uncover this reality. First, it helps to order the regions in the boxplots from poor to rich rather than alphabetically. This can be achieved using the reorder function. This function lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. Remember that many graphing functions coerce character vectors into a factor. The default behavior results in alphabetically ordered levels: fac &lt;- factor(c(&quot;Asia&quot;, &quot;Asia&quot;, &quot;West&quot;, &quot;West&quot;, &quot;West&quot;)) levels(fac) #&gt; [1] &quot;Asia&quot; &quot;West&quot; value &lt;- c(10, 11, 12, 6, 4) fac &lt;- reorder(fac, value, FUN = mean) levels(fac) #&gt; [1] &quot;West&quot; &quot;Asia&quot; Second, we can use color to distinguish the different continents, a visual cue that helps find specific regions. Here is the code: p &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% mutate(region = reorder(region, dollars_per_day, FUN = median)) %&gt;% ggplot(aes(region, dollars_per_day, fill = continent)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(&quot;&quot;) p This plot shows two clear groups, with the rich group composed of North America, Northern and Western Europe, New Zealand and Australia. As with the histogram, if we remake the plot using a log scale: p + scale_y_continuous(trans = &quot;log2&quot;) we are better able to see differences within the developing world. Show the data In many cases, we do not show the data because it adds clutter to the plot and obfuscates the message. In the example above, we don’t have that many points so adding them actually lets us see all the data. We can add this layer using geom_point(): p + scale_y_continuous(trans = &quot;log2&quot;) + geom_point(show.legend = FALSE) 22.9 Comparing distributions The exploratory data analysis above has revealed two characteristics about average income distribution in 1970. Using a histogram, we found a bimodal distribution with the modes relating to poor and rich countries. Then by stratifying by region and examining boxplots, we found that the rich countries were mostly in Europe and Northern America, along with Australia and New Zealand. We define a vector with these regions: west &lt;- c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;, &quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) Now we want to focus on comparing the differences in distributions across time. We start by confirming that the bimodality observed in 1970 is explained by a “west versus developing world”&quot; dichotomy. We do this by creating histograms for the previously identified groups. We create the two groups with an ifelse inside a mutate and then we use facet_grid to make a histogram for each group: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% mutate(group = ifelse(region%in%west, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(. ~ group) Now we are ready to see if the separation is worse today than it was 40 years ago. We do this by faceting by both region and year: past_year &lt;- 1970 present_year &lt;- 2010 gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; !is.na(gdp)) %&gt;% mutate(group = ifelse(region%in%west, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year ~ group) Before we interpret the findings of this plot, we notice that there are more countries represented in the 2010 histograms than in 1970: the total counts are larger. One reason for this is that several countries were founded after 1970. For example, the Soviet Union divided into several countries, including Russia and Ukraine, during the 1990s. Another reason is that data was available for more countries in 2010. We remake the plots using only countries with data available for both years. In the data wrangling chapter, we will learn tidyverse tools that permit us to write efficient code for this, but here we can use simple code using the intersect function: country_list_1 &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(dollars_per_day)) %&gt;% .$country country_list_2 &lt;- gapminder %&gt;% filter(year == present_year &amp; !is.na(dollars_per_day)) %&gt;% .$country country_list &lt;- intersect(country_list_1, country_list_2) These 108 account for 86 % of the world population, so this subset should be representative. Let’s remake the plot, but only for this subset by simply adding country %in% country_list to the filter function: We now see that while the rich countries have become a bit richer, percentage-wise, the poor countries appear to have improved more. In particular, we see that the proportion of developing countries earning more than $16 a day increases substantially. To see which specific regions improved the most, we can remake the boxplots we made above but now adding the year 2010: p &lt;- gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% ggplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab(&quot;&quot;) + scale_y_continuous(trans = &quot;log2&quot;) and then using facet to compare the two years: p + geom_boxplot(aes(region, dollars_per_day, fill = continent)) + facet_grid(year~.) Here, we pause to introduce another powerful ggplot2 feature. Because we want to compare each region before and after, it would be convenient to have the 1970 boxplot next to the 2010 boxplot for each region. In general, comparisons are easier when data are plotted next to each other. So, instead of faceting, we keep the data from each year together and ask ggplot2 to color (or fill) them depending on the year. Note that groups are automatically separated by year and each pair of boxplots drawn next to each other. Finally, because year is a number, we turn it into a factor since ggplot2 automatically assigns a color to each category of a factor: p + geom_boxplot(aes(region, dollars_per_day, fill = factor(year))) Finally, we point out that if what we are most interested is in comparing before and after values, it might make more sense to plot the ratios, or difference in the log scale. We are still not ready to learn to code this, but here is what the plot would look like: Density plots We have used data exploration to discover that the income gap between rich and poor countries has narrowed considerably during the last 40 years. We used a series of histograms and boxplots to see this. Here we suggest a succinct way to convey this message with just one plot. We will use smooth density plots. Let’s start by noting that density plots for income distribution in 1970 and 2010 deliver the message that the gap is closing: gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% ggplot(aes(dollars_per_day)) + geom_density(fill = &quot;grey&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year~.) In the 1970 plot, we see two clear modes: poor and rich countries. In 2010, it appears that some of the poor countries have shifted towards the right, closing the gap. The next message we need to convey is that the reason for this change in distribution is that poor countries became richer, rather than some rich countries becoming poorer. To do this, we need to assign a color to the groups we identified during data exploration. However, we first need to learn how to make these smooth densities in a way that preserves information on the number of countries in each group. To understand why we need this, note the discrepancy in the size of each group: group n Developing 87 West 21 Yet when we overlay two densities, the default is to have the area represented by each distribution add up to 1, regardless of the size of each group: gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% mutate(group = ifelse(region %in% west, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day, fill = group)) + scale_x_continuous(trans = &quot;log2&quot;) + geom_density(alpha = 0.2) + facet_grid(year ~ .) which makes it appear as if there are the same number of countries in each group. To change this, we will need to learn to access computed variables with geom_density function. Accessing computed variables To have the areas of these densities be proportional to the size of the groups, we can simply multiply the y-axis values by the size of the group. From the geom_density help file, we see that the functions compute a variable called count that does exactly this. We want this variable to be on the y-axis rather than the density. In ggplot2, we access these variables by surrounding the name with two dots ... So we will use the following mapping: aes(x = dollars_per_day, y = ..count..) We can now create the desired plot by simply changing the mapping in the previous code chunk: p &lt;- gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% mutate(group = ifelse(region %in% west, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day, y = ..count.., fill = group)) + scale_x_continuous(trans = &quot;log2&quot;) p + geom_density(alpha = 0.2) + facet_grid(year ~ .) If we want the densities to be smoother, we use the bw argument. We tried a few and decided on 0.75: p + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .) This plot now shows what is happening very clearly. The developing world distribution is changing. A third mode appears consisting of the countries that most narrowed the gap. 22.9.1 ‘case_when’ We can actually make this figure somewhat more informative. From the exploratory data analysis, we noticed that many of the countries that most improved were from Asia. We can easily alter the plot to show key regions separately. We introduce the case_when function which is useful for defining groups. It currently does not have a data argument (this might change) so we need to access the components of our data table using the dot placeholder: gapminder &lt;- gapminder %&gt;% mutate(group = case_when( .$region %in% west ~ &quot;West&quot;, .$region %in% c(&quot;Eastern Asia&quot;, &quot;South-Eastern Asia&quot;) ~ &quot;East Asia&quot;, .$region %in% c(&quot;Caribbean&quot;, &quot;Central America&quot;, &quot;South America&quot;) ~ &quot;Latin America&quot;, .$continent == &quot;Africa&quot; &amp; .$region != &quot;Northern Africa&quot; ~ &quot;Sub-Saharan Africa&quot;, TRUE ~ &quot;Others&quot;)) We turn this group variable into a factor to control the order of the levels: gapminder &lt;- gapminder %&gt;% mutate(group = factor(group, levels = c(&quot;Others&quot;, &quot;Latin America&quot;, &quot;East Asia&quot;,&quot;Sub-Saharan Africa&quot;, &quot;West&quot;))) We pick this particular order for a reason that becomes clear later. Now we can now easily plot the densities for each region. We use color and size to clearly see the tops: p &lt;- gapminder %&gt;% filter(year %in% c(past_year, present_year) &amp; country %in% country_list) %&gt;% ggplot(aes(dollars_per_day, y = ..count.., fill = group, color = group)) + scale_x_continuous(trans = &quot;log2&quot;) p + geom_density(alpha = 0.2, bw = 0.75, size = 2) + facet_grid(year ~ .) The plot is cluttered and somewhat hard to read. A clearer picture is sometimes achieved by stacking the densities on top of each other: p + geom_density(alpha = 0.2, bw = 0.75, position = &quot;stack&quot;) + facet_grid(year ~ .) Here we can clearly see how the distributions for East Asia, Latin America and others shift markedly to the right. While Sub-Saharan Africa remains stagnant. Notice that we order the levels of the group so that the West’s density are plotted first, then Sub-Saharan Africa. Having the two extremes plotted first allows us see the remaining bimodality better. Weighted densities As a final point, we note that these distributions weigh every country the same. So if most of the population is improving, but living in a very large country, such as China, we might not appreciate this. We can actually weight the smooth densities using the weight mapping argument. The plot then looks like this: This particular figure shows very clearly how the income distribution gap is closing with most of the poor remaining in Sub-Saharan Africa. 22.10 Ecological fallacy Throughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income. We start by comparing these quantities across regions, but, before doing this, we define a few more regions: gapminder &lt;- gapminder %&gt;% mutate(group = case_when( .$region %in% west ~ &quot;The West&quot;, .$region %in% &quot;Northern Africa&quot; ~ &quot;Northern Africa&quot;, .$region %in% c(&quot;Eastern Asia&quot;, &quot;South-Eastern Asia&quot;) ~ &quot;East Asia&quot;, .$region == &quot;Southern Asia&quot;~ &quot;Southern Asia&quot;, .$region %in% c(&quot;Central America&quot;, &quot;South America&quot;, &quot;Caribbean&quot;) ~ &quot;Latin America&quot;, .$continent == &quot;Africa&quot; &amp; .$region != &quot;Northern Africa&quot; ~ &quot;Sub-Saharan Africa&quot;, .$region %in% c(&quot;Melanesia&quot;, &quot;Micronesia&quot;, &quot;Polynesia&quot;) ~ &quot;Pacific Islands&quot;)) We then compute these quantities for each region: surv_income &lt;- gapminder %&gt;% filter(year %in% present_year &amp; !is.na(gdp) &amp; !is.na(infant_mortality) &amp; !is.na(group)) %&gt;% group_by(group) %&gt;% summarize(income = sum(gdp)/sum(population)/365, infant_survival_rate = 1-sum(infant_mortality/1000*population)/sum(population)) surv_income %&gt;% arrange(income) #&gt; # A tibble: 7 x 3 #&gt; group income infant_survival_rate #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Sub-Saharan Africa 1.76 0.936 #&gt; 2 Southern Asia 2.07 0.952 #&gt; 3 Pacific Islands 2.70 0.956 #&gt; 4 Northern Africa 4.94 0.970 #&gt; 5 Latin America 13.2 0.983 #&gt; 6 East Asia 13.4 0.985 #&gt; # ... with 1 more row This shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%! The relationship between these two variables is almost perfectly linear: In this plot, we introduce the use of the limit argument which lets us change the range of the axes. We are making the range larger than the data requires because we will later compare this plot to one with more variability and we want the ranges to be the same. We also introduce the breaks argument, which lets us set the location of the axes labels. Finally, we introduce a new transformation, the logistic transformation. Logistic transformation The logistic or logit transformation for a proportion or rate \\(p\\) is defined as: \\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\] When \\(p\\) is a proportion or probability, the quantity that is being logged, \\(p/(1-p)\\) is called the odds. In this case \\(p\\) is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments respectively. This scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases. Show the data Now, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that all survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on? Jumping to this conclusion based on a plot showing averages is referred to as the ecological fallacy. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story: Specifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries. "],
["data-visualization-principles.html", "Chapter 23 Data visualization principles 23.1 Encoding data using visual cues 23.2 Know when to include 0 23.3 Do not distort quantities 23.4 Order by a meaningful value 23.5 Show the data 23.6 Ease comparisons: use common axes 23.7 Ease comparisons: align plots vertically to see horizontal changes and horizontally to see vertical changes 23.8 Consider transformations 23.9 Ease comparisons: Visual cues to be compared should be adjacent. 23.10 Ease comparison: use color 23.11 Think of the color blind 23.12 Use scatterplots to examine the relationship between two variables 23.13 Encoding a third variable 23.14 Avoid pseudo three dimensional plots 23.15 Avoid gratuitous three dimensional plots 23.16 Avoid too many significant digits 23.17 Further reading: Exercises", " Chapter 23 Data visualization principles We have already provided some rules to follow as we created plots for our examples. Here, we aim to provide some general principles we can use as a guide for effective data visualization. Much of this section is based on a talk by Karl Broman titled “Creating effective figures and tables” and includes some of the figures which were made with code that Karl makes available on his GitHub repository, as well as class notes from Peter Aldhous’ Introduction to Data Visualization course. Following Karl’s approach, we show some examples of plot styles we should avoid, explain how to improve them, and use these as motivation for a list of principles. We compare and contrast plots that follow these principles to those that don’t. The principles are mostly based on research related to how humans detect patterns and make visual comparisons. The preferred approaches are those that best fit the way our brains process visual information. When deciding on a visualization approach, it is also important to keep our goal in mind. We may be comparing a viewable number of quantities, describing distribution for categories or numeric values, comparing the data from two groups, or describing the relationship between two variables. As a final note, we want to emphasize that for a data scientist it is important to adapt and optimize graphs to the audience. For example, an exploratory plot made for ourselves will be different than a chart intended to communicate a finding to a general audience. We will be using these libraries: library(tidyverse) library(gridExtra) library(dslabs) 23.1 Encoding data using visual cues We start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue. To illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. Here, for each year, we are simply comparing four quantities – the four percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart: Figure 23.1: Pie chart of browser usage. Here we are representing quantities with both areas and angles since both the angle and area of each pie slice is proportional to the quantity it represents. This turns out to be a suboptimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when only area is the only available visual cue. The donut chart is an example of a plot that uses only area: Figure 23.2: Pie chart of browser usage. To see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the pie R function help file states that: Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data. In this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy. Browser 2000 2015 Opera 3 2 Safari 21 22 Firefox 23 21 Chrome 26 29 IE 28 27 The preferred way to plot these quantities is to use length and position as visual cues since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures. Figure 23.3: Barplot of browser usage. Notice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis. If for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area: In general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once. 23.2 Know when to include 0 When using barplots it is dishonest not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example: (Source: Fox News, via Peter Aldhous via Media Matters via Fox News) via Media Matters. From the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly: Here is another example, described in detail: here. which makes a 13% increase look like a five fold change. Here is the appropriate plot: When using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within the group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012: p1 &lt;- gapminder %&gt;% filter(year == 2012) %&gt;% ggplot(aes(continent, life_expectancy)) + geom_point() p2 &lt;- p1 + scale_y_continuous(limits = c(0, 84)) grid.arrange(p2, p1, ncol = 2) Note that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability. 23.3 Do not distort quantities During President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations: Judging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area: Not surprisingly, ggplot defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length: 23.4 Order by a meaningful value When one of the axes is used to show categories, as is done in barplots, the default ggplot behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead we should order by a meaningful quantity. In all the cases above, the barplots where ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case we kept the order the same across the barplots to ease the comparison. Specifically instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015. We previously learned how to use the reorder function, which helps us achieve this goal. To appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate: data(murders) p1 &lt;- murders %&gt;% mutate(murder_rate = total / population * 100000) %&gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + xlab(&quot;&quot;) p2 &lt;- murders %&gt;% mutate(murder_rate = total / population * 100000) %&gt;% mutate(state = reorder(state, murder_rate)) %&gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + xlab(&quot;&quot;) grid.arrange(p1, p2, ncol = 2) The reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other: The first orders the regions alphabetically, while the second orders them by the group’s median. 23.5 Show the data We have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups. To motivate our first principle, ‘show the data’, we go back to our artificial example of describing heights to ET, an extraterrestrial. This time let’s assume ET is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, shows the average and standard errors (standard errors are defined in a later chapter, but do not confuse them with the standard deviation of the data). The plot looks like this: The average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0, does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution. This brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points: heights %&gt;% ggplot(aes(sex, height)) + geom_point() For example, this plot gives us an idea of the range of the data. However, this plot has limitations as well since we can’t really see all the 238 and 812 points plotted for females and males respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points. The first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the height of the points do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending: heights %&gt;% ggplot(aes(sex, height)) + geom_jitter(width = 0.1, alpha = 0.2) Now we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer. 23.6 Ease comparisons: use common axes Since there are so many points, it is more effective to show distributions rather than individual points. We therefore show histograms for each group: However, from this plot it is not immediately obvious that males are, on average, taller than females. We have to look carefully to notice that the x-axis has a higher range of values in the male histogram. An important principle here is to keep the axes the same when comparing data across to plots. Below we see how the comparison becomes easier: 23.7 Ease comparisons: align plots vertically to see horizontal changes and horizontally to see vertical changes In these histograms, the visual cue related to decreases or increases in height are shifts to the left or right respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed: p2 &lt;- heights %&gt;% ggplot(aes(height, ..density..)) + geom_histogram(binwidth = 1, color=&quot;black&quot;) + facet_grid(sex~.) p2 This plot makes it much easier to notice that men are, on average, taller. If, instead of histograms, we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points: p3 &lt;- heights %&gt;% ggplot(aes(sex, height)) + geom_boxplot(coef=3) + geom_jitter(width = 0.1, alpha = 0.2) + ylab(&quot;Height in inches&quot;) p3 Now contrast and compare these three plots, based on exactly the same data: grid.arrange(p1, p2, p3, ncol = 3) Notice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions. 23.8 Consider transformations We have motivated the use of the log transformation in cases where the changes are multiplicative. Population size was an example in which we found a log transformation to yield a more informative transformation. The combination of an incorrectly chosen barplot and a failure to use a log transformation when one is merited can be particularly distorting. As an example, consider this barplot showing the average population sizes for each continent in 2015: From this plot, one would conclude that countries in Asia are much more populous than in other continents. Following the show the data principle, we quickly notice that this is due to two very large countries, which we assume are India and China: Here, using a log transformation provides a much more informative plot. We compare the original barplot to a boxplot using the log scale transformation for the y-axis: With the new plot, we realize that countries in Africa actually have a larger median population size than those in Asia. Other transformations you should consider are the logistic transformation, useful to better see fold changes in odds, and the square root transformation, useful for count data. 23.9 Ease comparisons: Visual cues to be compared should be adjacent. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below. A difference is that here we look at continents instead of regions, but this is not relevant to the point we are making. For each continent, we want to compare the distributions from 1970 to 2010. The default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging. But it is much easier to make the comparison when the boxplots are next to each other: 23.10 Ease comparison: use color The comparison becomes even easier to make if we use color to denote the two things we want to compare: 23.11 Think of the color blind About 10% of the population is color blind. Unfortunately, the default colors used in ggplot are not optimal for this group. However, ggplot does it make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: color_blind_friendly_cols &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) p1 &lt;- data.frame(x=1:8, y=1:8, col = as.character(1:8)) %&gt;% ggplot(aes(x, y, color = col)) + geom_point(size=5) p1 + scale_color_manual(values=color_blind_friendly_cols) There are several resources that can help you select colors, for example this one. 23.12 Use scatterplots to examine the relationship between two variables In every single instance in which we have examined the relationship between two variables, including total murders versus population size, life expectancy versus fertility rates, and infant mortality versus income, we have used scatterplots. This is the plot we generally recommend. 23.12.1 Slope charts One exception where another type of plot may be more informative is when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a slope chart. There is no geometry for slope charts in ggplot2 but we can construct one using geom_lines. We need to do some tinkering to add labels. Below is an example comparing 2010 to 2015 for large western countries: west &lt;- c(&quot;Western Europe&quot;,&quot;Northern Europe&quot;,&quot;Southern Europe&quot;, &quot;Northern America&quot;,&quot;Australia and New Zealand&quot;) dat &lt;- gapminder %&gt;% filter(year%in% c(2010, 2015) &amp; region %in% west &amp; !is.na(life_expectancy) &amp; population &gt; 10^7) dat %&gt;% mutate(location = ifelse(year == 2010, 1, 2), location = ifelse(year == 2015 &amp; country %in% c(&quot;United Kingdom&quot;,&quot;Portugal&quot;), location+0.22, location), hjust = ifelse(year == 2010, 1, 0)) %&gt;% mutate(year = as.factor(year)) %&gt;% ggplot(aes(year, life_expectancy, group = country)) + geom_line(aes(color = country), show.legend = FALSE) + geom_text(aes(x = location, label = country, hjust = hjust), show.legend = FALSE) + xlab(&quot;&quot;) + ylab(&quot;Life Expectancy&quot;) An advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot: In the scatterplot, we have followed the principle use common axes since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines. 23.12.2 Bland-Altman plot Since we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also know as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average: library(ggrepel) dat %&gt;% mutate(year = paste0(&quot;life_expectancy_&quot;, year)) %&gt;% select(country, year, life_expectancy) %&gt;% spread(year, life_expectancy) %&gt;% mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2, difference = life_expectancy_2015 - life_expectancy_2010) %&gt;% ggplot(aes(average, difference, label = country)) + geom_point() + geom_text_repel() + geom_abline(lty = 2) + xlab(&quot;Average of 2010 and 2015&quot;) + ylab(&quot;Difference between 2015 and 2010&quot;) Here, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis. 23.13 Encoding a third variable An earlier scatterplot showed the relationship between infant survival and average income. Below is a version of this plot that encodes three variables: OPEC membership, region, and population. We encode categorical variables with color and shape. These shapes can be controlled with shape argument. Below are the shapes available for use in R. For the last five, the color goes inside. For continuous variables we can use color, intensity or size. We now show an example of how we do this with a case study. When selecting colors to quantify a numeric variable, we choose between two options: sequential and diverging. Sequential colors are suited for data that goes from high to low. High values are clearly distinguished from low values. Here are some examples offered by the package RColorBrewer library(RColorBrewer) display.brewer.all(type=&quot;seq&quot;) Diverging colors are used to represent values that diverge from a center. We put equal emphasis on both ends of the data range: higher than the center and lower than the center. An example of when we would use a divergent pattern would be if we were to show height in standard deviations away from the average. Here are some examples of divergent patterns: library(RColorBrewer) display.brewer.all(type=&quot;div&quot;) 23.14 Avoid pseudo three dimensional plots The figure below, taken from the scientific literature, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable. Humans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D: Figure 23.4: This plot demonstrates that using color is more than enough to distinguish the three lines. Notice how much easier it is to determine the survival values. 23.15 Avoid gratuitous three dimensional plots Pseudo 3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples: 23.16 Avoid too many significant digits By default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information, and the added the visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates for California across the five decades state year Measles Pertussis Polio California 1940 37.8826320 18.3397861 18.3397861 California 1950 13.9124205 4.7467350 4.7467350 California 1960 14.1386471 0.0000000 0.0000000 California 1970 0.9767889 0.0000000 0.0000000 California 1980 0.3743467 0.0515466 0.0515466 We are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing: state year Measles Pertussis Polio California 1940 37.9 18.3 18.3 California 1950 13.9 4.7 4.7 California 1960 14.1 0.0 0.0 California 1970 1.0 0.0 0.0 California 1980 0.4 0.1 0.1 Useful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3). Another principle, related to displaying tables, is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one: state disease 1940 1950 1960 1970 1980 California Measles 37.9 13.9 14.1 1 0.4 California Pertussis 18.3 4.7 0.0 0 0.1 California Polio 18.3 4.7 0.0 0 0.1 ## Know your audience Graphs can be used for our 1) own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot. As a simple example, consider that for your own exploration, it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis will be much easier to digest. 23.17 Further reading: ER Tufte (1983) The visual display of quantitative information. Graphics Press. ER Tufte (1990) Envisioning information. Graphics Press. ER Tufte (1997) Visual explanations. Graphics Press. WS Cleveland (1993) Visualizing data. Hobart Press. WS Cleveland (1994) The elements of graphing data. CRC Press. A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130. NB Robbins (2004) Creating more effective graphs. Wiley. Nature Methods columns A Cairo (2013) The Functional Art: An Introduction to Information Graphics and Visualization. New Riders. N Yau (2013) Data Points: Visualization That Means Something. Wiley. Exercises For these exercises, we will be using the vaccines data in the dslabs package: library(dslabs) data(us_contagious_diseases) Pie charts are appropriate: A. When we want to display percentages. B. When ggplot2 is not available. C. When I am in a bakery. D. Never. Barplots and tables are always better. What is the problem with the plot below: A. The values are wrong. The final vote was 306 to 232. B. The axis does not start at 0. Judging by the length, it appears Trump received 3 times as many votes when in fact it was about 30% more. C. The colors should be the same. D. Percentages should be shown as a pie chart. Take a look at the following two plots. They show the same information: 1928 rates of measles across the 50 states. Which plot is easier to read if you are interested in determining which are the best and worst states in terms of rates, and why? A. They provide the same information so they are both equally as good. B. The plot on the right is better because it orders the states alphabetically. C. The plot on the right is better because alphabetical order has nothing to do with the disease and by ordering according to actual rate, we quickly see the states with most and least rates. D. Both plots should be a pie chart. To make the plot on the left, we have to reorder the levels of the states’ variables. dat &lt;- us_contagious_diseases %&gt;% filter(year == 1967 &amp; disease==&quot;Measles&quot; &amp; !is.na(population)) %&gt;% mutate(rate = count / population * 10000 * 52 / weeks_reporting) Note what happens when we make a barplot: dat %&gt;% ggplot(aes(state, rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() Define these objects: state &lt;- dat$state rate &lt;- dat$count/dat$population*10000*52/dat$weeks_reporting Redefine the state object so that the levels are re-ordered. Print the new object state and its levels so you can see that the vector is not re-ordered by the levels. Now with one line of code, define the dat table as done above, but change the use mutate to create a rate variable and reorder the state variable so that the levels are reordered by this variable. Then make a barplot using the code above, but for this new dat. Say we are interested in comparing gun homicide rates across regions of the US. We see this plot: library(dslabs) data(&quot;murders&quot;) murders %&gt;% mutate(rate = total/population*100000) %&gt;% group_by(region) %&gt;% summarize(avg = mean(rate)) %&gt;% mutate(region = factor(region)) %&gt;% ggplot(aes(region, avg)) + geom_bar(stat=&quot;identity&quot;) + ylab(&quot;Murder Rate Average&quot;) and decide to move to a state in the western region. What is the main problem with this interpretaion? A. The categories are ordered alphabetically. B. The graph does not show standard errors. C. It does not show all the data. We do not see the variability within a region and it’s possible that the safest states are not in the West. D. The Northeast has the lowest average. Answer C Make a boxplot of the murder rates. data(&quot;murders&quot;) murders %&gt;% mutate(rate = total/population*100000) #&gt; state abb region population total rate #&gt; 1 Alabama AL South 4779736 135 2.824 #&gt; 2 Alaska AK West 710231 19 2.675 #&gt; 3 Arizona AZ West 6392017 232 3.630 #&gt; 4 Arkansas AR South 2915918 93 3.189 #&gt; 5 California CA West 37253956 1257 3.374 #&gt; 6 Colorado CO West 5029196 65 1.292 #&gt; 7 Connecticut CT Northeast 3574097 97 2.714 #&gt; 8 Delaware DE South 897934 38 4.232 #&gt; 9 District of Columbia DC South 601723 99 16.453 #&gt; 10 Florida FL South 19687653 669 3.398 #&gt; 11 Georgia GA South 9920000 376 3.790 #&gt; 12 Hawaii HI West 1360301 7 0.515 #&gt; 13 Idaho ID West 1567582 12 0.766 #&gt; 14 Illinois IL North Central 12830632 364 2.837 #&gt; 15 Indiana IN North Central 6483802 142 2.190 #&gt; 16 Iowa IA North Central 3046355 21 0.689 #&gt; 17 Kansas KS North Central 2853118 63 2.208 #&gt; 18 Kentucky KY South 4339367 116 2.673 #&gt; 19 Louisiana LA South 4533372 351 7.743 #&gt; 20 Maine ME Northeast 1328361 11 0.828 #&gt; 21 Maryland MD South 5773552 293 5.075 #&gt; 22 Massachusetts MA Northeast 6547629 118 1.802 #&gt; 23 Michigan MI North Central 9883640 413 4.179 #&gt; 24 Minnesota MN North Central 5303925 53 0.999 #&gt; 25 Mississippi MS South 2967297 120 4.044 #&gt; 26 Missouri MO North Central 5988927 321 5.360 #&gt; 27 Montana MT West 989415 12 1.213 #&gt; 28 Nebraska NE North Central 1826341 32 1.752 #&gt; 29 Nevada NV West 2700551 84 3.110 #&gt; 30 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 31 New Jersey NJ Northeast 8791894 246 2.798 #&gt; 32 New Mexico NM West 2059179 67 3.254 #&gt; 33 New York NY Northeast 19378102 517 2.668 #&gt; 34 North Carolina NC South 9535483 286 2.999 #&gt; 35 North Dakota ND North Central 672591 4 0.595 #&gt; 36 Ohio OH North Central 11536504 310 2.687 #&gt; 37 Oklahoma OK South 3751351 111 2.959 #&gt; 38 Oregon OR West 3831074 36 0.940 #&gt; 39 Pennsylvania PA Northeast 12702379 457 3.598 #&gt; 40 Rhode Island RI Northeast 1052567 16 1.520 #&gt; 41 South Carolina SC South 4625364 207 4.475 #&gt; 42 South Dakota SD North Central 814180 8 0.983 #&gt; 43 Tennessee TN South 6346105 219 3.451 #&gt; 44 Texas TX South 25145561 805 3.201 #&gt; 45 Utah UT West 2763885 22 0.796 #&gt; 46 Vermont VT Northeast 625741 2 0.320 #&gt; 47 Virginia VA South 8001024 250 3.125 #&gt; 48 Washington WA West 6724540 93 1.383 #&gt; 49 West Virginia WV South 1852994 27 1.457 #&gt; 50 Wisconsin WI North Central 5686986 97 1.706 #&gt; 51 Wyoming WY West 563626 5 0.887 by region, showing all the points and ordering the regions by their median rate. The plots below show three continuous variables. The line \\(x=2\\) appears to separate the points. But it is actually not the case, which we can see by plotting the data in a couple of two dimensional points. Why is this happening? A. Humans are not good at reading pseudo 3D plots. B. There must be an error in the code C. The colors confuse us. D. Scatterplots should not be used to compare two variables when we have access to 3. Reproduce the image plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks. Now reproduce the time series plot we previously made, but this time following the instructions of the previous question. For the state of California, make a time series plots showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease. Now do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population. "],
["case-study-the-impact-of-vaccines-on-battling-infectious-diseases.html", "Chapter 24 Case study: the impact of vaccines on battling infectious diseases", " Chapter 24 Case study: the impact of vaccines on battling infectious diseases Vaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today, despite all the scientific evidence for their importance, vaccination programs have become somewhat controversial. The controversy started with a paper published in 1988 and lead by Andrew Wakefield claiming there was a link between the administration of the measles, mumps and rubella (MMR) vaccine, and the appearance of autism and bowel disease. Despite much scientific evidence contradicting this finding, sensationalist media reports and fear mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Center for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR). The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines. Effective communication of data is a strong antidote to misinformation and fear mongering. Earlier we used an example provided by a Wall Street Journal article showing data related to the impact of vaccines on battling infectious diseases. Here, we reconstruct that example. The data used for these plots were collected, organized and distributed by the Tycho Project. They include weekly reported counts data for seven diseases from 1928 to 2011, from all fifty states. We include the yearly totals in the dslabs package: data(us_contagious_diseases) str(us_contagious_diseases) #&gt; &#39;data.frame&#39;: 18870 obs. of 6 variables: #&gt; $ disease : Factor w/ 7 levels &quot;Hepatitis A&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ state : Factor w/ 51 levels &quot;Alabama&quot;,&quot;Alaska&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ year : num 1966 1967 1968 1969 1970 ... #&gt; $ weeks_reporting: int 50 49 52 49 51 51 45 45 45 46 ... #&gt; $ count : num 321 291 314 380 413 378 342 467 244 286 ... #&gt; $ population : num 3345787 3364130 3386068 3412450 3444165 ... We create a temporary object dat that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 50s. Note that there is a weeks_reporting column that tells us for how many weeks of the year data was reported. So we have to adjust for that when computing the rate. the_disease &lt;- &quot;Measles&quot; dat &lt;- us_contagious_diseases %&gt;% filter(!state%in%c(&quot;Hawaii&quot;,&quot;Alaska&quot;) &amp; disease == the_disease) %&gt;% mutate(rate = count / population * 10000 * 52 / weeks_reporting) %&gt;% mutate(state = reorder(state, rate)) We can now easily plot disease rates per year. Here are the measles data from California: dat %&gt;% filter(state == &quot;California&quot; &amp; !is.na(rate)) %&gt;% ggplot(aes(year, rate)) + geom_line() + ylab(&quot;Cases per 10,000&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) We add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 the yellow book. p. 250. ISBN 9780199948505]. Now, can we show data for all states in one plot? We have three variables to show: year, state and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved. In our example, we want to use a sequential palette since there is no meaningful center, just low and high rates. We use the geometry geom_tile to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Once a disease was pretty much eradicated, some states stopped reporting cases all together. dat %&gt;% ggplot(aes(year, state, fill = rate)) + geom_tile(color = &quot;grey50&quot;) + scale_x_continuous(expand=c(0,0)) + scale_fill_gradientn(colors = brewer.pal(9, &quot;Reds&quot;), trans = &quot;sqrt&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) + theme_minimal() + theme(panel.grid = element_blank()) + ggtitle(the_disease) + ylab(&quot;&quot;) + xlab(&quot;&quot;) This plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity which we earlier explained makes it harder to know exactly how high it is going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this: avg &lt;- us_contagious_diseases %&gt;% filter(disease==the_disease) %&gt;% group_by(year) %&gt;% summarize(us_rate = sum(count, na.rm=TRUE)/sum(population, na.rm=TRUE)*10000) Now to make the plot we simply use the geom_line geometry: dat %&gt;% ggplot() + geom_line(aes(year, rate, group = state), color = &quot;grey50&quot;, show.legend = FALSE, alpha = 0.2, size = 1) + geom_line(mapping = aes(year, us_rate), data = avg, size = 1, color = &quot;black&quot;) + scale_y_continuous(trans = &quot;sqrt&quot;, breaks = c(5,25,125,300)) + ggtitle(&quot;Cases per 10,000 by state&quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + geom_text(data = data.frame(x=1955, y=50), mapping = aes(x, y, label=&quot;US average&quot;), color=&quot;black&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) #&gt; Warning: Removed 180 rows containing missing values (geom_path). In theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors. "],
["introduction-3.html", "Chapter 25 Introduction", " Chapter 25 Introduction Knowing how to compute probabilities gives you an edge in games of chance. Throughout history many smart individuals, including famous mathematicians such as Cardano, Fermat and Pascal, spent time and energy thinking through the math of these games. As a result, Probability Theory was born. Probability continues to be highly useful in modern games of chance. For example, in poker, we can compute the probability of winning a hand based on the cards on the table. Also, casinos rely on probability theory to develop games that guarantee a profit. Probability theory is useful in many other contexts and, in particular, in areas that depend on data affected by chance in some way. Knowledge of probability is therefore indispensable for data science. In games of chance, probability has a very intuitive definition. For instance, we know what it means that the chance of a pair of dice coming up seven is 1 in 6. However, this is not the case in other contexts. Today probability is being used much more broadly with the word probability commonly used in everyday language. Google’s auto-complete of “What are the chances of” give us: “having twins”, “rain today”, “getting struck by lightning”, and “getting cancer”. The word probability is also used by election forecasters. In 2008, Nate Silver gave Obama a 94% chance of winning. In 2012 it was a 90% chance. Obama won both elections. In 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. She lost. But 71% is still more than 50%. Was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed somewhere? To answer these questions, we will need to learn, among other things, some probability theory. We cover election forecasting in the next chapter since we also need to understand the concepts of statistical inference, which we explain in that chapter. But keep in mind that statistical inference builds upon probability theory. The motivation for this chapter are the circumstances surrounding the financial crisis of 2007-2008. This financial crisis was in part caused by underestimating the risk of certain securities sold by financial institutions. Specifically, the risk of mortgage-backed securities (MBS) and collateralized debt obligations (CDO) were grossly underestimated. These MBS and CDO were sold at prices that assumed most homeowners would make their monthly payments, and the probability of this not occurring was calculated as being low. A combination of factors resulted in many more defaults than were expected, which led to a price crash of these securities. As a consequence, banks lost so much money that they needed government bailouts to avoid closing down completely. To begin to comprehend this very complicated event, we need to understand the basics of probability. We will introduce important concepts such as random variables, independence, Monte Carlo simulations, expected values, standard errors, and the Central Limit Theorem. Before using probability concepts to understand our motivating example, we will use several examples related to games of chance since these are simple and illustrative. "],
["discrete-probability.html", "Chapter 26 Discrete probability 26.1 Relative frequency 26.2 Notation 26.3 Monte Carlo simulations 26.4 Setting the random seed 26.5 Probability distributions 26.6 Independence 26.7 Conditional probabilities 26.8 Multiplication rule 26.9 Combinations and permutations 26.10 Birthday problem 26.11 How many Monte Carlo experiments are enough 26.12 Addition rule 26.13 Monty Hall problem Exercises", " Chapter 26 Discrete probability We start by covering some basic principles related to categorical data. The subset of probability is referred to as discrete probability. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications. Discrete probability is more useful in card games and therefore we use these as examples. 26.1 Relative frequency The word probability is used in everyday language. Answering questions about probability is often hard, if not impossible. Here we discuss a mathematical definition of probability that does permit us to give precise answers to certain questions. For example, if I have 2 red beads and 3 blue beads inside an urn and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is .4 for red and .6 for blue. A more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment over and over, independently, and under the same conditions. 26.2 Notation We use the notation \\(\\mbox{Pr}(A)\\) to denote the probability of event \\(A\\) happening. We use the very general term event to refer to things that can happen when something occurs by chance. For example, in our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”. In data science applications, we will often deal with continuous variables. In these cases, events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: \\(X \\geq 6\\). We will see more of these examples later. Here we focus on categorical data. 26.3 Monte Carlo simulations Computers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random. An example is the sample function in R. We demonstrate its use in the code below. First, we use the function rep to generate the urn: beads &lt;- rep( c(&quot;red&quot;, &quot;blue&quot;), times = c(2,3)) beads #&gt; [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; and then use sample to pick a bead at random: sample(beads, 1) #&gt; [1] &quot;blue&quot; This line of code produces one random outcome. We want to repeat this experiment “over and over”, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent. This is an example of a Monte Carlo simulation. Much of what mathematical and theoretical statisticians study, which we do not cover in this book, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this section, we provide a practical approach to deciding what is “large enough”. To perform our first Monte Carlo simulation, we use the replicate function, which permits us to repeat the same task any number of times. Here we repeat the random event \\(B =\\) 10,000 times: B &lt;- 10000 events &lt;- replicate(B, sample(beads, 1)) We can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use table to see the distribution: tab &lt;- table(events) tab #&gt; events #&gt; blue red #&gt; 5954 4046 and prop.table gives us the proportions: prop.table(tab) #&gt; events #&gt; blue red #&gt; 0.595 0.405 The numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that \\(B\\) gets larger as the estimates get closer to 3/5=.6 and 2/5=.4. Although this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R. 26.4 Setting the random seed Before we continue, we will briefly explain the following important line of code: set.seed(1) Throughout this book, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the book may show a different result than what you obtain when you try to code shown in the book. This is actually fine since the results are random and change from time to time. However, if you want to to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1. ?set.seed In the exercises we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be. With and without replacement The function sample has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads: sample(beads, 5) #&gt; [1] &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; sample(beads, 5) #&gt; [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;blue&quot; &quot;red&quot; sample(beads, 5) #&gt; [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; This results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error: sample(beads, 6) Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE' However, the sample function can be used directly, without the use of replicate, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this we sample with replacement: return the bead back to the urn after selecting it. We can tell sample to do this changing the replace argument , which defaults to FALSE, to replace=TRUE: events &lt;- sample(beads, B, replace = TRUE) prop.table(table(events)) #&gt; events #&gt; blue red #&gt; 0.597 0.403 Not surprisingly, we get results very similar to those previously obtained with replicate. 26.5 Probability distributions Defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution. If we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided and 2% Green Party, these proportions define the probability for each group. The probability distribution is: \\[ \\mbox{Pr}(\\mbox{picking a Republican})=0.44\\\\ \\mbox{Pr}(\\mbox{picking a Democrat})=0.44\\\\ \\mbox{Pr}(\\mbox{picking an undecided})=0.10\\\\ \\mbox{Pr}(\\mbox{picking a Green})=0.02\\\\ \\] 26.6 Independence We say two events are independent if the outcome of one does not affect the other. The classic example are coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws. Many examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, \\(\\dots\\), Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore not independent. The first outcome affected the next one. To see an extreme case of non-independent events, consider our example of drawing five beads at random without replacement: x &lt;- sample(beads, 5) If you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes: x[2:5] #&gt; [1] &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; would you still guess blue? Of course not. Now you know that the probability of red is 1. The events are not independent so the probabilities changes. 26.7 Conditional probabilities When events are not independent, conditional probabilities are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation: \\[ \\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51 \\] We use the \\(\\mid\\) as shorthand for “given that” or “conditional on”. When two events, say \\(A\\) and \\(B\\), are independent, we have: \\[ \\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A) \\] This the mathematical way of saying: the fact that \\(B\\) happened does not affect the probability of \\(A\\) happening. In fact, this can be considered the mathematical definition of independence. 26.8 Multiplication rule If we want to know the probability of two events, say \\(A\\) and \\(B\\), occurring, we can use the multiplication rule: \\[ \\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A) \\] Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose). So, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being and Ace and multiply by the probability of drawing a face card given that the first was an Ace: \\(1/13 \\times 12/52 \\approx 0.02\\) The multiplicative rule also applies to more than two events. We can use induction to expand for more events: \\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\mbox{ and } B) \\] Multiplication rule under indepedence When we have independent events, then the multiplication rule becomes simpler: \\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C) \\] But we have to be very careful before using this, as assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence. As an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches so using the multiplication rule we conclude that only \\(1/10 \\times 1/5\\) or 0.02 have both. But to multiply like this we need to assume independence! The conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: 0.09. The multiplication rule also gives us a general formula for computing conditional probabilities: \\[ \\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\] To illustrate how we use these formulas and concepts in practice, we will use several examples related to card games. 26.9 Combinations and permutations In our very first example we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue. For more complicated cases the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush”&quot; in Poker? In a Discrete Probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers. First let’s construct a deck of cards. For this we will use the expand.grid and paste functions. We use paste to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this: number &lt;- &quot;Three&quot; suit &lt;- &quot;Hearts&quot; paste(number, suit) #&gt; [1] &quot;Three Hearts&quot; paste also works on pairs of vectors performing the operation element-wise: paste(letters[1:5], as.character(1:5)) #&gt; [1] &quot;a 1&quot; &quot;b 2&quot; &quot;c 3&quot; &quot;d 4&quot; &quot;e 5&quot; The function expand.grid gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey and plaid shirts, all your combinations are: expand.grid(pants = c(&quot;blue&quot;, &quot;black&quot;), shirt = c(&quot;white&quot;, &quot;grey&quot;, &quot;plaid&quot;)) #&gt; pants shirt #&gt; 1 blue white #&gt; 2 black white #&gt; 3 blue grey #&gt; 4 black grey #&gt; 5 blue plaid #&gt; 6 black plaid So here is how we generate a deck of cards: suits &lt;- c(&quot;Diamonds&quot;, &quot;Clubs&quot;, &quot;Hearts&quot;, &quot;Spades&quot;) numbers &lt;- c(&quot;Ace&quot;, &quot;Deuce&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;, &quot;Six&quot;, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;, &quot;Ten&quot;, &quot;Jack&quot;, &quot;Queen&quot;, &quot;King&quot;) deck &lt;- expand.grid(number=numbers, suit=suits) deck &lt;- paste(deck$number, deck$suit) With the deck constructed, we can now double check that the probability of a King in the first card is 1/13. We simply compute the proportion of possible outcomes that satisfy our condition: kings &lt;- paste(&quot;King&quot;, suits) mean(deck %in% kings) #&gt; [1] 0.0769 which is 1/13. Now, how about the conditional probability of the second card being a King given that the first was a King? Earlier we deduced that if one King is already out of the deck and there are 51 left then this probability is 3/51. Let’s confirm by listing out all possible outcomes. To do this, we can use the permutations function from the gtools package. This function computes, for any list of size n, all the different combinations we can get when we select r items. So here are all the ways we can choose two numbers from a list consisting of 1,2,3: library(gtools) permutations(3, 2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 1 #&gt; [4,] 2 3 #&gt; [5,] 3 1 #&gt; [6,] 3 2 Notice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2) and (3,3) do not appear because once we pick a number, it can’t appear again. Optionally, we can add a vector. So if you want to see five random seven digit phone numbers out of all possible phone numbers, you can type: all_phone_numbers &lt;- permutations(10, 7, v = 0:9) n &lt;- nrow(all_phone_numbers) index &lt;- sample(n, 5) all_phone_numbers[index,] Instead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9. To compute all possible ways we can choose two cards when the order matters, we type: hands &lt;- permutations(52, 2, v = deck) This is a matrix with two columns and 2652 rows. With a matrix we can get the first and second card like this: first_card &lt;- hands[,1] second_card &lt;- hands[,2] Now the cases for which the first hand was a King can be computed like this: kings &lt;- paste(&quot;King&quot;, suits) sum(first_card %in% kings) #&gt; [1] 204 To get the conditional probability we compute what fraction of these have a King in the second card: sum(first_card %in% kings &amp; second_card %in% kings) / sum(first_card %in% kings) #&gt; [1] 0.0588 which is exactly 3/51 as we had already deduced. Notice that the code above is equivalent to: mean(first_card %in% kings &amp; second_card %in% kings) / mean(first_card %in% kings) #&gt; [1] 0.0588 which uses mean instead of sum and is an R version of: \\[ \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\] How about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a Natural 21 and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter. Below are the differences: permutations(3,2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 1 #&gt; [4,] 2 3 #&gt; [5,] 3 1 #&gt; [6,] 3 2 combinations(3,2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 3 In the second line the outcome does not include (2,1) because the (1,2) already was enumerated. The same applies to (3,1) and (3,2). So to compute the probability of a Natural 21 in Blackjack, we can do this: aces &lt;- paste(&quot;Ace&quot;, suits) facecard &lt;- c(&quot;King&quot;, &quot;Queen&quot;, &quot;Jack&quot;, &quot;Ten&quot;) facecard &lt;- expand.grid( number=facecard, suit=suits) facecard &lt;- paste( facecard$number, facecard$suit) hands &lt;- combinations(52, 2, v = deck) mean(hands[,1] %in% aces &amp; hands[,2] %in% facecard) #&gt; [1] 0.0483 In the last line we assume the Ace comes first. This is only because we know the way combination generates enumerates possibilities and it will list this case first. But to be safe we could have written this and produced the same answer: mean((hands[,1] %in% aces &amp; hands[,2] %in% facecard) | (hands[,2] %in% aces &amp; hands[,1] %in% facecard)) #&gt; [1] 0.0483 Monte Carlo example Instead of using combinations to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw to cards without replacements: hand &lt;- sample(deck, 2) hand #&gt; [1] &quot;Eight Spades&quot; &quot;Ten Spades&quot; And then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say face card. Now we need t0 check both possibilities: (hands[1] %in% aces &amp; hands[2] %in% facecard) | (hands[2] %in% aces &amp; hands[1] %in% facecard) #&gt; [1] FALSE If we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21. Let’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment. blackjack &lt;- function(){ hand &lt;- sample(deck, 2) (hand[1] %in% aces &amp; hand[2] %in% facecard) | (hand[2] %in% aces &amp; hand[1] %in% facecard) } Here we do have to check both possibilities: Ace first or Ace second because we are not using the combinations function. The function returns TRUE if we get a 21 and FALSE otherwise: blackjack() #&gt; [1] FALSE Now we can play this game, say, 10,000 times: B &lt;- 10000 results &lt;- replicate(B, blackjack()) mean(results) #&gt; [1] 0.0488 26.10 Birthday problem Suppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much. First, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this: n &lt;- 50 bdays &lt;- sample(1:365, n, replace = TRUE) To check if in this particular set of 50 people we have at least two with the same birthday, we can use the function duplicated which returns TRUE whenever an element of a vector is a duplicate. Here is an example: duplicated(c(1,2,3,1,4,3,5)) #&gt; [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE The second time 1 and 3 appear we get a TRUE. So to check if two birthdays were the same we simply use the any and duplicated functions like this: any(duplicated(bdays)) #&gt; [1] TRUE In this case, we see that it did happen. At least two people had the same birthday. To estimate the probability, we repeat this experiment by sampling 50 birthdays, over and over: same_birthday &lt;- function(n){ bdays &lt;- sample(1:365, n, replace=TRUE) any(duplicated(bdays)) } B &lt;- 10000 results &lt;- replicate(B, same_birthday(50)) mean(results) #&gt; [1] 0.97 Where you expecting the probability to be this high? People tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group is close to 365. At this stage, we run out of days and the probability is one. Say we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%? Let’s create a look-up table. We can quickly create a function to compute this for any group size: compute_prob &lt;- function(n, B=10000){ results &lt;- replicate(B, same_birthday(n)) mean(results) } Using the function sapply, we can perform element-wise operations on any function: n &lt;- seq(1,60) prob &lt;- sapply(n, compute_prob) We can now make a plot of the estimated probabilities of two people having the same birthday in a group of size \\(n\\): prob &lt;- sapply(n, compute_prob) qplot(n, prob) Now let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments. To make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this we use the multiplication rule. Let’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is: \\[ 1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365} \\] We can write a function that does this for any number: exact_prob &lt;- function(n){ prob_unique &lt;- seq(365,365-n+1)/365 1 - prod( prob_unique) } eprob &lt;- sapply(n, exact_prob) qplot(n, prob) + geom_line(aes(n, eprob), col = &quot;red&quot;) This plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities. 26.11 How many Monte Carlo experiments are enough In the examples above, we used $B=$10,000 Monte Carlo experiments. It turns out that this provided very accurate estimates. But in more complex calculations, 10,000 may not nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is so we won’t know if our Monte Carlo estimate is accurate. We know that the larger \\(B\\), the better the approximation. But how big do we need it to be? This is actually a challenging questions and answering it often requires advanced theoretical statistics training. One practical approach we will describe here is to check for the stability of the estimate. Here is an example with the birthday problem for a group of 22 people. B &lt;- 10^seq(1, 5, len = 100) compute_prob &lt;- function(B, n=25){ same_day &lt;- replicate(B, same_birthday(n)) mean(same_day) } prob &lt;- sapply(B, compute_prob) qplot(log10(B), prob, geom = &quot;line&quot;) In this plot, we can see that the values start to stabilize, that is, they vary less than .01, around 1000. Note that the exact probability, which we know in this case, is 0.569. 26.12 Addition rule Another way to compute the probability of a Natural 21 is to notice that it is the probability of an Ace followed by face card or a face card followed by an Ace. Here we use the addition rule: \\[ \\mbox{Pr}(A \\mbox{ or } B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\mbox{ and } B) \\] This rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice. #&gt; Loading required package: grid #&gt; Loading required package: futile.logger #&gt; #&gt; Attaching package: &#39;futile.logger&#39; #&gt; The following object is masked from &#39;package:gtools&#39;: #&gt; #&gt; scat #&gt; (polygon[GRID.polygon.12972], polygon[GRID.polygon.12973], polygon[GRID.polygon.12974], polygon[GRID.polygon.12975], text[GRID.text.12976], text[GRID.text.12977], text[GRID.text.12978], text[GRID.text.12979], text[GRID.text.12980]) In the case of a Natural 21, the intersection is empty since both hands can’t happen simultaneously. The probability of an Ace followed by a face card is \\(1/13 \\times 16/51\\) and the probability of a face card followed by an Ace is \\(16/52 \\times 4/51\\). These two are actually the same, which makes sense due to symmetry. In any case, we get the same result using the addition rule: 1/13*16/51 + 16/52*4/51 + 0 #&gt; [1] 0.0483 26.13 Monty Hall problem In the 1970s there was a game show called “Let’s Make a Deal”. Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. If the contestant did not pick the prize door on his or her first try, Monty Hall would open one of the two remaining doors and show the contestant there was no prize. Then he would ask “Do you want to switch doors?” What would you do? We can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counter intuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed explanation here or read one here. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes. Let’s start with the stick strategy: B &lt;- 10000 stick &lt;- replicate(B, { doors &lt;- as.character(1:3) prize &lt;- sample(c(&quot;car&quot;,&quot;goat&quot;,&quot;goat&quot;)) prize_door &lt;- doors[prize == &quot;car&quot;] my_pick &lt;- sample(doors, 1) show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1) stick &lt;- my_pick stick == prize_door }) mean(stick) #&gt; [1] 0.336 As we write the code, we note that the lines starting with my_pick and show have no influence on the last logical operation. From this we should realize that the chance is 1 in 3, what we began with. Now let’s repeat the exercise, but consider the switch strategy: switch &lt;- replicate(B, { doors &lt;- as.character(1:3) prize &lt;- sample(c(&quot;car&quot;,&quot;goat&quot;,&quot;goat&quot;)) prize_door &lt;- doors[prize == &quot;car&quot;] my_pick &lt;- sample(doors, 1) show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)], 1) stick &lt;- my_pick switch &lt;- doors[!doors%in%c(my_pick, show)] switch == prize_door }) mean(switch) #&gt; [1] 0.672 The Monte Carlo estimate confirms the 2/3 calculation. The helps us gain some insight by showing that we are removing a door, show, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3. Exercises One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan? What is the probability that the ball will not be cyan? Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan? Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan? Two events \\(A\\) and \\(B\\) are independent if \\(\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A) P(B)\\). Under which situation are the draws independent? A. You don’t replace the draw. B. You replace the draw. C. Neither D. Both Say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow? If you roll a 6-sided die six times, what is the probability of not seeing a 6? Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game? Create a Monte Carlo simulation to confirm your answer to the previous problem. Use B &lt;- 10000 simulations. Hint: Use the following code to generate the results of the first four games: celtic_wins &lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4)) The Celtics must win one of these 4 games! Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games therefore wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series? Confirm the results of the previous question with a Monte Carlo simulation. B &lt;- 100000 results &lt;- replicate(B, { cavs_wins &lt;- sample(c(0,1), 6, replace = TRUE) sum(cavs_wins)&gt;=4 }) mean(results) #&gt; [1] 0.344 Two teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p&gt;0.5\\) chance of winning each game. Given a value \\(p\\), the probability of winning the series for the underdog team \\(B\\) can be computed with the following function based on a Monte Carlo simulation: prob_win &lt;- function(p){ B &lt;- 10000 result &lt;- replicate(B, { b_win &lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p)) sum(b_win)&gt;=4 }) mean(result) } Use the function sapply to compute the probability, call it Pr, of winning for p &lt;- seq(0.5, 0.95, 0.025). Then plot the result `plot(p, prob). Repeat the exercise above, but now keep the probability fixed at p &lt;- 0.75 and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, N &lt;- seq(1, 25, 2). Hint: use this function now. prob_win &lt;- function(N, p=0.75){ B &lt;- 10000 result &lt;- replicate(B, { b_win &lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p)) sum(b_win)&gt;=(N+1)/2 }) mean(result) } "],
["continuous-probability.html", "Chapter 27 Continuous probability 27.1 Theoretical distribution 27.2 Approximations 27.3 The probability density 27.4 Monte Carlo simulations 27.5 Other continuous distributions Exercise", " Chapter 27 Continuous probability We earlier explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. For example, if we measure every single person in a very large population of size \\(n\\) with extremely high precision, since no two people are exactly the same height, we need to assign the proportion \\(1/n\\) to each observed value and attain no useful summary at all. Similarly, when defining probability distributions, it is not useful to assign a very small probability to every single height. Just as when using distributions to summarize numeric data, it is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the cumulative distribution functions (CDF). We previously described empirical cumulative distribution function (eCDF) as a basic summary of a list of numeric values. As an example, we earlier defined the height distribution for adult male students. Here we define the vector \\(x\\) to contain the male heights: library(tidyverse) library(dslabs) data(heights) x &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% .$height We defined the empirical distribution function as: F &lt;- function(a) mean(x&lt;=a) which, for any value a, gives the proportion of values in the list x that are smaller or equal than a. Keep in mind that we have not yet introduced probability. Let’s do this by asking the following: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 feet. Using the CDF we obtain an answer by typing: 1 - F(70) #&gt; [1] 0.377 Once a CDF is defined, we can use this to compute the probability of any subset. For instance, the probability of a student being between height a and height b is: F(b)-F(a) Because we can compute the probability for any possible event this way, the cumulative probability function defines the probability distribution for picking a height at random from our vector of heights x. 27.1 Theoretical distribution In the data visualization chapter, we introduced the normal distribution as a useful approximation to many naturally occurring distributions, including that of height. The cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function pnorm. We say that a random quantity is normally distributed with average avg and standard deviation s, if its probability distribution is defined by: F(a) = rnorm(a, avg, s) This is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire data set to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation: avg &lt;- mean(x) s &lt;- sd(x) 1 - pnorm(70.5, avg, s) #&gt; [1] 0.371 27.2 Approximations The normal distribution is derived mathematically: we do no need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution: While most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787 which is 177 centimeters. The probability assigned to this height is 0.001 or 1 in 812. The probability for 70 inches is much higher 0.106, but does it really make sense to think of the probability of being exactly 70 inches as being the same as 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch. With continuous distributions the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. So we could ask what is the probability that someone is between 69.5 and 70.5. In cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. So, for example, the normal distribution is useful for approximating the proportion of students reporting between 69.5 and 70.5: mean(x &lt;= 68.5) - mean(x &lt;= 67.5) #&gt; [1] 0.115 mean(x &lt;= 69.5) - mean(x &lt;= 68.5) #&gt; [1] 0.119 mean(x &lt;= 70.5) - mean(x &lt;= 69.5) #&gt; [1] 0.122 Note how close we get with the normal approximation: pnorm(68.5, avg, s) - pnorm(67.5, avg, s) #&gt; [1] 0.103 pnorm(69.5, avg, s) - pnorm(68.5, avg, s) #&gt; [1] 0.11 pnorm(70.5, avg, s) - pnorm(69.5, avg, s) #&gt; [1] 0.108 However, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks when we try to estimate: mean(x &lt;= 70.9) - mean(x&lt;=70.1) #&gt; [1] 0.0222 with pnorm(70.9, avg, s) - pnorm(70.1, avg, s) #&gt; [1] 0.0836 In general, we call this situation discretization. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool. 27.3 The probability density For categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it \\(X\\), can be 1,2,3,4,5 or 6. The probability of 4 is defined as: \\[ \\mbox{Pr}(X=4) = 1/6 \\] The CFD can then easily be defined: \\[ F(4) = \\mbox{Pr}(X\\leq 4) = \\mbox{Pr}(X = 4) + \\mbox{Pr}(X = 3) + \\mbox{Pr}(X = 2) + \\mbox{Pr}(X = 1) \\] Although for continuous distributions the probability of a single value \\(\\mbox{Pr}(X=x)\\) is not defined, there is a theoretical definition that has a similar interpretation. The probability density at \\(x\\) is defined as the function \\(f(a)\\) such that: \\[ F(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx \\] For those that know Calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know Calculus, you can think of \\(f(x)\\) as a curve for which the area under that curve up to the value \\(a\\), gives you the probability of \\(X\\leq a\\). For example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use: 1 - pnorm(76, avg, s) #&gt; [1] 0.0321 which mathematically is the grey area below: Figure 27.1: Integral of a function. The curve you see is the probability density for the normal distribution. In R, we get this using the function dnorm. Although it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available. 27.4 Monte Carlo simulations R provides functions to generate normally distributed outcomes. Specifically, the rnorm function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produced these random numbers. Here is an example of how we could generate data that looks like our reported heights: n &lt;- length(x) avg &lt;- mean(x) s &lt;- sd(x) simulated_heights &lt;- rnorm(n, avg, s) Not surprisingly, the distribution looks normal: This is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations. If, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer? The following Monte Carlo simulation helps us answer that question: B &lt;- 10000 tallest &lt;- replicate(B, { simulated_data &lt;- rnorm(800, avg, s) max(simulated_data) }) A seven footer is quite rare: mean(tallest &gt;= 7*12) #&gt; [1] 0.0193 Here is the resulting distribution: Note that it does not look normal. 27.5 Other continuous distributions The normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, chi-squared, exponential, gamma, beta, and beta-binomial. R provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. R uses a convention that lets us remember the names, namely using the letters d, q, p and r in front of a shorthand for the distribution. We have already seen the functions dnorm, pnorm and rnorm for the normal distribution. The functions qnorm gives us the quantiles. We can therefore draw a distribution like this: x &lt;- seq(-4, 4, length.out = 100) data.frame(x, f = dnorm(x)) %&gt;% ggplot(aes(x, f)) + geom_line() For the student-t the shorthand t is used so the functions are dt for the density, qt for the quantiles, pt for the cumulative distribution function, and rt for Monte Carlo simulation. Exercise Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter? Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller? Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random what is the probability that she is between 61 and 67 inches. Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now? Notice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average. To see the math that explains why the answers to questions 4, 5, and 6 are the same, suppose we have a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma\\). Suppose we ask the probability of \\(X\\) being smaller or equal to \\(a\\). Remember that, by definition, \\(a\\) is \\((a - \\mu)/\\sigma\\) standard deviations \\(\\sigma\\) away from the average \\(\\mu\\). The probability is: \\[ \\mbox{Pr}(X \\leq a) \\] Now we subtract \\(\\mu\\) to both sides and then divide both sides by \\(\\sigma\\): \\[ \\mbox{Pr}\\left(\\frac{X-\\mu}{\\sigma} \\leq \\frac{a-\\mu}{\\sigma} \\right) \\] The quantity on the right is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it \\(Z\\): \\[ \\mbox{Pr}\\left(Z \\leq \\frac{a-\\mu}{\\sigma} \\right) \\] So, no matter the units, the probability of \\(X\\leq a\\) is the same as the probability of a standard normal variable being less than \\((a - \\mu)/\\sigma\\). If mu is the average and sigma the standard error, which of the following R code would give us the right answer in every situation: A. mean(X&lt;=a) B. pnorm((a - mu)/sigma) C. pnorm((a - mu)/sigma, mu, sigma) D. pnorm(a) Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm. The distribution of IQ scores is approximately normally distributed. The expected value is 100 and the standard deviation is 15. Suppose you want to know the distribution of the smartest person in your school district if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating one million IQ scores and keeping the highest. Make a histogram. "],
["random-variables.html", "Chapter 28 Random variables 28.1 Sampling models 28.2 The probability distribution of a random variable 28.3 Distributions versus probability distributions 28.4 Notation for random variables 28.5 Central Limit Theorem 28.6 The expected value and standard error 28.7 Central Limit Theorem approximation 28.8 Statistical properties of averages 28.9 Law of large numbers 28.10 How large is large in CLT? 28.11 Population SD versus the sample SD Exercises", " Chapter 28 Random variables Random variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define X to be 1, if a bead is blue and red otherwise. beads &lt;- rep( c(&quot;red&quot;, &quot;blue&quot;), times = c(2,3)) X &lt;- ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) Here X is a random variable: every time we select a new bead the outcome changes randomly. See below: ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 0 ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 1 ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 1 Sometimes it’s 1 and sometimes it’s 0. In data science, we often deal with data that is affected by chance in some way: the data comes from a random sample, the data is affected by measurement error or the data measures some outcome that is random in nature. Being able to quantify the uncertainty introduced by randomness is one of the most important jobs of a data scientist. Statistical inference offers a framework, as well as several practical tools, for doing this. The first step is to learn how to mathematically describe random variables. We start with games of chance. 28.1 Sampling models Many data generation procedures, those that produce the data we study, can be modeled quite well as draws from a urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing that outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real world situations in which sampling models are used to answer specific questions. We will therefore start with such examples. Suppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels. We are going to define a random variable S that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn: color &lt;- rep(c(&quot;Black&quot;,&quot;Red&quot;,&quot;Green&quot;), c(18,18,2)) The 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -$1. Otherwise, the casino wins a dollar and we draw a $1. We code these draws like this: To construct our random variable S we can use this code: n &lt;- 10000 X &lt;- sample(ifelse( color==&quot;Red&quot;, -1, 1), n, replace = TRUE) X[1:10] #&gt; [1] 1 -1 -1 -1 -1 1 1 1 -1 1 Because we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining color: X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) We call this a sampling model since we are modelling the random behavior of roulette with the sampling of draws from an urn. The total winnings S is simply the sum of these 1,000 independent draws: X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) S &lt;- sum(X) S #&gt; [1] 664 28.2 The probability distribution of a random variable If you run the code above, you see that \\(S\\) changes every time. This is, of course, because S is a random variable. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that S is in the interval \\(S&lt;0\\). Note that if we can define a cumulative distribution function \\(F(a) = \\mbox{Pr}(S\\leq a)\\), then we will be able to answer any question related to the probability of events defined by our random variable \\(S\\), including the event \\(S&lt;0\\). We call this \\(F\\) the random variable’s distribution function. We can estimate the distribution function for the random variable \\(S\\) by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically B=10,000 times: n &lt;- 1000 B &lt;- 10000 roulette_winnings &lt;- function(n){ X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) sum(X) } S &lt;- replicate(B, roulette_winnings(n)) So now we can ask the following: in our simulations, how often did we get sums less than or equal toa? mean(S &lt;= a) This will be a very good approximation of \\(F(a)\\). In fact, we can visualize the distribution by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\): Now we can easily answer the casino’s question: how likely is it that we will lose money? mean(S&lt;0) #&gt; [1] 0.046 We can see it is quite low. In the histogram above, we see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to perfect. If in fact the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these: mean(S) #&gt; [1] 52.5 sd(S) #&gt; [1] 31.7 If we add a normal density with this average and standard deviation to the histogram above, we see that it matches very well: This average and this standard deviation have special names. They are referred to as the expected value and standard error of the random variable \\(S\\). We will say more about these in the next section. It turns out that statistical theory provides a way to derive the distribution of random variables defined as independent random draws from an urn. Specifically, in our example above, we can show that \\((S+n)/2\\) follows a binomial distribution. We therefore do not need to run for Monte Carlo simulations to know the probability distribution of \\(S\\). We did this for illustrative purposes. We can use the function dbinom and pbinom to compute the probabilities exactly. For example, to know compute \\(\\mbox{Pr}(S &lt; 0)\\) we note that: \\[\\mbox{Pr}(S &lt; 0) = \\mbox{Pr}((S+n)/2 &lt; (0+n)/2)\\] and we can use the pbinom to compute \\[\\mbox{Pr}(S \\leq 0)\\] pbinom(n/2, size = n, prob = 9/19) #&gt; [1] 0.955 Because this is a discrete probability function, to get \\(\\mbox{Pr}(S &lt; 0)\\) rather than \\(\\mbox{Pr}(S \\leq 0)\\) we write: pbinom(n/2-1, size = n, prob = 9/19) #&gt; [1] 0.949 For the details of the binomial distribution you can consult any basic probability book or even Wikipedia. Here we do not cover these details. Instead, we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT). 28.3 Distributions versus probability distributions Before we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization chapter, we described how any list of numbers \\(x_1,\\dots,x_n\\) has a distribution. The definition is quite straightforward. We define \\(F(a)\\) as the function that answers the question: what proportion of the list is less than or equal to \\(a\\)?. Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers x: avg &lt;- sum(x)/length(x) s &lt;- sqrt(sum((x - avg)^2) / length(x)) A random variable \\(X\\) has a distribution function. To define this we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the \\(F(a)\\) that answers the question: what is the probability that \\(X\\) is less than or equal to \\(a\\)? There is no list of numbers. However, if \\(X\\) is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of \\(X\\) and the average and standard deviation of that list are the expected value and standard error of the random variable. Another way to think about it, that does not involve an urn, is to run a Monte Carlo simulation and generate a very large list of outcomes of \\(X\\). These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of \\(X\\). The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable. 28.4 Notation for random variables In statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as \\(X \\leq x\\). Here \\(X\\) is a random variable, making it a random event, and \\(x\\) is an arbitrary value and not random. So, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see. So in this case, the probability of \\(X=x\\) is 1/6 regardless of the value of \\(x\\). This notation is a bit strange because, when we ask questions about probability, \\(X\\) is not an observed quantity. Instead it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of \\(X\\). So data scientists talk of what could have been after we see what actually happened. 28.5 Central Limit Theorem The Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history. Previously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error. 28.6 The expected value and standard error We have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work. The first important concept to learn is the expected value. In statistics books, it is common to use letter \\(\\mbox{E}\\) like this: \\[\\mbox{E}[X]\\] to denote the expected value of the random variable \\(X\\). A random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take. Theoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. For example, a useful formula tells us that the expected value of a random variable defined by one draw is the average of the numbers in the urn. So in the urn used to model betting on red in roulette, we have 20 one dollars, and 18 negative one dollars. The expected value is thus: \\[ \\mbox{E}[X] = (20 + -18)/38 \\] which is about 5 cents. It is a bit counterintuitive to say that \\(X\\) varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this: B &lt;- 10^6 X &lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19)) mean(X) #&gt; [1] 0.0515 In general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is: \\[ap + b(1-p).\\] To see this, notice that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s, \\(n(1-p)\\) \\(b\\)s and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\). Now the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the expected value of the sum of the draws is: \\[ \\mbox{number of draws } \\times \\mbox{ average of the numbers in the urn} \\] So if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The standard error (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use: \\[\\mbox{SE}[X]\\] to denote the standard error of a random variable. If our draws are independent, then the standard error of the sum is given by the equation: \\[ \\sqrt{\\mbox{number of draws }} \\times \\mbox{ standard deviation of the numbers in the urn} \\] Using the definition of standard deviation, we can derive, with a bit of math, that if a jar contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\) respectively, the standard deviation is: \\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\] So in our roulette example, the standard deviation of the values inside the urn is: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) or 2 * sqrt(90)/19 #&gt; [1] 0.999 The standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1. Using the formula above, the sum of 1,000 people playing has standard error of about $32: n &lt;- 1000 sqrt(n) * 2 * sqrt(90)/19 #&gt; [1] 31.6 As a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help. Advanced note: Before continuing we should point out that exact probability calculations for the casino winnings can be performed with the binomial distribution. However, here we focus on the CLT which can be generally applied to sums of random variables in a way that the binomial distribution can’t. 28.7 Central Limit Theorem approximation The Central Limit Theorem (CLT) tells us that the sum S is approximated by a normal distribution. Using the formulas above, we know that the expected value and standard error are: n * (20-18)/38 #&gt; [1] 52.6 sqrt(n) * 2 * sqrt(90)/19 #&gt; [1] 31.6 The theoretical values above match those obtained with the Monte Carlo simulation: mean(S) #&gt; [1] 52.5 sd(S) #&gt; [1] 31.7 Using the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation: mu &lt;- n * (20-18)/38 se &lt;- sqrt(n) * 2 * sqrt(90)/19 pnorm(0, mu, se) #&gt; [1] 0.0478 which is also in very good agreement with our Monte Carlo result: mean(S &lt; 0) #&gt; [1] 0.046 28.8 Statistical properties of averages There are two useful mathematical results that we used above and often employ when working with data. We list them below. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this: \\[ \\mbox{E}[X_1+X_2+\\dots+X_n] = \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n] \\] If the \\(X\\) are independent draws from the urn, then they all have the same expected value. Let’s call it \\(\\mu\\) and thus: \\[ \\mbox{E}[X_1+X_2+\\dots+X_n]= n\\mu \\] which is another way of writing the result we show above for the sum of draws. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols: \\[ \\mbox{E}[aX] = a\\times\\mbox{E}[X] \\] To see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it \\(\\mu\\) again: \\[ \\mbox{E}[(X_1+X_2+\\dots+X_n) / n]= \\mbox{E}[X_1+X_2+\\dots+X_n] / n = n\\mu/n = \\mu \\] The square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form: \\[ \\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2 } \\] The square of the standard error is referred to as the variance in statistical textbooks. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation: \\[ \\mbox{SE}[aX] = a \\times \\mbox{SE}[X] \\] To see why this is intuitive, again think of units. A consequence of 3 and 4 is that the standard error of the the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma\\): \\[ \\begin{aligned} \\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] &amp;= \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\ &amp;= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\ &amp;= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\ &amp;= \\sqrt{n\\sigma^2}/n\\\\ &amp;= \\sigma / \\sqrt{n} \\end{aligned} \\] If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\). Why we use \\(\\mu\\) and \\(\\sigma\\) Statistical textbooks use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error respectively. This is because \\(\\mu\\) is the Greek letter for \\(m\\), the first letter of mean, which is another term used for expected value. Similarly, \\(\\sigma\\) is the Greek letter for \\(s\\), the first letter of standard error. 28.9 Law of large numbers An important implication of the final result is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages. 28.9.1 Misinterpreting law of averages The law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses. Another funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row. 28.10 How large is large in CLT? The CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need larger sample sizes. By way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate. You can examine the properties of the Poisson distribution using dpois and ppois. You can generate random variables following this distribution with rpois. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia. 28.11 Population SD versus the sample SD The standard deviation of a list x (we use heights as an example) is defined as the square root of the average of the squared differences: library(dslabs) x &lt;- heights$height m &lt;- mean(x) s &lt;- sqrt(mean((x-m)^2)) Using mathematical notation we write: \\[ \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\ \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\] However, be aware that the sd function returns a slightly different result: identical(s,sd(x)) #&gt; [1] FALSE s-sd(x) #&gt; [1] -0.00194 This is because the sd function R does not return the sd of the list, but rather uses a formula that estimates standard deviations of a population from a random sample \\(X_1, \\dots, X_N\\) which, for reasons not discussed here, divide by the \\(N-1\\). \\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i \\\\ s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2} \\] You can see that this is the case by typing: N &lt;- length(x) s-sd(x)*sqrt((N-1)/N) #&gt; [1] 0 For all the theory discussed here, you need to compute the actual standard deviation as defined: sqrt(mean((x-m)^2)) So be careful when using the sd function in R. However, keep in mind that throughout the book we sometimes use the sd function when we really want the actual SD. This is because when the list size is big, these two are practically equivalent since \\(\\sqrt{(N-1)/N}\\) is close to 1. Exercises In American Roulette you can also bet on green. Remember there are 18 reds, 18 blacks and 2 greens (0 and 00). What are the chances the green comes out? The payout for winning on green is $17 dollars. This means that if you bet a dollar and it lands on green, you get $17. Create a sampling model using sample to simulate the random variable \\(X\\) for your winnings. Hint: see the example below for how it should look like when betting on red. X &lt;- sample(c(1,-1), 1, prob = c(9/19, 10/19)) Compute the expected value of \\(X\\). Compute the standard error of \\(X\\). Now create a random variable \\(S\\) that is the sum of your winnings after betting on green 1000 times. Hint: change the argument size and replace in your answer to question 2. Start your code by setting the seed to 1 with set.seed(1). What is the expected value of \\(S\\)? What is the standard error of \\(S\\)? What is the probability that you end up winning money? Hint: use the CLT. Create a Monte Carlo simulation that generates 10,000 outcomes of \\(S\\). Compute the average and standard deviation of the resulting list to confirm the results of 6 and 7. Start your code by setting the seed to 1 with set.seed(1). Now check your answer to 8 using the Monte Carlo result. The Monte Carlo result and the CLT approximation are close, but not that close. What could account for this? A. 10,000 simulations is not enough. If we do more, they match. B. The CLT does not work as well when the probability of success is small. In this case, it was 1/19. If we make the number of roulette plays bigger, they will match better. C. The difference is within rounding error. D. The CLT only works for the averages. Now create a random variable \\(Y\\) that is your average winnings per bet after playing off your winnings after betting on green 10,000 times. What is the expected value of \\(Y\\)? What is the standard error of \\(S\\)? What is the probability that you end up winnings per game that are positive? Hint: use the CLT. Create a Monte Carlo simulation that generates 2,500 outcomes of \\(S\\). Compute the average and standard deviation of the resulting list to confirm the results of 6 and 7. Start your code by setting the seed to 1 with set.seed(1) Now check your answer to 8 using the Monte Carlo result. The Monte Carlo result and the CLT approximation are now much closer. What could account for this? A. We are now computing averages instead of sums. B. 2,500 Monte Carlo simulations is not better than 10,000. C. The CLT does works better when the sample size is larger. We increased from 1,000 to 10,000. D. It is not closer. The difference is within rounding error. "],
["case-study-the-big-short.html", "Chapter 29 Case study: The Big Short 29.1 Interest rates explained with chance model 29.2 The Big Short Exercises", " Chapter 29 Case study: The Big Short 29.1 Interest rates explained with chance model More complex versions of the sampling models we have discussed are also used by banks to decide interest rates. Suppose you run a small bank that has a history of identifying potential homeowners that can be trusted to make payments. In fact, historically, in a given year, only 2% of your customers default, meaning that they don’t pay back the money that you lent them. However, you are aware that if you simply loan money to everybody without interest, you will end up losing money due to this 2%. Although you know 2% of your clients will probably default, you don’t know which ones. Yet by charging everybody just a bit extra in interest, you can make up the loses incurred due to that 2% and also cover your operating costs. You can also make a profit, but if you set the interest rates too high, your clients will go to another bank. We use all these facts and some probability theory to decide what interest rate you should charge. Suppose your bank will give out 1,000 loans for $180,000 this year. Also, after adding up all costs, suppose your bank loses $200,000 per foreclosure. For simplicity, we assume this includes all operational costs. A sampling model for this scenario can be coded like this: n &lt;- 1000 loss_per_foreclosure &lt;- -200000 p &lt;- 0.02 defaults &lt;- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) sum(defaults * loss_per_foreclosure) #&gt; [1] -3e+06 Note that the total loss defined by the final sum is a random variable. Every time you run the above code you get a different answer. We can easily construct a Monte Carlo simulation to get an idea of the distribution of this random variable. B &lt;- 10000 losses &lt;- replicate(B, { defaults &lt;- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) sum(defaults * loss_per_foreclosure) }) Here is the distribution of this random variable: We don’t really need a Monte Carlo simulation though. Using what we have learned, the CLT tells us that because our losses are a sum of independent draws, its distribution is approximately normal with expected value and standard errors given by: n*(p*loss_per_foreclosure + (1-p)*0) #&gt; [1] -4e+06 sqrt(n)*abs(loss_per_foreclosure)*sqrt(p*(1-p)) #&gt; [1] 885438 We can now set an interest rate to guarantee that, on average, we break even. Basically, we need to add a quantity \\(x\\) to each loan, which in this case are represent by draws, so that the expected value is 0. If we define \\(l\\) to be the loss per foreclosure, we need: \\[ lp + x(1-p) = 0 \\] which implies \\(x\\) is - loss_per_foreclosure*p/(1-p) #&gt; [1] 4082 or an interest rate of 0.023. However, we still have a problem. Although this interest rate guarantees that on average we break even, there is 50% chance that we lose money. If our bank loses money, we have to close it down. We therefore need to pick an interest rate that makes it unlikely for this to happen. At the same time, if the interest rate is too high, our clients will go to another bank so we must be willing to take some risks. So let’s say that we want our chances of losing money to be 1 in 100, what does the \\(x\\) quantity need to be now? This one is a bit harder. We want the sum \\(S\\) to have: \\[\\mbox{Pr}(S&lt;0) = 0.01\\] We know that \\(S\\) is approximately normal. The expected value of \\(S\\) is\\(\\{ lp + x(1-p)\\}n\\) with \\(n\\) the number of draws, which in this case represents loans. The standard error is \\(|x-l| \\sqrt{np(1-p)}\\). Because \\(x\\) is positive and \\(l\\) negative \\(|x-l|=x-l\\). Note that these are just the formulas from above, but using more compact symbols. Now we are going to use a mathematical “trick” that is very common in statistics. We are going add and subtract the same quantities to both sides of the event \\(S&lt;0\\) so that the probability does not change and we end up with a standard normal random variable on the left, which will then permit us to write down an equation with only \\(x\\) as an unknown. This “trick” is as follows: If \\[\\mbox{Pr}(S&lt;0) = 0.01\\] then \\[ \\mbox{Pr}\\left(\\frac{S - \\mbox{E}[S]}{\\mbox{SE}[S]} &lt; \\frac{ - \\mbox{E}[S]}{\\mbox{SE}[S]}\\right) \\] And remember \\(\\mbox{E}[S]\\) and \\(\\mbox{SE}[S]\\) are the expected value and standard error of \\(S\\) respectively. All we did above was add and divide by the same quantity on both sides. We did this because now the term on the left is a standard normal random variable, which we will rename \\(Z\\). Now we fill in the blanks with the actual formula for expected value and standard error: \\[ \\mbox{Pr}\\left(Z &lt; \\frac{- \\{ lp + x(1-p)\\}n}{(x-l) \\sqrt{np(1-p)}}\\right) = 0.01 \\] It may look complicated, but remember that \\(l\\), \\(p\\) and \\(n\\) are all known amounts so eventually we will turn them into numbers. Now because the term on the left side is a normal random with expected value 0 and standard error 1, it means that the quantity on the left must be equal to: qnorm(0.01) #&gt; [1] -2.33 for the equation to hold true. Remember that \\(z=\\)qnorm(0.01) gives us the value of \\(z\\) for which: \\[ \\mbox{Pr}(Z \\leq z) = 0.01 \\] So this means that right side of the complicated equation must be \\(z\\)=qnorm(0.01). \\[ \\frac{- \\{ lp + x(1-p)\\}N}{(x-l) \\sqrt{Np(1-p)}} = z \\] The trick works because we end up with an expression containing \\(x\\) that we know has to be equal to a known quantity \\(z\\). Solving for \\(x\\) is now simply algebra: \\[ x = - l \\frac{ np - z \\sqrt{np(1-p)}}{N(1-p) + z \\sqrt{np(1-p)}}\\] which is: l &lt;- loss_per_foreclosure z &lt;- qnorm(0.01) x &lt;- -l*( n*p - z*sqrt(n*p*(1-p)))/ ( n*(1-p) + z*sqrt(n*p*(1-p))) x #&gt; [1] 6249 Our interest rate now goes up to 0.035. This is still a very competitive interest rate. By choosing this interest rate, we now have an expected profit per loan of: loss_per_foreclosure*p + x*(1-p) #&gt; [1] 2124 which is a total expected profit of about: n*(loss_per_foreclosure*p + x*(1-p)) #&gt; [1] 2124198 dollars! We can run a Monte Carlo simulation to double check our theoretical approximations: B &lt;- 100000 profit &lt;- replicate(B, { draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-p, p), replace = TRUE) sum(draws) }) mean(profit) #&gt; [1] 2122436 mean(profit&lt;0) #&gt; [1] 0.0128 29.2 The Big Short One of your employees points out that since the bank is making 2124 dollars per loan that you should give out more loans! Why just n? You explain that finding those n clients was hard. You need a group that is predictable and that keeps the chances of defaults low. He then points out that even if the probability of default is higher, as long as our expected value is positive, you can minimize your chances of losses by increasing \\(n\\) and relying on the law of large numbers. He claims that even if the default rate is twice as high, say 4%, if we set the rate just a bit higher than: p &lt;- 0.04 r &lt;- (- loss_per_foreclosure*p/(1-p)) / 180000 r #&gt; [1] 0.0463 At 5%, we are guaranteed a positive expected value of: r &lt;- 0.05 x &lt;- r*180000 loss_per_foreclosure*p + x * (1-p) #&gt; [1] 640 and can minimize our chances of losing money by simply increasing \\(n\\) since: \\[ \\mbox{Pr}(S &lt; 0) = \\mbox{Pr}\\left(Z &lt; - \\frac{\\mbox{E}[S]}{\\mbox{SE}[S]}\\right) \\] with \\(Z\\) a standard normal random variable as above. If we define \\(\\mu\\) and \\(\\sigma\\) to be the expected value and standard deviation of the urn respectively (that is of a single loan), using the formulas above we have: \\(\\mbox{E}[S]= n\\mu\\) and \\(\\mbox{SE}[S]= \\sqrt{n}\\sigma\\). So if we define \\(z\\)=qnorm(0.01), we have: \\[ - \\frac{n\\mu}{\\sqrt{n}\\sigma} = - \\frac{\\sqrt{n}\\mu}{\\sigma} = z \\] which implies that if we let: \\[ n \\geq z^2 \\sigma^2 / \\mu^2 \\] we are guaranteed to have a probability of less than 0.01. The implication is that, as long as \\(\\mu\\) is positive, we can find an \\(n\\) that minimizes the probability of a loss. This is a form of the law of large numbers: when \\(n\\) is large, our average earnings per loan converges to the expected earning \\(\\mu\\). With \\(x\\) fixed, now we can ask what \\(n\\) do we need for the probability to be 0.01? In our example, if we give out: z &lt;- qnorm(0.01) n &lt;- ceiling( (z^2*(x-l)^2*p*(1-p))/(l*p + x*(1-p))^2 ) n #&gt; [1] 22163 loans, the probability of losing is about 0.01 and we are expected to earn a total of: n*(loss_per_foreclosure*p + x * (1-p)) #&gt; [1] 14184320 dollars! We can confirm this with a Monte Carlo simulation: p &lt;- 0.04 x &lt;- 0.05*180000 profit &lt;- replicate(B, { draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-p, p), replace = TRUE) sum(draws) }) mean(profit&lt;0) #&gt; [1] 0.0107 This seems like a no brainier. As a result, your colleague decides to leave your bank and start his own high risk mortgage company. A few months later, your colleague’s bank has gone bankrupt. A book is written and eventually a movie is made relating the mistake your friend, and many others, made. What happened? Your colleague’s scheme was mainly based on this mathematical formula: \\[ \\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] = \\sigma / \\sqrt{n} \\] By making \\(n\\) large, we minimize the standard error of our per-loan profit. However, for this rule to hold, the \\(X\\)s must be independent draws: one person defaulting must be independent of others defaulting. Note that in the case of averaging the same event over and over, an extreme example of events that are not independent, we get a standard error that is \\(\\sqrt{n}\\) times bigger: \\[ \\mbox{SE}[(X_1+X_1+\\dots+X_1) / n] = \\mbox{SE}[n X_1 / n] = \\sigma &gt; \\sigma / \\sqrt{n} \\] To construct a more realistic simulation than the original one your colleague ran, let’s assume there is a global event that affects everybody with high risk mortgages and changes their probability. We will assume that with 50-50 chance, all the probabilities go up or down slightly to somewhere between 0.03 and 0.05. But it happens to everybody at once, not just one person. These draws are no longer independent. p &lt;- 0.04 x &lt;- 0.05*180000 profit &lt;- replicate(B, { new_p &lt;- 0.04 + sample(seq(-0.01, 0.01, length = 100), 1) draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-new_p, new_p), replace = TRUE) sum(draws) }) Note that our expected profit is still large: mean(profit) #&gt; [1] 14188864 However, the probability of the bank having negative earning shoots up to: mean(profit&lt;0) #&gt; [1] 0.346 Even scarier is that the probability of losing more than 10 million dollars is: mean(profit &lt; -10000000) #&gt; [1] 0.238 To understand how this happens look at the distribution: data.frame(profit_in_millions=profit/10^6) %&gt;% ggplot(aes(profit_in_millions)) + geom_histogram(color=&quot;black&quot;, binwidth = 5) The theory completely breaks down and the random variable has much more variability than expected. The financial meltdown of 2007 was due, among other things, to financial “experts” assuming independence when there was none. Exercises Create a random variable \\(S\\) with the earnings of your bank if you give out 10,000 loans, the default rate is 0.3, and you lose $200,000 in each foreclosure. Hint: use the code we showed in the previous section, but change the parameters. Run a Monte Carlo simulation with 10,000 outcomes for \\(S\\). Make a histogram of the results. What is the expected value of \\(S\\)? What is the standard error of \\(S\\)? Suppose we give out loans for $180,000. What should the interest rate be so that our expected value is 0? (Harder) What should the interest rate be so that the chance of losing money is 1 in 20? In math notation, what should the interest rate be so that \\(\\mbox{Pr}(S&lt;0) = 0.05\\) ? If the bank wants to minimize the probabilities of losing money, which of the following does not make interest rates go up? A. A smaller pool of loans. B. A larger probability of default. C. A smaller required probability of losing money. D. The number of Monte Carlo simulations. "],
["introduction-4.html", "Chapter 30 Introduction", " Chapter 30 Introduction The day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. The week before the 2012, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. Political commentator Joe Scarborough said during his show: Anybody that thinks that this race is anything but a tossup right now is such an ideologue … they’re jokes.&quot; To which Nate Silver responded via Twitter: If you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal? How was Mr. Silver so confident? We will demonstrate how poll aggregators, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. The two main statistical tools used by the aggregators are the topic of this chapter: inference and modeling. To begin to understand how election forecasting works, we need to understand the basic data point they use: poll results. Opinion polling has been conducted since the 19th century. The general goal of these is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as inference and is the main topic of this chapter. "],
["polls.html", "Chapter 31 Polls 31.1 The sampling model for polls", " Chapter 31 Polls Perhaps the best know opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know which geographical locations to focus their get out the vote efforts. Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results. Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and, therefore, are often made public. We will eventually be looking at such data. Real Clear Politics is an example of a news aggregator that organizes and publishes poll results. For example, here are examples of polls reporting estimates of the popular vote for the 2016 presidential election: Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC. Let’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error. In this section we will show how the probability concepts we learned in the previous chapter can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modelling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election. We start by connecting probability theory to the task of using polls to learn about a population. 31.1 The sampling model for polls To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. We will use an urn instead of voters and, rather than competing with other pollsters for media attention, we will have a competition with a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar): Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it cost you $0.10 per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay me $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner. The dslabs package includes a function that shows a random draw from this urn: library(tidyverse) library(dslabs) ds_theme_set() take_poll(25) Think about how you would construct your interval based on the data shown above. We have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors; that is, that there are just two parties. "],
["populations-samples-parameters-and-estimates.html", "Chapter 32 Populations, samples, parameters and estimates 32.1 The sample average 32.2 Parameters 32.3 Polling versus forecasting 32.4 Properties of our estimate: expected value and standard error Exercises", " Chapter 32 Populations, samples, parameters and estimates We want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\). In statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample. Can we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) &gt; .9 or \\(p\\) &lt; .1. But are we ready to predict with certainty that there are more red beads than blue? We want to construct an estimate of \\(p\\) using only the information we observe. An estimate can be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times: we get a different answer each time since the sample proportion is a random variable. Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better. 32.1 The sample average Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate we can can easily report an estimate for the spread \\(2p-1\\), but for simplicity we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion \\(p\\). We start by defining the random variable \\(X=1\\) if we pick a blue bead at random and \\(0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads and dividing it by the total \\(N\\) turns this into a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because if we know the distribution of the sum \\(N \\bar{X}\\), we know the distribution of the average \\(\\bar{X}\\) because \\(N\\) is a non-random constant. For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads. Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\). 32.2 Parameters Just like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. \\(p\\) is the average of the urn since, if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is \\(p\\), we are going to estimate this parameter. The ideas presented here on how we estimate parameters and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample. 32.3 Polling versus forecasting Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment and not for election day. The \\(p\\) for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night result taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in our a later section. 32.4 Properties of our estimate: expected value and standard error To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered in the probability chapter apply. Using what we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, \\(p\\). So dividing by the non-random constant \\(N\\) gives us that the expected value of the average \\(\\bar{X}\\) is \\(p\\). We can write it using our mathematical notation: \\[ \\mbox{E}(\\bar{X}) = p \\] We can also use what we learned to figure out the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error for the average: \\[ \\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N} \\] This result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll our estimate converges to \\(p\\). If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small? One problem is that we do not know \\(p\\), so we can’t compute the standard error. For illustrative purposes let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\): From the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is: sqrt(p*(1-p))/sqrt(1000) #&gt; [1] 0.0158 or 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\). Exercises Suppose you poll a population in which a proportion \\(p\\) of voters are Democrats and \\(1-p\\) are Republicans. Your sample size is \\(N=25\\). Consider the random variable \\(S\\) which is the total number of Democrats in your sample. What is the expected value of this random variable? Hint: it’s a function of \\(p\\). What is the standard error of \\(S\\) ? Hint: it’s a function of \\(p\\). Consider the random variable \\(S/N\\). This is equivalent to the sample average, which we have been denoting as \\(\\bar{X}\\). What is the expected value of the \\(\\bar{X}\\)? Hint: it’s a function of \\(p\\). What is the standard error of \\(\\bar{X}\\)? Hint: it’s a function of \\(p\\). Write a line of code that gives you the standard error se for the problem above for p &lt;- seq(0, 1, length = 100). Make a plot of se versus p. Copy the code above and put it inside a for-loop to make the plot for \\(N=25\\), \\(N=100\\) and \\(N=1000\\). If we are interested in the difference in proportions, \\(p - (1-p)\\), our estimate is \\(d = \\bar{X} - (1-\\bar{X})\\). Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of \\(d\\). What is the standard error of \\(d\\)? If the actual \\(p=.45\\), it means the Republicans are winning by a relatively large margin since \\(d= -.1\\), which is a 10% margin of victory. What is the standard error of \\(2\\hat{X}-1\\) in this case? Given the answer to 9, which of the following best describes your strategy of using a sample size of \\(N=25\\)? A. The expected value of our estimate \\(2\\bar{X}-1\\) is \\(d\\) so our prediction will be right on. B. Our standard error is larger than the difference, so the chances of \\(2\\bar{X}-1\\) being positive and throwing us off were not that small. We should pick a larger sample size. C. The difference is 10% and the standard error is about 0.2, therefore much smaller than the difference. D. Because we don’t know \\(p\\), we have no way knowing that making \\(N\\) larger would actually improve our standard error. "],
["central-limit-theorem-in-practice.html", "Chapter 33 Central Limit Theorem in practice 33.1 A Monte Carlo simulation 33.2 The spread 33.3 Bias: why not run a very large poll? Exercises", " Chapter 33 Central Limit Theorem in practice The CLT tells us that the distribution function for a sum of draws is approximately normal. We also learned that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of \\(\\bar{X}\\) is approximately normal. In summary, we have that \\(\\bar{X}\\) has an approximately normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\). Now how does this help us? Suppose we want to know what is the probability that we are within 1% from \\(p\\). We are basically asking if: \\[ \\mbox{Pr}(| \\bar{X} - p| \\leq .01) \\] which is the same as: \\[ \\mbox{Pr}(\\bar{X}\\leq p + .01) - \\mbox{Pr}(\\bar{X} \\leq p - .01) \\] Can we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it \\(Z\\), on the left. Since \\(p\\) is the expected value and \\(\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\\) is the standard error we get: \\[ \\mbox{Pr}\\left(Z \\leq \\,.01 / \\mbox{SE}(\\bar{X}) \\right) - \\mbox{Pr}\\left(Z \\leq - \\,.01 / \\mbox{SE}(\\bar{X}) \\right) \\] One problem we have is that since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\). But it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\). We say that we plug-in the estimate. Our estimate of the standard error is therefore: \\[ \\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N} \\] In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and \\(N\\). Now we continue with our calculation, but dividing by \\(\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\) instead. In our first sample we had 12 blue and 13 red so \\(\\bar{X} = 0.48\\) and so our estimate of standard error is X_hat &lt;- 0.48 se &lt;- sqrt(X_hat*(1-X_hat)/25) se #&gt; [1] 0.0999 And now we can answer the question of the probability of being close to \\(p\\). The answer is: pnorm(0.01/se) - pnorm(-0.01/se) #&gt; [1] 0.0797 So there is a small chance that we will be close. A poll of only \\(N=25\\) people is not really very useful, at least not for a close election. Earlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is: 2*se #&gt; [1] 0.2 Why do we multiply by 2? Because if you ask what is the probability that we are within two standard errors from \\(p\\), we get: \\[ \\mbox{Pr}\\left(Z \\leq \\, 2\\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right) - \\mbox{Pr}\\left(Z \\leq - 2 \\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right) \\] which is: \\[ \\mbox{Pr}\\left(Z \\leq 2 \\right) - \\mbox{Pr}\\left(Z \\leq - 2\\right) \\] which we know is about 95%: pnorm(2)-pnorm(-2) #&gt; [1] 0.954 So there is a 95% probability that \\(\\bar{X}\\) will be within \\(2\\times \\hat{SE}(\\bar{X})\\), in our case 0, to \\(p\\). Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. In summary, the CLT tells us that our poll based on a sample size of \\(25\\) is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes. From the table above. we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a \\(\\bar{X}\\)=0.48 with a sample size of 2,000, our standard error \\(\\hat{\\mbox{SE}}(\\bar{X})\\) would have been 0.011. So our result is an estimate of 48% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. But keep in mind, this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition. 33.1 A Monte Carlo simulation Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this: B &lt;- 10000 N &lt;- 1000 Xhat &lt;- replicate(B, { X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) mean(X) }) The problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results. One thing we therefore do to corroborate theoretical results is to pick one or several values of p, and run the simulations. Let’s set p=0.45. We can then simulate a poll: p &lt;- 0.45 N &lt;- 1000 X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) Xhat &lt;- mean(X) In this particular sample our estimate is Xhat. We can use that code to do a Monte Carlo simulation: B &lt;- 10000 Xhat &lt;- replicate(B, { X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) mean(X) }) To review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.016. The simulation confirms this: mean(Xhat) #&gt; [1] 0.45 sd(Xhat) #&gt; [1] 0.0158 A histogram and qq-plot confirm that the normal approximation is accurate as well: Of course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N. 33.2 The spread The competition is to predict the spread, not the proportion \\(p\\). However, because we are assuming there are only two parties, we know that the spread if \\(p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(2p - 1\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\hat{\\mbox{SE}}(\\bar{X})\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\hat{\\mbox{SE}}(\\bar{X})\\). Note that subtracting 1 does not add any variability so it does not affect the standard error. For our 25 sample above, our estimate \\(p\\) is .48 with margin of error .20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for \\(p\\), we have it for the spread \\(2p-1\\). 33.3 Bias: why not run a very large poll? For realistic values of \\(p\\), say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%. Here are the calculations: N &lt;- 100000 p &lt;- seq(0.35, 0.65, length = 100) SE &lt;- sapply(p, function(x) 2*sqrt(x*(1-x)/N)) data.frame(p=p, SE = SE) %&gt;% ggplot(aes(p, SE)) + geom_line() One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\). We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in a later chapter. Exercises Write an urn model function that takes the proportion of Democrats \\(p\\) and the sample size \\(N\\) as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function take_sample. Now assume p &lt;- 0.45 and that your sample size is \\(N=100\\). Take a sample of \\(N = 100\\) observations, repeat that 10,000 times and save the vector of mean(X)- p into an object called errors. Hint: use the function you wrote for exercise 1 to write this in one line of code. Vector errors contains, for each simulated sample, the difference between the actual \\(p\\) and our estimate \\(\\bar{X}\\). We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions: mean(errors) hist(errors) The error \\(\\bar{X}-p\\) is a random variable. In practice, the error is not observed because we do not know \\(p\\). Here we observe it because we constructed the simulation. What we can do in practice is describe the size of the error. What is the average size of the error if we define the size by taking the absolute value \\(\\mid \\bar{X} - p \\mid\\) ? The standard error is related to the value we just computed. It is related to the typical size of the error we make when predicting. We say size because we just saw that the errors are centered around 0, so in that sense the typical error is 0. For mathematical reasons related to the central limit theorem, we actually use the standard deviation of errors rather than the average of the absolute values. As we have discussed, the standard error is the squared root of the average squared distance \\((\\bar{X} - p)^2\\). What is this standard deviation, defined as the squared root of the distance squared? The theory we just learned tells us what this standard deviation is going to be because it is the standard error of \\(\\bar{X}\\). Generate a sample and create an estimate of the standard error of \\(\\bar{X}\\). In practice, we don’t know \\(p\\), so we construct an estimate of the theoretical prediction based by plugging in \\(\\bar{X}\\) for \\(p\\). Compute this estimate. Set the seed at 1 with set.seed(1). Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict \\(p\\) with \\(\\hat{X}\\). Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for \\(p=0.5\\). Create a plot of the largest standard error for \\(N\\) ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%? A. 100 B. 500 C. 2,500 D. 4,000 For \\(N=100\\), the central limit theorem tells us that the distribution of \\(\\hat{X}\\) is: A. practically equal to \\(p\\). B. approximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\). C. approximately normal with expected value \\(\\bar{X}\\) and standard error \\(\\sqrt{\\bar{X}(1-\\bar{X})/N}\\). D. not a random variable. Based on the answer from exercise 8, errors \\(\\bar{X} - p\\) are: A. practically equal to 0. B. approximately normal with expected value \\(0\\) and standard error \\(\\sqrt{p(1-p)/N}\\). C. approximately normal with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\). D. not a random variable. To corroborate your answer to exercise 9, make a qq-plot of the errors you generated in exercise 2 to see if they follow a normal distribution. If \\(p=0.45\\) and \\(N=100\\) as in exercise 2, use the CLT to estimate the probability that \\(\\bar{X}&gt;0.5\\). You can assume you know \\(p=0.45\\) for this calculation. Assume you are in a practical situation and you don’t know \\(p\\). Take a sample of size \\(N=100\\) and obtain a sample average of \\(\\bar{X} = 0.51\\). What is the CLT approximation for the probability that your error is equal or larger than 0.01? "],
["confidence-intervals.html", "Chapter 34 Confidence intervals 34.1 A Monte Carlo simulation 34.2 The correct language 34.3 Power 34.4 p-values Exercises", " Chapter 34 Confidence intervals Confidence intervals are a very useful concept widely employed by data scientists. A version of these that are commonly seen come from the ggplot geometry geom_smooth. Here is an example using a temperature dataset available in R: data(&quot;nhtemp&quot;) data.frame(year = as.numeric(time(nhtemp)), temperature=as.numeric(nhtemp)) %&gt;% ggplot(aes(year, temperature)) + geom_point() + geom_smooth() + ggtitle(&quot;Average Yearly Temperatures in New Haven&quot;) We will later learn how the curve is formed, but for now consider the shaded area around the curve. This is created using the concept of confidence intervals. In our earlier competition, you were asked to give an interval. If the interval you submitted includes the \\(p\\), you get half the money you spent on your “poll” back and pass to the next stage of the competition. One way to pass to the second round is to report a very large interval. For example, the interval \\([0,1]\\) is guaranteed to include \\(p\\). However, with an interval this big, we have no chance of winning the competition. Similarly, if you are an election forecaster and predict the spread will be between -100% and 100%, you will be ridiculed for stating the obvious. Even a smaller interval, such as saying the spread will be between -10 and 10%, will not be considered serious. On the other hand, the smaller the interval we report, the smaller our chances are of winning the prize. Likewise, a bold pollster that reports very small intervals and misses the mark most of the time will not be considered a good pollster. We want to be somewhere in between. We can use the statistical theory we have learned to compute the probability of any given interval including \\(p\\). If we are asked to create an interval with, say, a 95% chance of including \\(p\\), we can do that as well. These are called 95% confidence intervals. When a pollster reports an estimate and a margin of error, they are, in a way, reporting a 95% confidence interval. Let’s show how this works mathematically. We want to know the probability that the interval \\([\\bar{X} - 2\\hat{\\mbox{SE}}(\\bar{X}), \\bar{X} - 2\\hat{\\mbox{SE}}(\\bar{X})]\\) contains the true proportion \\(p\\). First, consider that the start and end of these intervals are random variables: every time we take a sample, they change. To illustrate this, run the Monte Carlo simulation above twice. We use the same parameters as above: p &lt;- 0.45 N &lt;- 1000 And notice that the interval here: X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) X_hat &lt;- mean(X) SE_hat &lt;- sqrt(X_hat*(1-X_hat)/N) c(X_hat - 2*SE_hat, X_hat + 2*SE_hat) #&gt; [1] 0.418 0.480 is different from this one: X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) X_hat &lt;- mean(X) SE_hat &lt;- sqrt(X_hat*(1-X_hat)/N) c(X_hat - 2*SE_hat, X_hat + 2*SE_hat) #&gt; [1] 0.421 0.483 Keep sampling and creating intervals and you will see the random variation. To determine the probability that the interval includes \\(p\\), we need to compute this: \\[ \\mbox{Pr}\\left(\\bar{X} - 2\\hat{\\mbox{SE}}(\\bar{X}) \\leq p \\leq \\bar{X} + 2\\hat{\\mbox{SE}}(\\bar{X})\\right) \\] By subtracting and dividing the same quantities in all parts of the equation, we get that the above is equivalent to: \\[ \\mbox{Pr}\\left(-2 \\leq \\frac{\\bar{X}- p}{\\hat{\\mbox{SE}}(\\bar{X})} \\leq 2\\right) \\] The term in the middle is an approximately normal random variable with expected value 0 and standard error 1, which we have been denoting with \\(Z\\), so we have: \\[ \\mbox{Pr}\\left(-2 \\leq Z \\leq 2\\right) \\] which we can quickly compute using : pnorm(2) - pnorm(-2) #&gt; [1] 0.954 proving that we have a 95% probability. If we want to have a larger probability, say 99%, we need to multiply by whatever z satisfies the following: \\[ \\mbox{Pr}\\left(-z \\leq Z \\leq z\\right) = 0.99 \\] Using: z &lt;- qnorm(0.995) z #&gt; [1] 2.58 will achieve this because by definition pnorm(qnorm(0.995) is 0.995 and by symmetry pnorm(1-qnorm(0.995)) is 1 - 0.995. As a consequence, we have that pnorm(z)-pnorm(-z) #&gt; [1] 0.99 is 0.995 - 0.005 = 0.99. We can use this approach for any percentile \\(q\\): we use \\(1 - (1 - q)/2\\). Why this number? Because \\(1 - (1 - q)/2 + (1 - q)/2 = q\\). To get exactly 0.95 confidence interval, we actually use a slightly smaller number than 2: qnorm(0.975) #&gt; [1] 1.96 34.1 A Monte Carlo simulation We can run a Monte Carlo simulation to confirm that, in fact, a 95% confidence interval includes \\(p\\) 95% of the time. B &lt;- 10000 inside &lt;- replicate(B, { X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) X_hat &lt;- mean(X) SE_hat &lt;- sqrt(X_hat*(1-X_hat)/N) between(p, X_hat - 2*SE_hat, X_hat + 2*SE_hat) }) mean(inside) #&gt; [1] 0.955 The following plot shows the first 100 confidence intervals. In this case, we created the simulation so the black line denotes the parameter we are trying to estimate: 34.2 The correct language When using the theory we described above, it is important to remember that it is the intervals that are random, not \\(p\\). In the plot above, we can see the random intervals moving around and \\(p\\), represented with the vertical line, staying in the same place. The proportion of blue in the urn \\(p\\) is not. So the 95% relates to the probability that this random interval falls on top of \\(p\\). Saying the \\(p\\) has a 95% of being between this and that is technically an incorrect statement because \\(p\\) is not random. 34.3 Power Pollsters are not successful at providing correct confidence intervals, but rather at predicting who will win. When we took a 25 bead sample size, the confidence interval for the spread: N &lt;- 25 X_hat &lt;- 0.48 (2*X_hat - 1) + c(-2,2)*2*sqrt(X_hat*(1-X_hat)/sqrt(N)) #&gt; [1] -0.934 0.854 includes 0. If this were a poll and we were forced to make a declaration, we would have to say it was a “toss-up”. A problem with our poll results is that given the sample size and the value of \\(p\\), we would have to sacrifice on the probability of an incorrect call to create an interval that does not include 0. This does not mean that the election is close. It only means that we have a small sample size. In statistical textbooks this is called lack of power. In the context of polls, power is the probability of detecting spreads different from 0. By increasing our sample size, we lower our standard error and therefore have a much better chance of detecting the direction of the spread. 34.4 p-values p-values are ubiquitous in the scientific literature. They are related to confidence interval so we introduce the concept here. Let’s consider the blue and red beads. Suppose that rather than wanting an estimate of the spread or the proportion of blue, I am interested only in the question: are there more blue beads or red beads? I want to know if the spread \\(2p-1 &gt; 0\\). Let’s say we take a random sample of \\(N=100\\) and we observe \\(52\\) blue beads, which gives us \\(2\\bar{X}-1=0.04\\). This seems to be pointing to the existence of more blue than red beads since 0.04 is larger than 0. However, as data scientists we need to be skeptical. We know there is chance involved in this process and we could get a 52 even when the actual spread is 0. We call this a null hypothesis. The null hypothesis is the skeptic’s hypothesis: the spread is \\(2p-1=0\\). We have observed a random variable \\(2*\\bar{X}-1 = 0.52\\) and the p-value is the answer to the question: how likely is it to see a value this large, when the null hypothesis is true? So we write: \\[\\mbox{Pr}(\\mid \\bar{X} - 0.5 \\mid &gt; 0.02 ) \\] assuming the \\(2p-1=0\\) or \\(p=0.5\\). Under the null hypothesis we know that: \\[ \\sqrt{N}\\frac{\\bar{X} - 0.5}{\\sqrt{0.5(1-0.5)}} \\] is standard normal. We therefore can compute the probability above, which is the p-value. \\[\\mbox{Pr}\\left(\\sqrt{N}\\frac{\\mid \\bar{X} - 0.5\\mid}{\\sqrt{0.5(1-0.5)}} &gt; \\sqrt{N} \\frac{0.02}{ \\sqrt{0.5(1-0.5)}}\\right)\\] N=100 z &lt;- sqrt(N)*0.02/0.5 1 - (pnorm(z) - pnorm(-z)) #&gt; [1] 0.689 In this case, there is actually a large chance of seeing 52 or larger under the null hypothesis. Keep in mind that there is a close connection between p-values and confidence intervals. If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05. To learn more about p-values you can consult any statistics textbook. However, in general, we prefer reporting confidence intervals over p-values since it gives us an idea of the size of the estimate. The p-value simply reports a probability and says nothing about the significance of the finding in the context of the problem. Exercises For these exercises, we will use actual polls from the 2016 election. You can load the data from the dslabs package. library(dslabs) data(&quot;polls_us_election_2016&quot;) Specifically, we will use all the national polls that ended within one week before the election. library(tidyverse) polls &lt;- polls_us_election_2016 %&gt;% filter(enddate &gt;= &quot;2016-10-31&quot; &amp; state == &quot;U.S.&quot;) For the first poll you can obtain the samples size and estimated Clinton percentage with: N &lt;- polls$samplesize[1] X_hat &lt;- polls$rawpoll_clinton[1]/100 Assume there are only two candidates and construct a 95% confidence interval for the election night proportion \\(p\\). Now use dplyr to add a confidence interval as two columns, call them lower and upper, to the object poll. Then use select to show the pollster, enddate, X_hat,lower, upper variables. Hint: define temporary columns X_hat and se_hat. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion \\(p=0.482\\) or not. For the table you just created, what proportion of confidence intervals included \\(p\\)? If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include \\(p\\)? A much smaller proportion of the polls than expected produce confidence intervals containing \\(p\\). If you look closely at the table, you will see that most polls that fail to include \\(p\\) are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates \\(d\\), which in this election was \\(0.482 - 0.461 = 0.021\\). Assume that there are only two parties and that \\(d = 2p - 1\\), define: polls &lt;- polls_us_election_2016 %&gt;% filter(enddate &gt;= &quot;2016-10-31&quot; &amp; state == &quot;U.S.&quot;) %&gt;% mutate(d_hat = rawpoll_clinton/100 - rawpoll_trump/100) and re-do exercise 1, but for the difference. Now repeat exercise 3, but for the difference. Now repeat exercise 4, but for the difference. Although the proportion of confidence intervals goes up substantially, it is still lower that 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual \\(d=0.021\\). Stratify by pollster. Re-do the plot that you made for exercise 9, but only for pollsters that took five or more polls. "],
["statistical-models.html", "Chapter 35 Statistical models 35.1 Poll aggregators 35.2 Poll data 35.3 Pollster bias 35.4 Data driven model Exercises", " Chapter 35 Statistical models “All models are wrong, but some are useful” -George E. P. Box 35.1 Poll aggregators As we described earlier, in the 2012 Nate Silver was giving a Obama a 90% chance of winning. How was Mr. Silver so confident? We will demonstrate Mr. Silver saw what others did not by using a Monte Carlo simulation. We generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls: d &lt;- 0.039 Ns &lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516) p &lt;- (d + 1)/2 confidence_intervals &lt;- sapply(Ns, function(N) { X &lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) X_hat &lt;- mean(X) SE_hat &lt;- sqrt(X_hat*(1-X_hat)/N) 2*c(X_hat, X_hat - 2*SE_hat, X_hat + 2*SE_hat)-1 }) Let’s save the results from this simulation in a data frame: polls &lt;- data.frame(poll=1:ncol(confidence_intervals), t(confidence_intervals), sample_size=Ns) %&gt;% setNames(c(&quot;poll&quot;, &quot;estimate&quot;, &quot;low&quot;, &quot;high&quot;, &quot;sample_size&quot;)) polls #&gt; poll estimate low high sample_size #&gt; 1 1 0.0277 -0.02776 0.0832 1298 #&gt; 2 2 0.0356 -0.05093 0.1222 533 #&gt; 3 3 0.0283 -0.02626 0.0829 1342 #&gt; 4 4 0.0279 -0.03888 0.0946 897 #&gt; 5 5 0.0129 -0.05896 0.0848 774 #&gt; 6 6 0.0000 -0.12549 0.1255 254 #&gt; 7 7 0.0197 -0.05047 0.0899 812 #&gt; 8 8 0.0494 -0.06159 0.1604 324 #&gt; 9 9 0.0194 -0.03629 0.0750 1291 #&gt; 10 10 0.0455 -0.01603 0.1069 1056 #&gt; 11 11 0.0359 -0.00697 0.0788 2172 #&gt; 12 12 0.0775 -0.01026 0.1653 516 Here is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney: Not surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing. Poll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction. Although as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with: sum(polls$sample_size) #&gt; [1] 11269 participants. Basically, we construct an estimate of the spread, let’s call it \\(d\\), with a weighted average in the following way: d_hat &lt;- polls %&gt;% summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %&gt;% .$avg Once we have an estimate of \\(d\\), we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.018. Thus we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote. Of course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits. Since the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. For example, the Princeton Election Consortium gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton. By understanding statistical models and how these forecasters use them, we will start to understand how this happened. Although not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her a 81.4% chance. We introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. 35.2 Poll data We use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of of the dslabs package: data(polls_us_election_2016) names(polls_us_election_2016) #&gt; [1] &quot;state&quot; &quot;startdate&quot; &quot;enddate&quot; #&gt; [4] &quot;pollster&quot; &quot;grade&quot; &quot;samplesize&quot; #&gt; [7] &quot;population&quot; &quot;rawpoll_clinton&quot; &quot;rawpoll_trump&quot; #&gt; [10] &quot;rawpoll_johnson&quot; &quot;rawpoll_mcmullin&quot; &quot;adjpoll_clinton&quot; #&gt; [13] &quot;adjpoll_trump&quot; &quot;adjpoll_johnson&quot; &quot;adjpoll_mcmullin&quot; The table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls that happened during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those: polls &lt;- polls_us_election_2016 %&gt;% filter(state == &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot; &amp; (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) We add a spread estimate: polls &lt;- polls %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) For illustrative purposes, we will assume that there are only two parties and call \\(p\\) the proportion voting for Clinton and \\(1-p\\) the proportion voting for Trump. We are interested in the spread \\(2p-1\\). Let’s call the spread \\(d\\) (for difference). Note that we have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(d\\) and the standard error is \\(\\sqrt{p (1 - p) / N}\\). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is: d_hat &lt;- polls %&gt;% summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %&gt;% .$d_hat and the standard error is: p_hat &lt;- (d_hat+1)/2 moe &lt;- 1.96 * 2 * sqrt(p_hat*(1-p_hat)/sum(polls$samplesize)) moe #&gt; [1] 0.00662 So we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened? A histogram of the reported spreads shows a problem: polls %&gt;% ggplot(aes(spread)) + geom_histogram(color=&quot;black&quot;, binwidth = .01) The data does not appear to be normally distributed and the standard error appears to be larger than 0.007. The theory is not quite working here. 35.3 Pollster bias Notice that various pollsters are involved and some are taking several polls a week: polls %&gt;% group_by(pollster) %&gt;% summarize(n()) #&gt; # A tibble: 15 x 2 #&gt; pollster `n()` #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 ABC News/Washington Post 7 #&gt; 2 Angus Reid Global 1 #&gt; 3 CBS News/New York Times 2 #&gt; 4 Fox News/Anderson Robbins Research/Shaw &amp; Company Research 2 #&gt; 5 IBD/TIPP 8 #&gt; 6 Insights West 1 #&gt; # ... with 9 more rows Let’s visualize the data for the pollsters that are regularly polling: polls %&gt;% group_by(pollster) %&gt;% filter(n() &gt;= 6) %&gt;% ggplot(aes(pollster, spread)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) This plot reveals a unexpected result. First, consider that the standard error predicted by theory for each poll: polls %&gt;% group_by(pollster) %&gt;% filter(n() &gt;= 6) %&gt;% summarize(se = 2 * sqrt( p_hat * (1-p_hat) / median(samplesize))) #&gt; # A tibble: 5 x 2 #&gt; pollster se #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 ABC News/Washington Post 0.0265 #&gt; 2 IBD/TIPP 0.0333 #&gt; 3 Ipsos 0.0225 #&gt; 4 The Times-Picayune/Lucid 0.0196 #&gt; 5 USC Dornsife/LA Times 0.0183 is between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences across the polls. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We can also call them pollster bias. In the following section, rather than use the urn model theory, we are instead going to develop a data-driven model. 35.4 Data driven model For each pollster, let’s collect their last reported result before the election: one_poll_per_pollster &lt;- polls %&gt;% group_by(pollster) %&gt;% filter(enddate == max(enddate)) %&gt;% ungroup() Here is a histogram of the data for these 15 pollsters: one_poll_per_pollster %&gt;% ggplot(aes(spread)) + geom_histogram(binwidth = 0.01) In the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead we will model this spread data directly. The new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We assume that the expected value of our urn is the actual spread \\(d=2p-1\\). Because instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\). Rather than voter sampling variability, the standard error now includes the pollster to pollster variability. Our new urn, also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol \\(\\sigma\\) is used to represent this parameter. In summary, we have two unknown parameters: the expected value \\(d\\) and the standard deviation \\(\\sigma\\). Our task is to estimate \\(d\\). Because we model the observed values \\(X_1,\\dots X_N\\) as a random sample from the urn, the CLT still works in this situation because it is an average of independent random variables. For a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{X}\\) is approximately normal with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider \\(N=15\\) large enough, we can use this to construct confidence intervals. A problem is that we don’t know \\(\\sigma\\). But theory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as: \\[ s = \\sqrt{ \\frac{1}{N-1}\\sum_{i=1}^N (X_i - \\bar{X})^2} \\] Unlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here. The sd function in R computes the sample standard deviation: sd(one_poll_per_pollster$spread) #&gt; [1] 0.0242 We are now ready to form a new confidence interval based on our new data driven model: results &lt;- one_poll_per_pollster %&gt;% summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %&gt;% mutate(start = avg - 1.96*se, end = avg + 1.96*se) round(results*100,1) #&gt; avg se start end #&gt; 1 2.9 0.6 1.7 4.1 Our confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the electoral vote. Are we now ready to declare a probability of Clinton winning the popular vote? Not yet. In our model \\(d\\) is a fixed parameter so we can’t talk about probabilities. To provide probabilities, we will need to learn about Bayesian statistics. Exercises We have been using urn models to motivate the use of probability models. Most data science applications are not related to data obtained from urns. More common are data that come from individuals. The reason probability plays a role here is because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population. Let’s revisit the heights dataset. Suppose we consider the males in our course the population. library(dslabs) data(heights) x &lt;- heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% .$height Mathematically speaking, x is our population. Using the urn analogy, we have an urn with the values of x in it. What are the population average and standard deviation of our population? Call the population average computed above \\(\\mu\\) and the standard deviation \\(\\sigma\\). Now take a sample of size 50, with replacement, and construct an estimate for \\(\\mu\\) and \\(\\sigma\\). Set the seed at 1 based on what has been described in this section. What does the theory tell us about the sample average \\(\\bar{X}\\) and how it is related to \\(\\mu\\)? A. It is practically identical to \\(\\mu\\). B. It is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). C. It is a random variable with expected value \\(\\mu\\) and standard error \\(\\sigma\\). D. Contains no information. So how is this useful? We are going to use an oversimplified yet illustrative example. Suppose we want to know the average height of our male students, but we only get to measure 50 of the 708. We will use \\(\\bar{X}\\) as our estimate. We know from the answer to exercise 3 that the standard estimate of our error \\(\\bar{X}-\\mu\\) is \\(\\sigma/\\sqrt{N}\\). We want to compute this, but we don’t know \\(\\sigma\\). Based on what is described in this section, show your estimate of \\(\\sigma\\). Now that we have an estimate of \\(\\sigma\\), let’s call our estimate \\(s\\). Construct a 95% confidence interval for \\(\\mu\\). Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. What proportion of these intervals include \\(\\mu\\)? Set the seed to 1. In this section we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election. data(polls_us_election_2016) polls &lt;- polls_us_election_2016 %&gt;% filter(pollster %in% c(&quot;Rasmussen Reports/Pulse Opinion Research&quot;,&quot;The Times-Picayune/Lucid&quot;) &amp; enddate &gt;= &quot;2016-10-15&quot; &amp; state == &quot;U.S.&quot;) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) We want to answer the question: is there a poll bias? Make a plot showing the spreads for each poll. The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance. The urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(d\\). To answer the question: is there an urn model?, we will model the observed data \\(Y_ij\\) in the following way: \\[ Y_{ij} = d + b_i + \\varepsilon_{ij} \\] with \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\) and \\(\\varepsilon_ij\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\) and standard deviation \\(\\sigma_i\\) regardless of \\(j\\). Which of the following best represents our question? A. Is \\(\\varepsilon_ij\\) = 0? B. How close are the \\(Y_ij\\) to \\(d\\)? C. Is \\(b_1 \\neq b_2\\)? D. Are \\(b_1 = 0\\) and \\(b_2 = 0\\) ? In the right side of this model only \\(\\varepsilon_ij\\) is a random variable. The other two are constants. What is the expected value of \\(Y_{1j}\\)? Suppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first poll, \\(Y_{11},\\dots,Y_{1N_1}\\) with \\(N_1\\) the number of polls conducted by the first pollster: polls %&gt;% filter(pollster==&quot;Rasmussen Reports/Pulse Opinion Research&quot;) %&gt;% summarize(N_1 = n()) #&gt; N_1 #&gt; 1 16 What is the expected values \\(\\bar{Y}_1\\)? What is the standard error of \\(\\bar{Y}_1\\) ? What is the expected value \\(\\bar{Y}_2\\)? What is the standard error of \\(\\bar{Y}_2\\) ? Using what we learned by answering the questions above, what is the expected value of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)? Using what we learned by answering the questions above, what is the standard error of \\(\\bar{Y}_{2} - \\bar{Y}_1\\)? \\[ \\mbox{SE}(\\bar{Y}_{2} - \\bar{Y}_1) = \\sqrt{\\mbox{SE}(\\bar{Y}_{2})^2 + \\mbox{SE}(\\bar{Y}_1)^2} = \\sqrt{\\sigma_2^2/N_2 + \\sigma_1^2/N_1}\\] The answer to the question above depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), which we don’t know. We learned that we can estimate these with the sample standard deviation. Write code that computes these two estimates. What does the CLT tells us about the distribution of \\(\\bar{Y}_2 - \\bar{Y}_1\\)? A. Nothing because this not the average of a sample. B. Because the \\(Y_{ij}\\) are approximately normal, so are the averages. \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages so if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normals is also normal. D. The data are not 0 or 1, so CLT does not apply. So we have constructed a random variable that has expected value \\(b_2 - b_1\\), the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on \\(\\sigma_1\\) and \\(\\sigma_2\\), but we can plug the sample standard deviations we computed above. We started off by asking: is \\(b_2 - b_1\\) different from 0? Use all the information we have learned above to construct a 95% confidence interval for the difference \\(b_2\\) and \\(b_1\\). The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. Random variability does not seem to explain it. We can compute a p-value to relay the fact that chance does not explain it. What is the p-value? The statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error: \\[ \\frac{\\bar{Y}_2 - \\bar{Y}_1}{\\sqrt{s_2^2/N_2 + s_1^2/N_1}} \\] is called a the t-statistic. Later we learn of another approximation for the distribution of this statistic for values of \\(N_2\\) and \\(N_1\\) that aren’t large enough for the CLT. Now note that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can actually construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect? For this exercise, create a new table: polls &lt;- polls_us_election_2016 %&gt;% filter(enddate &gt;= &quot;2016-10-15&quot; &amp; state == &quot;U.S.&quot;) %&gt;% group_by(pollster) %&gt;% filter(n() &gt;= 5) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;% ungroup() Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation. "],
["bayesian-statistics.html", "Chapter 36 Bayesian statistics 36.1 Bayes theorem 36.2 Bayes Theorem simulation 36.3 Bayes in practice 36.4 The hierarchical model Exercises", " Chapter 36 Bayesian statistics What does it mean when an election forecaster tell us that a given candidate has a 90% chance of winning? In the context of the urn model, this would be equivalent to stating that the probability \\(p&gt;0.5\\) is 90%. But, as we discussed, in the urn model \\(p\\) is a fixed parameter and it does not make sense to talk about probability. With Bayesian statistics, we assume it is in fact random. Forecasters also use models to describe variability at different levels. For example, sampling variability, pollster to pollster variability, day to day variability, and election to election variability. One of the most successful approaches used for this are hierarchical models, which can be explained in the context of Bayesian statistics. 36.1 Bayes theorem We start by reviewing Bayes theorem. We do this using a hypothetical cystic fibrosis test as an example. Suppose a test for cystic fibrosis has an accuracy of 99%. We will use the following notation: \\[ \\mbox{Prob}(+ \\mid D=1)=0.99, \\mbox{Prob}(- \\mid D=0)=0.99 \\] with \\(+\\) meaning a positive test and \\(D\\) representing if you actually have the disease (1) or not (0). Suppose we select a random person and they test positive, what is the probability that they have the disease? We write this as \\(\\mbox{Prob}(D=1 \\mid +)?\\) The cystic fibrosis rate is 1 in 3,900 which implies that \\(\\mbox{Prob}(D=1)=0.00025\\). To answer this question, we will use Bayes Theorem, which in general tells us that: \\[ \\mbox{Pr}(A \\mid B) = \\frac{\\mbox{Pr}(B \\mid A)\\mbox{Pr}(A)}{\\mbox{Pr}(B)} \\] This equation applied to our problem becomes: \\[ \\begin{aligned} \\mbox{Pr}(D=1 \\mid +) &amp; = \\frac{ P(+ \\mid D=1) \\cdot P(D=1)} {\\mbox{Pr}(+)} \\\\ &amp; = \\frac{\\mbox{Pr}(+ \\mid D=1)\\cdot P(D=1)} {\\mbox{Pr}(+ \\mid D=1) \\cdot P(D=1) + \\mbox{Pr}(+ \\mid D=0) \\mbox{Pr}( D=0)} \\end{aligned} \\] Plugging in the numbers we get: \\[ \\frac{0.99 \\cdot 0.00025}{0.99 \\cdot 0.00025 + 0.01 \\cdot (.99975)} = 0.02 \\] This says that despite the test having 0.99 accuracy, the probability of having the disease given a positive test is only 0.02. This may appear counterintuitive to some, but the reason this is the case is because we have to factor in the very rare probability that a person, chosen at random, has the disease. To illustrate this we run a Monte Carlo simulation. 36.2 Bayes Theorem simulation The following simulation is meant to help you visualize Bayes Theorem. We start by randomly selecting 100,000 people from a population in which the disease in question has a 5% prevalence. prev &lt;- 0.00025 N &lt;- 100000 outcome &lt;- sample(c(&quot;Disease&quot;,&quot;Healthy&quot;), N, replace = TRUE, prob = c(prev,1-prev)) Note that there are very few people with the disease: N_D &lt;- sum(outcome == &quot;Disease&quot;) N_D #&gt; [1] 23 N_H &lt;- sum(outcome == &quot;Healthy&quot;) N_H #&gt; [1] 99977 Also, there are many without the disease, which makes it more probable that we will see some false positives given that the test is not perfect. Now each person gets the test which is correct 90% of the time. accuracy &lt;- 0.99 test &lt;- vector(&quot;character&quot;,N) test[outcome==&quot;Disease&quot;] &lt;- sample(c(&quot;+&quot;,&quot;-&quot;), N_D, replace=TRUE, prob = c(accuracy, 1 - accuracy)) test[outcome==&quot;Healthy&quot;] &lt;- sample(c(&quot;-&quot;,&quot;+&quot;), N_H, replace=TRUE, prob = c(accuracy, 1 - accuracy)) Because there are so many more controls than cases, even with a low false positive rate, we get more controls than cases in the group that tested positive (code not shown): table(outcome, test) #&gt; test #&gt; outcome - + #&gt; Disease 0 23 #&gt; Healthy 99012 965 From this table, we see that the proportion of positive tests that have the disease is 23 out of 988. We can run this over and over again to see that, in fact, the probability converges to about 0.022 The proportions in the top plot shows \\(\\mbox{Pr}(D=1)\\). The bottom left shows \\(\\mbox{Pr}(D=1 \\mid +)\\) and the bottom right shows \\(\\mbox{Pr}(D=0 \\mid +)\\). 36.3 Bayes in practice José Iglesias is a professional baseball player. In April 2013, when he was starting his career, he was performing rather well: Month At Bats H AVG April 20 9 .450 The batting average (AVG) statistic is one way of measuring success. Roughly speaking, it tells us the success rate when batting. An AVG of .450 means José has been successful 45% of the times he has batted (At Bats) which is rather high, historically speaking. Keep in mind, for example, that no one has finished a season with an AVG of .400 or more since Ted Williams did it in 1941! To illustrate the way hierarchical models are powerful, we will try to predict José’s batting average at the end of the season. Note that in a typical season, players have about 500 at bats. With the techniques we have learned up to now, referred to as frequentist techniques, the best we can do is provide a confidence interval. We can think of outcomes from hitting as a binomial with a success rate of \\(p\\). So if the success rate is indeed .450, the standard error of just 20 at bats is: \\[ \\sqrt{\\frac{.450 (1-.450)}{20}}=.111 \\] This means that our confidence interval is .450-.222 to .450+.222 or .228 to .672. This prediction has two problems. First, it is very large, so not very useful. Second, it is centered at .450, which implies that our best guess is that this new player will break Ted Williams’ record. If you follow baseball, this last statement will seem wrong and this is because you are implicitly using a hierarchical model that factors in information from years of following baseball. Here we show how we can quantify this intuition. First, let’s explore the distribution of batting averages for all players with more than 500 at bats during the previous three seasons: Figure 36.1: Batting average histograms for 2010, 2011, and 2012. The average player had an AVG of .275 and the standard deviation of the population of players was 0.027. So we can see already that .450 would be quite an anomaly since it is over six standard deviations away from the mean. So is José lucky or is he the best batter seen in the last 50 years? Perhaps it’s a combination of both. But how lucky and how good is he? If we become convinced that he is lucky, we should trade him to a team that trusts the .450 observation and is maybe overestimating his potential. 36.4 The hierarchical model The hierarchical model provides a mathematical description of how we came to see the observation of .450. First, we pick a player at random with an intrinsic ability summarized by, for example, \\(p\\). Then we see 20 random outcomes with success probability \\(p\\). We use a model to represents two levels of variability in our data. First, each player is assigned a natural ability to hit at birth. We will use the symbol \\(p\\) to represent this ability. You can think of \\(\\theta\\) as the batting average you would converge to if this particular player batted over and over again. Based on the plots above, we assume that \\(p\\) has a normal distribution. With expected value .270 and standard error 0.027. Now the second level of variability has to do with luck when batting. Regardless of how good the player is, sometimes you have bad luck and sometimes you have good luck. At each at bat, this player has a probability of success \\(p\\). If we add up these successes and failures, then the CLT tells us that the observed average, call it \\(Y\\), has a normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/\\sqrt{N}}\\) with \\(N\\) the number of at bats. Statistical textbooks will write the model like this: \\[ \\begin{aligned} p &amp;\\sim N(\\mu, \\tau^2) \\mbox{ describes randomness in picking a player}\\\\ Y \\mid p &amp;\\sim N(p, \\sigma^2) \\mbox{ describes randomness in the performance of this particular player} \\end{aligned} \\] with \\(\\mu = .270\\), \\(\\tau = 0.027\\), and \\(\\sigma^2 = p(1-p)/N\\). Note the two levels (this is why we call them hierarchical): 1) Player to player variability and 2) variability due to luck when batting. In a Bayesian framework, the first level is called a prior distribution and the second the sampling distribution. Now, let’s use this model for José’s data. Suppose we want to predict his innate ability in the form of his true batting average \\(p\\). This would be the hierarchical model for our data: \\[ \\begin{aligned} p &amp;\\sim N(.275, .027^2) \\\\ Y \\mid p &amp;\\sim N(p, .111^2) \\end{aligned} \\] We now are ready to compute a posterior distribution to summarize our prediction of \\(p\\). The continuous version of Bayes’ rule can be used here to derive the posterior probability function, which is the distribution of \\(p\\) assuming we observe \\(Y=y\\). In our case we can show that \\(p\\) when \\(Y=y\\) follows a normal distribution with expected value: \\[ \\begin{aligned} \\mbox{E}(p \\mid Y=y) &amp;= B \\mu + (1-B) y\\\\ &amp;= \\mu + (1-B)(y-\\mu)\\\\ \\mbox{with } B &amp;= \\frac{\\sigma^2}{\\sigma^2+\\tau^2} \\end{aligned} \\] This is a weighted average of the population average \\(\\mu\\) and the observed data \\(Y\\). The weight depends on the SD of the population \\(\\tau\\) and the SD of our observed data \\(\\sigma\\). This weighted average is sometimes referred to as shrinking because it shrinks estimates towards a prior mean. In the case of José Iglesias, we have: \\[ \\begin{aligned} \\mbox{E}(p \\mid Y=.450) &amp;= B \\times .275 + (1 - B) \\times .450 \\\\ &amp;= .275 + (1 - B)(.450 - .275) \\\\ B &amp;=\\frac{.111^2}{.111^2 + .027^2} = 0.944\\\\ \\mbox{E}(p \\mid Y=450) &amp;\\approx .285 \\end{aligned} \\] The standard error can be shown to be: \\[ \\mbox{SE}(p\\mid y)^2 = \\frac{1}{1/\\sigma^2+1/\\tau^2} = \\frac{1}{1/.111^2 + 1/.027^2} = 0.00069 \\] and the standard deviation is therefore \\(0.026\\). So we started with a frequentist 95% confidence interval that ignored data from other players and summarized just José’s data: .450 \\(\\pm\\) 0.220. Then we used a Bayesian approach that incorporated data from other players and other years to obtain a posterior probability. This is actually referred to as an empirical Bayes approach because we used data to construct the prior. From the posterior, we can report what is called a 95% credible interval by reporting a region, centered at the mean, with a 95% chance of occurring. In our case, this turns out to be: .285 \\(\\pm\\) 0.052. The Bayesian credible interval suggests that if another team is impressed by the .450 observation, we should consider trading José as we are predicting he will be just slightly above average. Interestingly, the Red Sox traded José to the Detroit Tigers in July. Here are the José Iglesias batting averages for the next five months. Month At Bat Hits AVG April 20 9 .450 May 26 11 .423 June 86 34 .395 July 83 17 .205 August 85 25 .294 September 50 10 .200 Total w/o April 330 97 .293 Although both intervals included the final batting average, the Bayesian credible interval provided a much more precise prediction. In particular, it predicted that he would not be as good during the remainder of the season. Exercises In 1999, in England, Sally Clark was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998. In both cases, she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500 \\(\\times\\) 8,500 \\(\\approx\\) 73 million. Which of the following do you agree with? A. Sir Meadow assumed that the probability of the second son being affected by SIDS was independent of the first son being affected, thereby ignoring possible genetic causes. If genetics plays a role then: \\(\\mbox{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) &lt; \\mbox{P}r(\\mbox{first case of SIDS})\\). B. Nothing. The multiplicative rule always applies in this way: \\(\\mbox{Pr}(A \\mbox{ and } B) =\\mbox{Pr}(A)\\mbox{Pr}(B)\\) C. Sir Meadow is an expert and we should trust his calculations. D. Numbers don’t lie. Let’s assume that there is in fact a genetic component to SIDS and the the probability of \\(\\mbox{Pr}(\\mbox{second case of SIDS} \\mid \\mbox{first case of SIDS}) = 1/100\\), is much higher than 1 in 8,500. What is the probability of both of her sons dying of SIDS? Many press reports stated that the expert claimed the probability of Sally Clark being innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written like this: \\[ \\mbox{Pr}(\\mbox{A mother is a son-murdering psychopath} \\mid \\mbox{two of her children are found dead with no evidence of physical harm }) \\] Bayes’ rule tells us this probability is equal to: Assume that the chance of a son-murdering psychopath finding a way to kill her children, without leaving evidence of physical harm, is: \\[ \\mbox{Pr}(A \\mid B) = 0.50 \\] with \\(A = \\mbox{two of her children are found dead with no evidence of physical harm}\\) and \\(B=\\mbox{a mother is a son-murdering psychopath} ) = 0.50\\). Assume that the rate of rate of son-murdering psychopaths mothers is 1 in 1,000,000. According to Bayes’ rule, what is the probability of \\(\\mbox{Pr}(B \\mid A)\\) ? After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clarke was acquitted in June 2003. What did the expert miss? A. He made an arithmetic error. B. He made two mistakes. First, he misused the multiplicative rule and did not take into account how rare it is for a mother to murder her children. After using Bayes’ rule, we found a probability closer to 0.5 than 1 in 73 million. C. He mixed up the numerator and denominator of Bayes’ rule. D. He did not use R. Florida is one of the most closely watched states in the U.S. election because it has many electoral votes and the election is generally close. Create the following table with the polls taken during the last two weeks: library(tidyverse) library(dslabs) data(polls_us_election_2016) polls &lt;- polls_us_election_2016 %&gt;% filter(state == &quot;Florida&quot; &amp; enddate &gt;= &quot;2016-11-04&quot; ) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) Take the average of these 11. The CLT tells us this average is approximately normal. Calculate an average and provide an estimate of the standard error. Save your results in an object called results. Now assume a Bayesian model that sets the prior distribution for Florida’s election night spread \\(d\\) to be Normal with expected value \\(\\mu\\) and standard deviation \\(\\tau\\). What are the interpretations of \\(\\mu\\) and \\(\\tau\\)? A. \\(\\mu\\) and \\(\\tau\\) are arbitrary numbers that let us make probability statements about \\(d\\). B. \\(\\mu\\) and \\(\\tau\\) summarize what we would predict for Florida before seeing any polls. Based on past elections, we would set \\(\\mu\\) close to 0 because both Republicans and Democrats have won, and \\(\\tau\\) at about \\(0.02\\), because these elections tend to be close. C. \\(\\mu\\) and \\(\\tau\\) summarize what we want to be true. We therefore set \\(\\mu\\) at \\(0.10\\) and \\(\\tau\\) at \\(0.01\\). D. The choice of prior has no effect on Bayesian Analysis. The CLT tells us that our estimate of the spread \\(\\hat{d}\\) has normal distribution with expected value \\(d\\) and standard deviation \\(\\sigma\\) calculated in problem 6. Use the formulas we showed for the posterior distribution to calculate the expected value of the posterior distribution if we set \\(\\mu = 0\\) and \\(\\tau = 0.01\\). Now compute the standard deviation of the posterior distribution. Using the fact that the posterior distribution is normal, create an interval that has a 95% probability of occurring centered at the posterior expected value. Note that we call these credible intervals. According to this analysis, what was the probability that Trump wins Florida? Now use sapply function to change the prior variance from seq(0.05, 0.05, len = 100) and observe how the probability changes by making a plot. "],
["election-forecasting.html", "Chapter 37 Election forecasting 37.1 Bayesian approach 37.2 The general bias 37.3 Mathematical representations of models 37.4 Predicting the electoral college 37.5 Forecasting Exercise", " Chapter 37 Election forecasting In previous section we generated these data tables: polls &lt;- polls_us_election_2016 %&gt;% filter(state == &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot; &amp; (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) one_poll_per_pollster &lt;- polls %&gt;% group_by(pollster) %&gt;% filter(enddate == max(enddate)) %&gt;% ungroup() results &lt;- one_poll_per_pollster %&gt;% summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %&gt;% mutate(start = avg - 1.96*se, end = avg + 1.96*se) We will use these below for our forecasting. 37.1 Bayesian approach Pollsters tend to make probabilistic statements about the results of the election. For example, “The chance that Obama wins the electoral colleges is 91%” is a probabilistic statement about the parameter \\(d\\). We showed that for the 2016 election, FiveThirtyEight gave Clinton a 81.4% chance of winning the popular vote. To do this, they used the Bayesian approach we described. We assume a hierarchical model similar to what we did to predict the performance of a baseball player. Statistical textbooks will write the model like this: \\[ \\begin{aligned} d &amp;\\sim N(\\mu, \\tau^2) \\mbox{ describes our best guess had we not seen any polling data}\\\\ \\bar{X} \\mid d &amp;\\sim N(d, \\sigma^2) \\mbox{ describes randomness due to sampling and the pollster effect} \\end{aligned} \\] For our best guess, we note that before any poll data is available we can use data sources other than polling data. A popular approach is to use what are called fundamentals, which are based on properties about the current economy that historically appear to have an effect in favor or against the incumbent party. We won’t use these here. Instead we will use \\(\\mu = 0\\), which is interpreted as a model that simply does not provide any information on who will win. For the standard deviation, we will use recent historical data that shows the winner of the popular vote has an average spread of about 3.5%. Therefore we set \\(\\tau = 0.035\\). Now we can use the formulas for the posterior distribution for the parameter \\(d\\): the probability of \\(d&gt;0\\) given the observed poll data: mu &lt;- 0 tau &lt;- 0.035 sigma &lt;- results$se Y &lt;- results$avg B &lt;- sigma^2 / (sigma^2 + tau^2) posterior_mean &lt;- B*mu + (1-B)*Y posterior_se &lt;- sqrt( 1/ (1/sigma^2 + 1/tau^2)) posterior_mean #&gt; [1] 0.0281 posterior_se #&gt; [1] 0.00615 To make a probability statement, we use the fact that the posterior distribution is also normal. And we have a credible interval of: posterior_mean + c(-1.96, 1.96)*posterior_se #&gt; [1] 0.0160 0.0401 The posterior probability \\(\\mbox{Pr}(d&gt;0 \\mid \\bar{X})\\) is: 1 - pnorm(0, posterior_mean, posterior_se) #&gt; [1] 1 This says we are 100% sure Clinton will win the popular vote which seems too overconfident. Also, it is not in agreement with FiveThirtyEight’s 81.4%. What explains this difference? 37.2 The general bias After elections are over, one can look at the difference between pollster predictions and actual result. An important observation that our model does not take into account, is that it is common to see the general bias that affects many pollsters in the same way. There is no good explanation for this, but we do observe it in historical data: in one election, the average of polls favors Democrats by 2%, then in the following election they favor Republicans by 1%, then in the next election there is no bias, then in the following one Republicans are favored by 3%, and so on. In 2016, the polls were biased in favor of the democrats by 1-2%. Although we know this bias term affects our polls, we have no way of knowing what this bias is until election. So we can’t correct our polls accordingly. What we can do is include a term in our model that accounts for this variability. 37.3 Mathematical representations of models Suppose we are collecting data from one pollster and we assume there is no general bias. The pollster collects several polls with a sample size of \\(N\\), so we observe several measurements of the spread \\(X_1, \\dots, X_J\\). The theory tells us that these random variables have expected value \\(d\\) and \\(2 \\sqrt{p(1-p)/N}\\). For reasons that will soon become clear, we can represent this model mathematically like this: \\[ X_j = d + \\varepsilon_j \\] We use the index \\(j\\) to represent the different polls and we define \\(\\varepsilon_j\\) to be a random variable that explains the poll to poll variability introduced by sampling error. To do this, we assume its average is 0 and standard error is \\(2 \\sqrt{p(1-p)/N}\\). If \\(d\\) is 2.1 and the sample size for these polls is 2,000, we can simulate \\(J=6\\) data points from this model like this: set.seed(3) J&lt;- 6 N &lt;- 2000 d &lt;- .021 p &lt;- (d + 1)/2 X &lt;- d + rnorm(J,0,2*sqrt(p*(1-p)/N)) Now suppose we \\(J=6\\) data points from have data from \\(I=5\\) different pollsters. To represent this we now need two indexes, one for pollster and one for the polls each pollster takes. We use \\(X_{ij}\\) with \\(i\\) representing the pollster and \\(j\\) representing the \\(j\\)-th poll from that pollster. If we apply the same model, we write: \\[ X_{i,j} = d + h_i + \\varepsilon_{i,j} \\] To simulate data, we now have to loop through the pollsters: I &lt;- 5 J &lt;- 6 N &lt;- 2000 X &lt;- sapply(1:I, function(i){ d + rnorm(J,0,2*sqrt(p*(1-p)/N)) }) The simulated data: does not really seem to capture the features of the actual data: The model above does not account for pollster to pollster variability. To fix this, we add a new term for the pollster effect. We will use \\(\\theta_i\\) to represent the house effect of the \\(i\\)-th pollster. The model is now augmented to: \\[ X_{i,j} = d + h_i + \\varepsilon_{i,j} \\] To simulate data from a specific pollster, we now need to draw an \\(h_i\\) and then add the \\(\\varepsilon\\)s. Here is how we would do it for one specific pollster. We assume \\(\\sigma_h\\) is 0.025: I &lt;- 5 J &lt;- 6 N &lt;- 2000 d &lt;- .021 p &lt;- (d + 1)/2 h &lt;- rnorm(I, 0, 0.025) X &lt;- sapply(1:I, function(i){ d + h[i] + rnorm(J,0,2*sqrt(p*(1-p)/N)) }) The simulated data now looks more like the actual data: Note that \\(h_i\\) is common to all the observed spreads from a specific pollster. Different pollsters have a different \\(h_i\\), which explains why we can see the groups of points shift up and down from pollster to pollster. Now, in the model above, we assume the average house effect is 0. We think that for every pollster biased in favor of our party, there is another one in favor of the other and assume the standard deviation is \\(\\sigma_h\\). But historically we see that every election has a general bias affecting all polls. We can observe this with the 2016 data, but if we collect historical data we see that the average of polls misses by more than models like the one above predict. To see this, we would take the average of polls for each election year and compare it to the actual value. If we did this, we would see a difference with a standard deviation of between 2-3%. To incorporate this into the model, we can add another term to account for this variability: \\[ X_{ij} = d + b + h_i + \\varepsilon_{ij} \\] and model \\(b\\) as having expected value 0 and, based on historical data, assume the standard error for \\(b\\) is \\(\\sigma_b = 0.025\\). \\[ X_{ij} = d + b + h_i + \\varepsilon_{ij} \\] The variability of \\(b\\) is not observed because every single poll we observe in 2016 has this general bias. An implication of adding this term to the model is that the standard deviation for \\(X_{ij}\\) is actually higher than what we earlier called \\(\\sigma\\), which combines the pollster variability and the sample in variability, and was estimated with: sd(one_poll_per_pollster$spread) #&gt; [1] 0.0242 since we have to add \\(\\sigma_b\\). And note that: \\[ \\bar{X} = d + b + \\frac{1}{N}\\sum_{i=1}^N X_i \\] which implies that the standard deviation of \\(\\bar{X}\\) is: \\[ \\sqrt{\\sigma^2/N + \\sigma_b^2} \\] Since the same \\(b\\) is in every measurement, the average does not reduce its variance. This is an important point: it does not matter how many polls you take, this bias does not get reduced. If we redo the Bayesian calculation taking this variability into account, we get a result much closer to FiveThirtyEight’s: mu &lt;- 0 tau &lt;- 0.035 sigma &lt;- sqrt(results$se^2 + .025^2) Y &lt;- results$avg B &lt;- sigma^2 / (sigma^2 + tau^2) posterior_mean &lt;- B*mu + (1-B)*Y posterior_se &lt;- sqrt( 1/ (1/sigma^2 + 1/tau^2)) 1 - pnorm(0, posterior_mean, posterior_se) #&gt; [1] 0.817 37.4 Predicting the electoral college Up to now we have focused on the popular vote. But in the United States, elections are not decided by the popular vote but rather by what is known as the electoral college. Each state gets a number of electoral votes that depends, in a somewhat complex way, on the population size of the state. Here are the top 5 states ranked by electoral votes: results_us_election_2016 %&gt;% top_n(5, electoral_votes) #&gt; state electoral_votes clinton trump others #&gt; 1 California 55 61.7 31.6 6.7 #&gt; 2 Florida 29 47.8 49.0 3.2 #&gt; 3 Illinois 20 55.8 38.8 5.4 #&gt; 4 New York 29 59.0 36.5 4.5 #&gt; 5 Pennsylvania 20 47.9 48.6 3.6 #&gt; 6 Texas 38 43.2 52.2 4.5 With some minor exceptions we don’t discuss, the electoral votes are won all or nothing. For example, if you win California by just 1 vote, you still get all 55 of its electoral votes. This means that by winning a few big states by a large margin, but losing many small states by small margins, you can win the popular vote and yet lose the electoral college. This happened in 1876, 1888, 2000 and 2016. The idea behind this is to avoid a few large states having the power to dominate the presidential election. Nonetheless, many people in the US consider the electoral college unfair and would like to see it abolished. We are now ready to predict the electoral college result for 2016. We start by aggregating results from a poll taken during the last week before the election: results &lt;- polls_us_election_2016 %&gt;% filter(state!=&quot;U.S.&quot; &amp; !grepl(&quot;CD&quot;, state) &amp; enddate &gt;=&quot;2016-10-31&quot; &amp; (grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;% group_by(state) %&gt;% summarize(avg = mean(spread), sd = sd(spread), n = n()) %&gt;% mutate(state = as.character(state)) Here are the 10 closest races according to the polls: results %&gt;% arrange(abs(avg)) #&gt; # A tibble: 47 x 4 #&gt; state avg sd n #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Florida 0.00356 0.0163 7 #&gt; 2 North Carolina -0.00730 0.0306 9 #&gt; 3 Ohio -0.0104 0.0252 6 #&gt; 4 Nevada 0.0169 0.0441 7 #&gt; 5 Iowa -0.0197 0.0437 3 #&gt; 6 Michigan 0.0209 0.0203 6 #&gt; # ... with 41 more rows We now introduce the command left_join that will let us easily add the number of electoral votes for each state from the data set us_electoral_votes_2016. We will describe this function in detail in the Wrangling chapter. Here, we simply say that the function combines the two datasets so that the information from the second argument is added to the information in the first: results &lt;- left_join(results, results_us_election_2016, by = &quot;state&quot;) Notice that some states have no polls because the winner is pretty much known: results_us_election_2016 %&gt;% filter(!state %in% results$state) #&gt; state electoral_votes clinton trump others #&gt; 1 Alaska 3 36.6 51.3 12.2 #&gt; 2 Rhode Island 4 54.4 38.9 6.7 #&gt; 3 Wyoming 3 21.9 68.2 10.0 #&gt; 4 District of Columbia 3 90.9 4.1 5.0 No polls were conducted in DC, Rhode Island, Alaska, and Wyoming because the first two are sure to be Democrats and the last two Republicans. The code below assigns a standard deviation, the median of the rest, to states with just one poll. results &lt;- results %&gt;% mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm=TRUE), sd)) To make probabilistic arguments, we will use a Monte Carlo simulation. For each state, we apply the Bayesian approach to generate an election day \\(d\\). We could construct the priors for each state based on recent history. However, to keep it simple, we assign a prior to each state that assumes we know nothing about what will happen. Since from election year to election year the results from a specific state don’t change that much, we will assign a standard deviation of 2% or \\(\\tau=0.02\\). The Bayesian calculation looks like this: mu &lt;- 0 tau &lt;- 0.02 results %&gt;% mutate(sigma = sd/sqrt(n), B = sigma^2 / (sd^2 + tau^2), posterior_mean = B*mu + (1-B)*avg, posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2))) %&gt;% arrange(abs(posterior_mean)) #&gt; # A tibble: 47 x 12 #&gt; state avg sd n electoral_votes clinton trump others sigma #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Flor… 0.00356 0.0163 7 29 47.8 49 3.2 0.00618 #&gt; 2 Nort… -0.00730 0.0306 9 15 46.2 49.8 4 0.0102 #&gt; 3 Ohio -0.0104 0.0252 6 18 43.5 51.7 4.8 0.0103 #&gt; 4 Iowa -0.0197 0.0437 3 6 41.7 51.1 7.1 0.0252 #&gt; 5 Neva… 0.0169 0.0441 7 6 47.9 45.5 6.6 0.0167 #&gt; 6 Mich… 0.0209 0.0203 6 16 47.3 47.5 5.2 0.00827 #&gt; # ... with 41 more rows, and 3 more variables: B &lt;dbl&gt;, #&gt; # posterior_mean &lt;dbl&gt;, posterior_se &lt;dbl&gt; The estimates based on posterior does move the estimates towards 0, although the states with many polls are influenced less. This is expected as the more poll data we collect, the more we trust those results: results %&gt;% mutate(sigma = sd/sqrt(n), B = sigma^2 / (sigma^2 + tau^2), posterior_mean = B*mu + (1-B)*avg, posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2))) %&gt;% ggplot(aes(avg, posterior_mean, size = n)) + geom_point() + geom_abline(slope = 1, intercept = 0) Now we repeat this 10,000 times and generate an outcome from the posterior. In each iteration, we keep the total number of electoral votes for Clinton. Note that we add 7 to account for Rhode Island and D.C.: mu &lt;- 0 tau &lt;- 0.02 clinton_EV &lt;- replicate(1000, { results %&gt;% mutate(sigma = sd/sqrt(n), B = sigma^2 / (sigma^2 + tau^2), posterior_mean = B*mu + (1-B)*avg, posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)), simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se), clinton = ifelse(simulated_result&gt;0, electoral_votes, 0)) %&gt;% summarize(clinton = sum(clinton)) %&gt;% .$clinton + 7## 7 for Rhode Island and D.C. }) mean(clinton_EV&gt;269) #&gt; [1] 0.998 This model gives Clinton over 99% chance of winning. Here is a histogram of the possible outcomes: A similar prediction was made by the Princeton Election Consortium. We now know it was quite off. What happened? The model above ignores the general bias. The general bias in 2016 was not that big compared to other years: it was between 1 and 2%. But because the election was close in several big states, a large number of polls made the estimates of standard errors small, and by ignoring the variability introduced by the general bias, pollsters were over confident on the poll data. FiveThirtyEight, which models the general bias in a rather sophisticated way, reported a closer result. We can simulate the results now with a bias term. For the state level, the general bias can be larger so we set it at \\(\\sigma_b = 0.03\\): tau &lt;- 0.02 bias_sd &lt;- 0.03 clinton_EV_2 &lt;- replicate(1000, { results %&gt;% mutate(sigma = sqrt(sd^2/n + bias_sd^2), B = sigma^2 / (sigma^2 + tau^2), posterior_mean = B*mu + (1-B)*avg, posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)), simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se), clinton = ifelse(simulated_result&gt;0, electoral_votes, 0)) %&gt;% summarize(clinton = sum(clinton) + 7) %&gt;% .$clinton ## 7 for Rhode Island and D.C. }) mean(clinton_EV_2&gt;269) #&gt; [1] 0.837 This gives us a much more sensible result. Looking at the outcomes of the simulation, we see how the bias term adds variability to the final results. FiveThirtyEight includes many other features we do not include here. One is that they model variability with distributions that have high probabilities for extreme events compared to the normal. They predicted a probability of 71%. 37.5 Forecasting Forecasters like to make predictions well before the election. The predictions are adapted as new polls come out. However, an important question forecasters must ask is: how informative are polls taken several weeks before the election? Here we study the variability of poll results across time. To make sure the variability we observe is not due to pollster effects, let’s study data from one pollster: one_pollster &lt;- polls_us_election_2016 %&gt;% filter(pollster == &quot;Ipsos&quot; &amp; state == &quot;U.S.&quot;) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) Since there is no pollster effect, then perhaps the theoretical standard error matches the data-derived standard deviation. We compute both here: se &lt;- one_pollster %&gt;% summarize(empirical = sd(spread), theoretical = 2*sqrt(mean(spread)*(1-mean(spread))/min(samplesize))) se #&gt; empirical theoretical #&gt; 1 0.0403 0.0326 The empirical standard deviation is higher than the highest possible theoretical estimate. Furthermore, the spread data does not look normal as the theory would predict: one_pollster %&gt;% ggplot(aes(spread)) + geom_histogram(binwidth = 0.01, color = &quot;black&quot;) Where is the extra variability coming from? The following plots make a strong case that the extra variability comes from time annunciations not accounted for by the theory that assumes \\(p\\) is fixed: Some of the peaks and valleys we see coincide with events such as the party conventions, which tend to give the candidate a boost. We can see them consistently across several pollsters: polls_us_election_2016 %&gt;% filter(state == &quot;U.S.&quot; &amp; enddate&gt;=&quot;2016-07-01&quot;) %&gt;% group_by(pollster) %&gt;% filter(n()&gt;=10) %&gt;% ungroup() %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;% ggplot(aes(enddate, spread)) + geom_smooth(method = &quot;loess&quot;, span = 0.1) + geom_point(aes(color=pollster), show.legend = FALSE, alpha=0.6) This implies that, if we are going to forecast, our model must include a term to model that accounts for the time effect. We need to write a model including a bias term for time: \\[ Y_{ijt} = d + b + h_j + b_t + \\varepsilon_{ijt} \\] The standard deviation of \\(b_t\\) would depend on \\(t\\) since the closer we get to election day, the smaller this bias term should be. Pollsters also try to estimate trends, call them \\(f(t)\\), from these data and incorporate these into their predictions. The blue lines in the plots above: \\[ Y_{ijt} = d + b + h_j + b_t + f(t) + \\varepsilon_{ijt} \\] We usually see the estimated \\(f(t)\\) not for the difference, but for the actual percentages for each candidate: polls_us_election_2016 %&gt;% filter(state == &quot;U.S.&quot; &amp; enddate&gt;=&quot;2016-07-01&quot;) %&gt;% select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %&gt;% rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %&gt;% gather(candidate, percentage, -enddate, -pollster) %&gt;% mutate(candidate = factor(candidate, levels = c(&quot;Trump&quot;,&quot;Clinton&quot;)))%&gt;% group_by(pollster) %&gt;% filter(n()&gt;=10) %&gt;% ungroup() %&gt;% ggplot(aes(enddate, percentage, color = candidate)) + geom_point(show.legend = FALSE, alpha=0.4) + geom_smooth(method = &quot;loess&quot;, span = 0.15) + scale_y_continuous(limits = c(30,50)) #&gt; Warning: Removed 22 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 22 rows containing missing values (geom_point). Once a model like the one above is selected, we can use historical and present data to estimate all the necessary parameters to make predictions. There are a variety of methods for fitting models which we don’t discuss here. In a later chapter, we discuss some of these methods. Exercise Create this table: library(tidyverse) library(dslabs) data(&quot;polls_us_election_2016&quot;) polls &lt;- polls_us_election_2016 %&gt;% filter(state != &quot;U.S.&quot; &amp; enddate &gt;= &quot;2016-10-31&quot;) %&gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) Now for each poll use the CLT to create a 95% confidence interval for the spread reported by each poll. Call the resulting object cis with columns lower and upper for the limits of the confidence intervals. Use the `select` function to keep the columns `state, startdate, end date, pollster, grade, spread, lower, upper`. You can add the final result to the cis table you just created using the right_join function like this: add &lt;- results_us_election_2016 %&gt;% mutate(actual_spread = clinton/100 - trump/100) %&gt;% select(state, actual_spread) cis &lt;- cis %&gt;% mutate(state = as.character(state)) %&gt;% left_join(add, by = &quot;state&quot;) Now determine how often the 95% confidence interval includes the actual result. Now repeat this, but show the proportion of hits for each pollster. Show only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster. Hint: use n=n(), grade = grade[1] in the call to summarize. Repeat exercise 3, but instead of pollster, stratify by state. Here we can’t show grades. Make a barplot based on the result of exercise 4. Use coord_flip. For forecasters, it is important to call the correct winner. Hence, even if your confidence interval is incorrect, if you correctly called the right winner, your overall predictions will do better. Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same. Hint: use the function sign. Call the object resids Create a plot like in exercise 5, but for the proportion of times the sign of the spread agreed. In exercise 7, we see that for most states the polls had it right 100% of the time. For only 9 states did the polls miss more than 25% of the time. In particular, notice that in Wisconsin every single poll got it wrong. In Pennsylvania and Michigan more than 90% of the polls had the signs wrong. Make a histogram of the errors. What is the median of these errors? We see that at the state level, the median error was 3% in favor of Clinton. The distribution is not centered at 0, but at 0.03. This is the general bias we described in the section above. Create a boxplot to see if the bias was general to all states or it affected some states differently. Use filter(grade %in% c(&quot;A+&quot;,&quot;A&quot;,&quot;A-&quot;,&quot;B+&quot;) | is.na(grade))) to only include pollsters with high grades. Some of these states only have a few polls. Repeat exercise 9, but only include states with 5 good polls or more. Hint: use group_by, filter then ungroup.You will see that the West (Washington, New Mexico, California) underestimated Hillary’s performance, while the Midwest (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) overestimated it. In our simulation, we did not model this behavior since we added general bias, rather than a regional bias. Some pollsters are now modeling correlation between similar states and estimating this correlation from historical data. To learn more about this, you can learn about random effects and mixed models. "],
["association-tests.html", "Chapter 38 Association Tests 38.1 Lady Tasting Tea 38.2 Two-by-two tables 38.3 Chi-square Test 38.4 The Odds Ratio 38.5 Large samples, small p-values 38.6 Confidence intervals for the odds ratio", " Chapter 38 Association Tests The statistical tests we have covered up to now leave out a substantial portion of data types. Specifically, we have not discussed inference for binary, categorical and ordinal data. To give a very specific example, consider the following case study. A 2014 PNAS paper analyzed success rates from funding agencies in the Netherlands and concluded that their: results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not “quality of proposal”) evaluations and success rates, as well as in the language use in instructional and evaluation materials. The main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need. Here are the three columns showing the overall outcomes: data(&quot;research_funding_rates&quot;) research_funding_rates %&gt;% select(discipline, contains(&quot;total&quot;)) #&gt; discipline applications_total awards_total success_rates_total #&gt; 1 Chemical sciences 122 32 26.2 #&gt; 2 Physical sciences 174 35 20.1 #&gt; 3 Physics 76 20 26.3 #&gt; 4 Humanities 396 65 16.4 #&gt; 5 Technical sciences 251 43 17.1 #&gt; 6 Interdisciplinary 183 29 15.8 #&gt; 7 Earth/life sciences 282 56 19.9 #&gt; 8 Social sciences 834 112 13.4 #&gt; 9 Medical sciences 505 75 14.9 We have these values for each gender: names(research_funding_rates) #&gt; [1] &quot;discipline&quot; &quot;applications_total&quot; &quot;applications_men&quot; #&gt; [4] &quot;applications_women&quot; &quot;awards_total&quot; &quot;awards_men&quot; #&gt; [7] &quot;awards_women&quot; &quot;success_rates_total&quot; &quot;success_rates_men&quot; #&gt; [10] &quot;success_rates_women&quot; We can compute the totals that were successful and the totals that were not like this: totals &lt;- research_funding_rates %&gt;% select(-discipline) %&gt;% summarize_all(funs(sum)) %&gt;% summarize(yes_men = awards_men, no_men = applications_men - awards_men, yes_women = awards_women, no_women = applications_women - awards_women) So we see that a larger percent of men received awards than women: totals %&gt;% summarize(percent_men = yes_men/(yes_men+no_men), percent_women = yes_women/(yes_women+no_women)) #&gt; percent_men percent_women #&gt; 1 0.177 0.149 But could this be due just to random variability? Here we learn how to perform inference for this type of data. 38.1 Lady Tasting Tea R.A. Fisher was one of the first to formalize hypothesis testing. The “Lady Testing Tea” is one of the most famous examples. The story is as follows: an acquaintance of Fisher’s claimed that she could tell if milk was added before or after tea was poured. Fisher was skeptical. He designed an experiment to test this claim. He gave her four pairs of cups of tea: one with milk poured first, the other after. The order was randomized. The null hypothesis here is that she is guessing. Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent. As an example, suppose she picked 3 out of 4 correctly, do we believe she has a special ability? The basic question we ask is: if the tester is actually guessing, what are the chances that she gets 3 or more correct? Just as we have done before, we can compute a probability under the null hypothesis that she is guessing 4 of each. Under this null hypothesis, we can think of this particular example as picking 4 balls out of an urn with 4 blue (correct answer) and 4 red (incorrect answer) balls. Remember, she knows that there are four before tea and four after. Under the null hypothesis that she is simply guessing, each ball has the same chance of being picked. We can then use combinations to figure out each probability. The probability of picking 3 is \\({4 \\choose 3} {4 \\choose 1} / {8 \\choose 4} = 16/70\\). The probability of picking all 4 correct is \\({4 \\choose 4} {4 \\choose 0}/{8 \\choose 4}= 1/70\\). Thus, the chance of observing a 3 or something more extreme, under the null hypothesis, is \\(\\approx 0.24\\). This is the p-value. The procedure that produced this p-value is called Fisher’s exact test and it uses the hypergeometric distribution. 38.2 Two-by-two tables The data from the experiment is usually summarized by a table like this: tab &lt;- matrix(c(3,1,1,3),2,2) rownames(tab)&lt;-c(&quot;Poured Before&quot;,&quot;Poured After&quot;) colnames(tab)&lt;-c(&quot;Guessed before&quot;,&quot;Guessed after&quot;) tab #&gt; Guessed before Guessed after #&gt; Poured Before 3 1 #&gt; Poured After 1 3 These are referred to as a two-by-two table. For each of the four combinations one can get with a pair of binary variables, they show the observed counts for each occurrence. The function fisher.test performs the inference calculations above and can be obtained like this: fisher.test(tab, alternative=&quot;greater&quot;) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: tab #&gt; p-value = 0.2 #&gt; alternative hypothesis: true odds ratio is greater than 1 #&gt; 95 percent confidence interval: #&gt; 0.314 Inf #&gt; sample estimates: #&gt; odds ratio #&gt; 6.41 38.3 Chi-square Test Notice that, in a way, our funding rates example is similar to the Lady Tasting Tea. However, in the Tasting Tea example, the number of blue and red beads is experimentally fixed and the number of answers given for each category is also fixed. This is because Fisher made sure there were four before tea and four after tea and the lady knew this, so the answers would also have four and four. If this is the case, the sum of the rows and the sum of the columns are fixed. This defines constraints on the possible ways we can fill the two by two table and also permits us to use the hypergeometric distribution. In general, this is not the case. Nonetheless, there is another approach, the Chi-squared test, which is described below. Imagine we have 2823 applicants, some are men and some are women and some get funded, whereas other don’t. We saw that the success rates for men and woman were: totals %&gt;% summarize(percent_men = yes_men/(yes_men+no_men), percent_women = yes_women/(yes_women+no_women)) #&gt; percent_men percent_women #&gt; 1 0.177 0.149 respectively. Would we see this again if we randomly assign funding at the rate: funding_rate &lt;- totals %&gt;% summarize(percent_total = (yes_men + yes_women)/ (yes_men + no_men +yes_women + no_women)) %&gt;% .$percent_total funding_rate #&gt; [1] 0.165 The Chi-square test answers this question. The first step is to create the two-by-two data table: two_by_two &lt;- data.frame(awarded = c(&quot;no&quot;, &quot;yes&quot;), men = c(totals$no_men, totals$yes_men), women = c(totals$no_women, totals$yes_women)) two_by_two #&gt; awarded men women #&gt; 1 no 1345 1011 #&gt; 2 yes 290 177 The general idea of the Chi-square test is to compare this two-by-two table to what you expect to see, which would be: data.frame(awarded = c(&quot;no&quot;, &quot;yes&quot;), men = (totals$no_men + totals$yes_men) * c(1 - funding_rate, funding_rate), women = (totals$no_women + totals$yes_women)* c(1 - funding_rate, funding_rate)) #&gt; awarded men women #&gt; 1 no 1365 991 #&gt; 2 yes 270 197 We can see that more men than expected and less women than expected received funding. However, under the null hypothesis these observations are random variables. The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function chisq.test takes a two by two table and returns the results from the test: chisq_test &lt;- two_by_two %&gt;% select(-awarded) %&gt;% chisq.test() chisq_test #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: . #&gt; X-squared = 4, df = 1, p-value = 0.05 We see that the p-value is 0.0509: chisq_test$p.value #&gt; [1] 0.0509 38.4 The Odds Ratio An informative summary statistic associated with two-by-two tables is the odds ratio. Define the two variables as \\(X = 1\\) if you are a male and 0 otherwise, and \\(Y=1\\) if you are funded and 0 otherwise. The odds of getting funded if you are a man is defined: \\[\\mbox{Pr}(Y=1 \\mid X=1) / \\mbox{Pr}(Y=0 \\mid X=1)\\] and can be computed like this: odds_men &lt;- (two_by_two$men[2] / sum(two_by_two$men)) / (two_by_two$men[1] / sum(two_by_two$men)) And the odds of being funded if you are a woman is: \\[\\mbox{Pr}(Y=1 \\mid X=0) / \\mbox{Pr}(Y=0 \\mid X=0)\\] and can be computed like this: odds_women &lt;- (two_by_two$women[2] / sum(two_by_two$women)) / (two_by_two$women[1] / sum(two_by_two$women)) The odds ratio is the ratio for these two odds: how many times larger are the odds for men than for women? odds_men / odds_women #&gt; [1] 1.23 We often see two by two tables written out as Men Women Awarded a b Not Awarded c d In this case, the odds ratio is \\(\\frac{a/c}{b/d}\\) which is equivalent to \\((ad) / (bc)\\) 38.5 Large samples, small p-values As mentioned earlier, reporting only p-values is not an appropriate way to report the results of data analysis. In scientific journals, for example, some studies seem to overemphasize p-values. Some of these studies have large sample sizes and report impressively small p-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1. In this case the difference may not be practically significant or scientifically significant. Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean very large odds ratio. Notice what happens to the p-value if we multiply our two-by-two table by 10: two_by_two %&gt;% select(-awarded) %&gt;% mutate(men = men*10, women = women*10) %&gt;% chisq.test() #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity correction #&gt; #&gt; data: . #&gt; X-squared = 40, df = 1, p-value = 3e-10 Yet the odds ratio is unchanged. 38.6 Confidence intervals for the odds ratio Computing confidence intervals for the odds ratio is not mathematically straightforward. Unlike other statistics, for which we can derive useful approximations of their distributions, the odds ratio is not only a ratio, but a ratio of ratios. Therefore, there is no simple way of using, for example, the CLT. However, statistical theory tells us that when all four entries of the two by two table are large enough, then the log of the odds ratio is approximately normal with standard error \\[ \\sqrt{1/a + 1/b + 1/c + 1/d} \\] This implies that a 95% confidence interval for the log odds ratio can be formed by: \\[ \\log \\{ (ad) / (cd) \\} \\pm 1.96 \\sqrt{1/a + 1/b + 1/c + 1/d} \\] By exponentiating these two numbers we can construct a confidence interval of the odds ratio. Using R we can compute this confidence interval as follows: log_or &lt;- log( odds_men / odds_women ) se &lt;- two_by_two %&gt;% select(-awarded) %&gt;% summarize(se = sqrt(sum(1/men) + sum(1/women))) %&gt;% .$se ci &lt;- log_or + c(-1,1) * qnorm(0.975) * se ci #&gt; [1] 0.0043 0.4123 If we want to convert it back to the odds ratio scale we can exponentiate: exp(ci) #&gt; [1] 1.00 1.51 Note that 0 is not included in the confidence interval for the log odds ratio (1 not included for odds ratio) which must mean that the p-value is smaller then 0.05. We can confirm this using: 2*(1 - pnorm(log_or, 0, se)) #&gt; [1] 0.0454 This is a slightly different p-value that with the chi-squared test. This is because we are using a different asymptotic approximation to the null distribution. To learn more about inference and asymptotic theory for odds ratio consult the book by McCullagh and Nelder, 1989. "]
]
